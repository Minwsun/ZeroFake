# app/agent_synthesizer.py

import os
import json
import re
from dotenv import load_dotenv
from typing import Dict, Any, List

from app.weather import classify_claim
from app.model_clients import (
    call_gemini_model,
    call_agent_with_capability_fallback,
    ModelClientError,
    RateLimitError,
)
from app.tool_executor import execute_tool_plan  # Import for Re-Search

load_dotenv()


GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SYNTHESIS_PROMPT = ""
CRITIC_PROMPT = ""  # NEW: Prompt cho CRITIC agent


# C√†i ƒë·∫∑t an to√†n
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]


WEATHER_SOURCE_KEYWORDS = [
    "weather",
    "forecast",
    "accuweather",
    "windy",
    "meteoblue",
    "ventusky",
    "nchmf",
    "thoitiet",
    "openweathermap",
    "wunderground",
    "metoffice",
    "bom.gov",
]


def normalize_conclusion(conclusion: str) -> str:
    """
    Normalize conclusion to BINARY classification: TIN TH·∫¨T or TIN GI·∫¢ only.
    
    üü¢ NGUY√äN T·∫ÆC M·ªöI: PRESUMPTION OF TRUTH
    - M·∫∑c ƒë·ªãnh l√† TIN TH·∫¨T n·∫øu kh√¥ng c√≥ d·∫•u hi·ªáu TIN GI·∫¢ r√µ r√†ng
    - Ch·ªâ tr·∫£ v·ªÅ TIN GI·∫¢ khi c√≥ keywords ch·ªâ ƒë·ªãnh r√µ r√†ng
    """
    if not conclusion:
        return "TIN TH·∫¨T"  # ƒê·ªîI: M·∫∑c ƒë·ªãnh TIN TH·∫¨T n·∫øu kh√¥ng c√≥ k·∫øt lu·∫≠n
    
    conclusion_upper = conclusion.upper().strip()
    
    # üî¥ CH·ªà TIN GI·∫¢ KHI C√ì D·∫§U HI·ªÜU R√ï R√ÄNG
    fake_indicators = [
        # Vietnamese fake indicators
        "TIN GI·∫¢", "TIN GIA", "GI·∫¢ M·∫†O", "FAKE", "FALSE",
        "B·ªäA ƒê·∫∂T", "BIA DAT", "L·ª™A ƒê·∫¢O", "LUA DAO", "SCAM",
        "ZOMBIE", "OUTDATED", "L·ªñI TH·ªúI", "LOI THOI",
        "KH√îNG ƒê√öNG", "KHONG DUNG", "SAI S·ª∞ TH·∫¨T", "SAI SU THAT",
        "KH√îNG C√ì C∆† S·ªû", "KHONG CO CO SO", "V√î CƒÇN C·ª®", "VO CAN CU",
        "ALMOST CERTAINLY FALSE", "HIGHLY UNLIKELY",
        "B√ÅC B·ªé", "BAC BO", "KH√îNG X√ÅC NH·∫¨N", "KHONG XAC NHAN",
        # Y t·∫ø sai
        "Y T·∫æ SAI", "Y TE SAI", "MISLEADING",
        # S·ªë li·ªáu phi th·ª±c t·∫ø
        "PHI TH·ª∞C T·∫æ", "PHI THUC TE", "UNREALISTIC",
        # G√ÇY HI·ªÇU L·∫¶M - v·∫´n coi l√† TIN GI·∫¢
        "G√ÇY HI·ªÇU L·∫¶M", "GAY HIEU LAM",
    ]
    
    # N·∫øu c√≥ b·∫•t k·ª≥ indicator TIN GI·∫¢ n√†o -> TIN GI·∫¢
    for indicator in fake_indicators:
        if indicator in conclusion_upper:
            return "TIN GI·∫¢"
    
    # üü¢ T·∫§T C·∫¢ C√ÅC TR∆Ø·ªúNG H·ª¢P KH√ÅC -> TIN TH·∫¨T
    # Bao g·ªìm: TIN TH·∫¨T, CH∆ØA KI·ªÇM CH·ª®NG, TRUE, PROBABLE, LIKELY, etc.
    return "TIN TH·∫¨T"


# Product version database for outdated information detection
# Format: product_pattern -> (latest_version, release_year)
PRODUCT_VERSIONS = {
    # Apple iPhone (as of Dec 2025)
    r"iphone\s*(\d+)": {"latest": 17, "year": 2025, "name": "iPhone"},
    # Samsung Galaxy S
    r"galaxy\s*s\s*(\d+)": {"latest": 25, "year": 2025, "name": "Galaxy S"},
    # Samsung Galaxy Note
    r"galaxy\s*note\s*(\d+)": {"latest": 20, "year": 2020, "name": "Galaxy Note"},
    # Google Pixel
    r"pixel\s*(\d+)": {"latest": 9, "year": 2024, "name": "Pixel"},
    # PlayStation
    r"playstation\s*(\d+)|ps\s*(\d+)": {"latest": 5, "year": 2020, "name": "PlayStation"},
    # Xbox (Xbox One=1, Series X=2)
    r"xbox\s*series\s*([xs])": {"latest": "x", "year": 2020, "name": "Xbox Series"},
    # Windows
    r"windows\s*(\d+)": {"latest": 11, "year": 2021, "name": "Windows"},
    # macOS versions
    r"macos\s*(\d+)|mac\s*os\s*(\d+)": {"latest": 15, "year": 2024, "name": "macOS"},
    # MacBook chips
    r"macbook.*m(\d+)": {"latest": 4, "year": 2024, "name": "MacBook M-chip"},
}


def _detect_outdated_product(text_input: str) -> dict | None:
    """
    Detect if the input mentions an outdated product version.
    Returns dict with product info if outdated, None otherwise.
    """
    text_lower = text_input.lower()
    
    for pattern, info in PRODUCT_VERSIONS.items():
        match = re.search(pattern, text_lower)
        if match:
            # Get the version number from match groups
            version_str = None
            for group in match.groups():
                if group:
                    version_str = group
                    break
            
            if version_str:
                try:
                    # Handle numeric versions
                    if version_str.isdigit():
                        mentioned_version = int(version_str)
                        latest_version = info["latest"]
                        
                        if isinstance(latest_version, int) and mentioned_version < latest_version:
                            return {
                                "product": info["name"],
                                "mentioned_version": mentioned_version,
                                "latest_version": latest_version,
                                "latest_year": info["year"],
                                "is_outdated": True,
                                "years_behind": latest_version - mentioned_version
                            }
                except (ValueError, TypeError):
                    pass
    
    return None


def _is_common_knowledge(text_input: str) -> bool:
    """
    Detect if the claim is about well-known, easily verifiable facts.
    These are facts that are widely accepted and don't need extensive verification.
    """
    text_lower = text_input.lower()
    
    # Well-known tech facts
    common_knowledge_patterns = [
        # Company ownership/development
        ("chatgpt", "openai"),
        ("gpt-4", "openai"),
        ("gpt-3", "openai"),
        ("google", "alphabet"),
        ("youtube", "google"),
        ("instagram", "meta"),
        ("whatsapp", "meta"),
        ("facebook", "meta"),
        ("iphone", "apple"),
        ("android", "google"),
        ("windows", "microsoft"),
        ("azure", "microsoft"),
        ("aws", "amazon"),
        
        # Historical events that are well-documented
        ("facebook", "meta", "2021"),
        ("messi", "world cup", "2022"),
        ("argentina", "world cup", "2022"),
    ]
    
    for pattern in common_knowledge_patterns:
        if all(keyword in text_lower for keyword in pattern):
            return True
    
    return False


def _is_weather_source(item: Dict[str, Any]) -> bool:
    source = (item.get("source") or item.get("url") or "").lower()
    if not source:
        return False
    return any(keyword in source for keyword in WEATHER_SOURCE_KEYWORDS)


def load_synthesis_prompt(prompt_path="prompts/synthesis_prompt.txt"):
    """T·∫£i prompt cho Agent 2 (Synthesizer)"""
    global SYNTHESIS_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            SYNTHESIS_PROMPT = f.read()
        print("INFO: T·∫£i Synthesis Prompt th√†nh c√¥ng.")
    except Exception as e:
        print(f"L·ªñI: kh√¥ng th·ªÉ t·∫£i {prompt_path}: {e}")
        raise


def load_critic_prompt(prompt_path="prompts/critic_prompt.txt"):
    """T·∫£i prompt cho CRITIC agent (Devil's Advocate)"""
    global CRITIC_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            CRITIC_PROMPT = f.read()
        print("INFO: T·∫£i CRITIC Prompt th√†nh c√¥ng.")
    except FileNotFoundError:
        # Fallback to default prompt if file not found
        CRITIC_PROMPT = (
            "B·∫°n l√† Bi·ªán l√Ω ƒë·ªëi l·∫≠p (Devil's Advocate). "
            "H√£y ch·ªâ ra 3 ƒëi·ªÉm y·∫øu, m√¢u thu·∫´n ho·∫∑c kh·∫£ nƒÉng ƒë√¢y l√† tin c≈©/satire/tin ƒë·ªìn. "
            "Ch·ªâ tr·∫£ l·ªùi ng·∫Øn g·ªçn, gay g·∫Øt."
        )
        print(f"WARNING: Kh√¥ng t√¨m th·∫•y {prompt_path}, d√πng prompt m·∫∑c ƒë·ªãnh.")
    except Exception as e:
        print(f"L·ªñI: kh√¥ng th·ªÉ t·∫£i {prompt_path}: {e}")


def _parse_json_from_text(text: str) -> dict:
    """Tr√≠ch xu·∫•t JSON an to√†n t·ª´ text tr·∫£ v·ªÅ c·ªßa LLM"""
    if not text:
        print("L·ªñI: Agent 2 (Synthesizer) kh√¥ng t√¨m th·∫•y JSON.")
        return {}

    cleaned = text.strip()
    # Remove Markdown code fences if present
    if cleaned.startswith("```"):
        cleaned = re.sub(r"^```[a-zA-Z0-9_-]*\s*", "", cleaned)
        cleaned = cleaned.rstrip("`").strip()

    match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            print(f"L·ªñI: Agent 2 (Synthesizer) tr·∫£ v·ªÅ JSON kh√¥ng h·ª£p l·ªá. Text: {cleaned[:300]}...")
            return {}
    # Try direct JSON load if regex failed
    try:
        return json.loads(cleaned)
    except Exception:
        print(f"L·ªñI: Agent 2 (Synthesizer) kh√¥ng t√¨m th·∫•y JSON. Raw response: {cleaned[:300]}...")
        return {}


def _trim_snippet(s: str, max_len: int = 500) -> str:
    """TƒÉng max_len t·ª´ 280 l√™n 500 ƒë·ªÉ gi·ªØ nhi·ªÅu context h∆°n cho models."""
    if not s:
        return ""
    s = s.replace("\n", " ").strip()
    return s[:max_len]


def _trim_evidence_bundle(bundle: Dict[str, Any], cap_l2: int = 10, cap_l3: int = 10, cap_l4: int = 5) -> Dict[str, Any]:
    """TƒÉng cap t·ª´ 5/5/2 l√™n 10/10/5 ƒë·ªÉ g·ª≠i nhi·ªÅu evidence h∆°n cho CRITIC v√† JUDGE."""
    if not bundle:
        return {"layer_1_tools": [], "layer_2_high_trust": [], "layer_3_general": [], "layer_4_social_low": []}
    out = {
        "layer_1_tools": [], # OpenWeather API data
        "layer_2_high_trust": [],
        "layer_3_general": [],
        "layer_4_social_low": []
    }
    
    # L·ªõp 1: OpenWeather API data (quan tr·ªçng cho tin th·ªùi ti·∫øt)
    for it in (bundle.get("layer_1_tools") or []):
        out["layer_1_tools"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date"),
            "weather_data": it.get("weather_data")  # Gi·ªØ nguy√™n d·ªØ li·ªáu g·ªëc t·ª´ OpenWeather
        })
    
    # L·ªõp 2
    for it in (bundle.get("layer_2_high_trust") or [])[:cap_l2]:
        out["layer_2_high_trust"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    # L·ªõp 3
    for it in (bundle.get("layer_3_general") or [])[:cap_l3]:
        out["layer_3_general"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    # L·ªõp 4
    for it in (bundle.get("layer_4_social_low") or [])[:cap_l4]:
        out["layer_4_social_low"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    return out


def _as_str(x: Any) -> str:
    try:
        return x if isinstance(x, str) else ("" if x is None else str(x))
    except Exception:
        return ""


def _heuristic_summarize(text_input: str, bundle: Dict[str, Any], current_date: str) -> Dict[str, Any]:
    """
    (ƒê√É S·ª¨A ƒê·ªîI - ADVERSARIAL DIALECTIC)
    Logic d·ª± ph√≤ng khi LLM th·∫•t b·∫°i.
    ∆Øu ti√™n:
    1. Ph√°t hi·ªán s·∫£n ph·∫©m l·ªói th·ªùi (iPhone 12, Galaxy S21, etc.)
    2. L·ªõp 1 (OpenWeather API) cho tin th·ªùi ti·∫øt
    3. L·ªõp 2/3 cho tin t·ª©c kh√°c
    """
    l1 = bundle.get("layer_1_tools") or []
    l2 = bundle.get("layer_2_high_trust") or []
    l3 = bundle.get("layer_3_general") or []

    try:
        claim = classify_claim(text_input)
    except Exception:
        claim = {"is_weather": False}

    is_weather_claim = claim.get("is_weather", False)
    text_lower = text_input.lower()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PRIORITY 0: S·ª± th·∫≠t hi·ªÉn nhi√™n (Common Knowledge)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if _is_common_knowledge(text_input):
        debate_log = {
            "red_team_argument": "T√¥i kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng b√°c b·ªè s·ª± th·∫≠t khoa h·ªçc/k·ªπ thu·∫≠t n√†y.",
            "blue_team_argument": "ƒê√¢y l√† s·ª± th·∫≠t ƒë√£ ƒë∆∞·ª£c khoa h·ªçc/c·ªông ƒë·ªìng c√¥ng nh·∫≠n r·ªông r√£i.",
            "judge_reasoning": "Blue Team th·∫Øng. ƒê√¢y l√† ki·∫øn th·ª©c ph·ªï th√¥ng ƒë√£ ƒë∆∞·ª£c x√°c nh·∫≠n."
        }
        return {
            "conclusion": "TIN TH·∫¨T",
            "confidence_score": 99,
            "reason": "ƒê√¢y l√† s·ª± th·∫≠t khoa h·ªçc/k·ªπ thu·∫≠t ƒë√£ ƒë∆∞·ª£c c√¥ng nh·∫≠n r·ªông r√£i.",
            "debate_log": debate_log,
            "key_evidence_snippet": "Ki·∫øn th·ª©c ph·ªï th√¥ng",
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "",
            "cached": False
        }
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PRIORITY 1: Ph√°t hi·ªán s·∫£n ph·∫©m L·ªñI TH·ªúI (Outdated Product)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    outdated_info = _detect_outdated_product(text_input)
    if outdated_info and outdated_info.get("is_outdated"):
        product = outdated_info["product"]
        mentioned = outdated_info["mentioned_version"]
        latest = outdated_info["latest_version"]
        latest_year = outdated_info["latest_year"]
        
        # Build Adversarial Dialectic debate
        debate_log = {
            "red_team_argument": _as_str(
                f"Th√¥ng tin n√†y SAI! {product} {mentioned} l√† phi√™n b·∫£n c≈©. "
                f"Hi·ªán t·∫°i ƒë√£ c√≥ {product} {latest} (ra m·∫Øt nƒÉm {latest_year}). "
                f"Vi·ªác ƒëƒÉng tin v·ªÅ {product} {mentioned} nh∆∞ tin m·ªõi l√† SAI S·ª∞ TH·∫¨T."
            ),
            "blue_team_argument": _as_str(
                f"ƒê√∫ng l√† {product} {mentioned} ƒë√£ ra m·∫Øt th·∫≠t. "
                f"Tuy nhi√™n, ƒë√¢y l√† th√¥ng tin l·ªói th·ªùi. T√¥i th·ª´a nh·∫≠n thua cu·ªôc."
            ),
            "judge_reasoning": _as_str(
                f"Red Team th·∫Øng. {product} {mentioned} l√† phi√™n b·∫£n c≈©. "
                f"Hi·ªán t·∫°i ƒë√£ c√≥ {product} {latest}. Tin l·ªói th·ªùi = TIN GI·∫¢."
            )
        }
        
        return {
            "conclusion": "TIN GI·∫¢",
            "confidence_score": 95,
            "reason": _as_str(
                f"{product} {mentioned} ƒë√£ l·ªói th·ªùi. "
                f"Hi·ªán t·∫°i ƒë√£ c√≥ {product} {latest} (nƒÉm {latest_year}). "
                f"Tin v·ªÅ s·∫£n ph·∫©m c≈© = TIN GI·∫¢."
            ),
            "debate_log": debate_log,
            "key_evidence_snippet": _as_str(f"{product} {latest} ra m·∫Øt nƒÉm {latest_year}"),
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "Th√¥ng tin l·ªói th·ªùi ƒë∆∞·ª£c tr√¨nh b√†y nh∆∞ tin m·ªõi",
            "cached": False
        }

    # ∆Øu ti√™n L·ªõp 1 (OpenWeather API) cho tin th·ªùi ti·∫øt
    if is_weather_claim and l1:
        weather_item = l1[0]
        weather_data = weather_item.get("weather_data", {})
        if weather_data:
            # So s√°nh ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt
            main_condition = weather_data.get("main", "").lower()
            description = weather_data.get("description", "").lower()
            
            # Ki·ªÉm tra m∆∞a
            if "m∆∞a" in text_lower or "rain" in text_lower:
                if "rain" in main_condition or "rain" in description:
                    # Ki·ªÉm tra m·ª©c ƒë·ªô m∆∞a
                    if "m∆∞a to" in text_lower or "m∆∞a l·ªõn" in text_lower or "heavy rain" in text_lower:
                        if "heavy" in description or "torrential" in description:
                            return {
                                "conclusion": "TIN TH·∫¨T",
                                "reason": _as_str(f"Heuristic: OpenWeather API x√°c nh·∫≠n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}¬∞C) cho {weather_data.get('location')} ng√†y {weather_data.get('date')}."),
                                "style_analysis": "",
                                "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                                "key_evidence_source": _as_str(weather_item.get("source")),
                                "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                                "cached": False
                            }
                    else:
                        # M∆∞a th∆∞·ªùng
                        return {
                            "conclusion": "TIN TH·∫¨T",
                            "reason": _as_str(f"Heuristic: OpenWeather API x√°c nh·∫≠n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}¬∞C) cho {weather_data.get('location')} ng√†y {weather_data.get('date')}."),
                            "style_analysis": "",
                            "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                            "key_evidence_source": _as_str(weather_item.get("source")),
                            "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                            "cached": False
                        }
            # Ki·ªÉm tra n·∫Øng
            elif "n·∫Øng" in text_lower or "sunny" in text_lower or "clear" in text_lower:
                if "clear" in main_condition or "sunny" in description:
                    return {
                        "conclusion": "TIN TH·∫¨T",
                        "reason": _as_str(f"Heuristic: OpenWeather API x√°c nh·∫≠n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}¬∞C) cho {weather_data.get('location')} ng√†y {weather_data.get('date')}."),
                        "style_analysis": "",
                        "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                        "key_evidence_source": _as_str(weather_item.get("source")),
                        "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                        "cached": False
                    }
            # N·∫øu kh√¥ng kh·ªõp ƒëi·ªÅu ki·ªán c·ª• th·ªÉ, v·∫´n tr·∫£ v·ªÅ d·ªØ li·ªáu t·ª´ OpenWeather
            return {
                "conclusion": "TIN TH·∫¨T",
                "reason": _as_str(f"Heuristic: OpenWeather API cung c·∫•p d·ªØ li·ªáu th·ªùi ti·∫øt {weather_item.get('source')} - {description} ({weather_data.get('temperature')}¬∞C) cho {weather_data.get('location')} ng√†y {weather_data.get('date')}."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                "key_evidence_source": _as_str(weather_item.get("source")),
                "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                "cached": False
            }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PRIORITY 2: Ki·ªÉm tra ngu·ªìn L2 C√ì LI√äN QUAN ƒë·∫øn claim
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ quan tr·ªçng t·ª´ claim ƒë·ªÉ ki·ªÉm tra relevance
    person_keywords = []
    org_location_keywords = []
    
    # T√¨m t√™n ng∆∞·ªùi (vi·∫øt hoa, th∆∞·ªùng l√† t·ª´ ƒë·∫ßu ti√™n)
    name_pattern = re.compile(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b')
    names = name_pattern.findall(text_input)
    person_keywords.extend([n.lower() for n in names])
    
    # T√¨m t√™n t·ªï ch·ª©c/CLB/ƒë·ªãa ƒëi·ªÉm
    org_patterns = [
        (r'clb\s+(\w+\s*\w*)', 'clb'),
        (r'fc\s+(\w+\s*\w*)', 'fc'),
        (r'ƒë·ªôi\s+(\w+\s*\w*)', 'ƒë·ªôi'),
    ]
    for pat, prefix in org_patterns:
        match = re.search(pat, text_lower)
        if match:
            org_location_keywords.append(match.group(1).strip())
    
    # Th√™m c√°c ƒë·ªãa danh ph·ªï bi·∫øn
    location_names = ["h√† n·ªôi", "ha noi", "hanoi", "s√†i g√≤n", "saigon", "ho chi minh", 
                      "vi·ªát nam", "vietnam", "barca", "barcelona", "inter miami", "real madrid"]
    for loc in location_names:
        if loc in text_lower:
            org_location_keywords.append(loc)
    
    # Ki·ªÉm tra L2 sources c√≥ li√™n quan TH·ª∞C S·ª∞ kh√¥ng
    # ƒê·ªëi v·ªõi claim v·ªÅ ng∆∞·ªùi + t·ªï ch·ª©c: C·∫¶N KH·ªöP C·∫¢ HAI
    relevant_l2 = []
    has_person_org_claim = len(person_keywords) > 0 and len(org_location_keywords) > 0
    
    for item in l2:
        snippet = (item.get("snippet") or "").lower()
        title = (item.get("title") or "").lower()
        combined = snippet + " " + title
        
        if has_person_org_claim:
            # Claim c√≥ c·∫£ ng∆∞·ªùi + t·ªï ch·ª©c -> c·∫ßn kh·ªõp C·∫¢ HAI
            has_person = any(kw in combined for kw in person_keywords if kw and len(kw) > 2)
            has_org = any(kw in combined for kw in org_location_keywords if kw and len(kw) > 2)
            
            if has_person and has_org:
                relevant_l2.append(item)
        else:
            # Claim ƒë∆°n gi·∫£n -> ch·ªâ c·∫ßn kh·ªõp 1 keyword
            is_relevant = False
            all_keywords = person_keywords + org_location_keywords
            for kw in all_keywords:
                if kw and len(kw) > 2 and kw in combined:
                    is_relevant = True
                    break
            if is_relevant:
                relevant_l2.append(item)
    
    # Gi·∫£m y√™u c·∫ßu t·ª´ 2 xu·ªëng 1: Ch·ªâ c·∫ßn 1 ngu·ªìn uy t√≠n LI√äN QUAN TH·ª∞C S·ª∞ ƒë·ªÉ h·ªó tr·ª£ TIN TH·∫¨T
    if len(relevant_l2) >= 1:
        top = relevant_l2[0]
        return {
            "conclusion": "TIN TH·∫¨T",
            "debate_log": {
                "red_team_argument": "T√¥i kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng b√°c b·ªè.",
                "blue_team_argument": _as_str(f"C√≥ √≠t nh·∫•t 2 ngu·ªìn uy t√≠n x√°c nh·∫≠n: {top.get('source')}."),
                "judge_reasoning": "Blue Team th·∫Øng v·ªõi b·∫±ng ch·ª©ng t·ª´ nhi·ªÅu ngu·ªìn uy t√≠n."
            },
            "confidence_score": 85,
            "reason": _as_str(f"C√≥ t·ª´ 2 ngu·ªìn uy t√≠n x√°c nh·∫≠n th√¥ng tin n√†y ({top.get('source')})."),
            "style_analysis": "",
            "key_evidence_snippet": _as_str(top.get("snippet")),
            "key_evidence_source": _as_str(top.get("source")),
            "evidence_link": _as_str(top.get("url") or top.get("link")),
            "cached": False
        }
    
    # N·∫øu c√≥ ngu·ªìn L2 nh∆∞ng KH√îNG li√™n quan -> C√≥ th·ªÉ l√† TIN GI·∫¢
    all_claim_keywords = person_keywords + org_location_keywords
    if len(l2) >= 2 and len(relevant_l2) == 0 and all_claim_keywords:
        # Claim c√≥ th·ª±c th·ªÉ c·ª• th·ªÉ (t√™n ng∆∞·ªùi/t·ªï ch·ª©c) nh∆∞ng kh√¥ng c√≥ b·∫±ng ch·ª©ng li√™n quan
        debate_log = {
            "red_team_argument": _as_str(
                f"Kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ ngu·ªìn uy t√≠n n√†o x√°c nh·∫≠n th√¥ng tin n√†y. "
                f"C√°c ngu·ªìn t√¨m ƒë∆∞·ª£c kh√¥ng li√™n quan ƒë·∫øn n·ªôi dung claim."
            ),
            "blue_team_argument": _as_str(
                "T√¥i kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng x√°c nh·∫≠n. T√¥i th·ª´a nh·∫≠n thua cu·ªôc."
            ),
            "judge_reasoning": _as_str(
                "Red Team th·∫Øng. Kh√¥ng c√≥ ngu·ªìn uy t√≠n n√†o x√°c nh·∫≠n tin n√†y. "
                "ƒê√¢y c√≥ th·ªÉ l√† tin ƒë·ªìn ho·∫∑c tin gi·∫£."
            )
        }
        return {
            "conclusion": "TIN GI·∫¢",
            "confidence_score": 80,
            "reason": _as_str(
                "Kh√¥ng t√¨m th·∫•y ngu·ªìn uy t√≠n n√†o x√°c nh·∫≠n th√¥ng tin n√†y. "
                "C√°c k·∫øt qu·∫£ t√¨m ki·∫øm kh√¥ng li√™n quan ƒë·∫øn n·ªôi dung claim."
            ),
            "debate_log": debate_log,
            "key_evidence_snippet": "",
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "Tin c√≥ v·∫ª l√† tin ƒë·ªìn kh√¥ng c√≥ cƒÉn c·ª©",
            "cached": False
        }

    if is_weather_claim and l2:
        weather_sources = [item for item in l2 if _is_weather_source(item)]
        if weather_sources:
            top = weather_sources[0]
            return {
                "conclusion": "TIN TH·∫¨T",
                "reason": _as_str(f"Heuristic (weather): D·ª±a tr√™n ngu·ªìn d·ª± b√°o th·ªùi ti·∫øt {top.get('source')} ({top.get('date') or 'N/A'})."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(top.get("snippet")),
                "key_evidence_source": _as_str(top.get("source")),
                "evidence_link": _as_str(top.get("url") or top.get("link")),
                "cached": False
            }

    if is_weather_claim:
        layer3 = bundle.get("layer_3_general") or []
        weather_layer3 = [item for item in layer3 if _is_weather_source(item)]
        if weather_layer3:
            top = weather_layer3[0]
            return {
                "conclusion": "TIN TH·∫¨T",
                "reason": _as_str(f"Heuristic (weather): D·ª±a tr√™n trang d·ª± b√°o {top.get('source')} cho ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c n√™u."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(top.get("snippet")),
                "key_evidence_source": _as_str(top.get("source")),
                "evidence_link": _as_str(top.get("url") or top.get("link")),
                "cached": False
            }

    # Ph√°t hi·ªán th√¥ng tin g√¢y hi·ªÉu l·∫ßm do ƒë√£ c≈© (ƒë·∫∑c bi·ªát v·ªõi s·∫£n ph·∫©m/phi√™n b·∫£n)
    if not is_weather_claim:
        evidence_items = l2 + l3
        old_items = [item for item in evidence_items if item.get("is_old")]
        fresh_items = [item for item in evidence_items if item.get("is_old") is False]

        marketing_keywords = [
            "gi·∫£m gi√°", "khuy·∫øn m√£i", "sale", "ra m·∫Øt", "m·ªü b√°n", "ƒë·∫∑t tr∆∞·ªõc",
            "phi√™n b·∫£n", "model", "th·∫ø h·ªá", "ƒë·ªùi", "n√¢ng c·∫•p", "l√™n k·ªá", "∆∞u ƒë√£i",
            "launch", "promotion"
        ]
        product_pattern = re.compile(r"(iphone|ipad|macbook|galaxy|pixel|surface|playstation|xbox|sony|samsung|apple|oppo|xiaomi|huawei|vinfast)\s?[0-9a-z]{1,4}", re.IGNORECASE)
        mentions_product_cycle = any(kw in text_lower for kw in marketing_keywords) or bool(product_pattern.search(text_input))

        if old_items and (fresh_items or mentions_product_cycle):
            reference_old = old_items[0]
            old_source = reference_old.get("source") or reference_old.get("url") or "ngu·ªìn c≈©"
            old_date = reference_old.get("date") or "tr∆∞·ªõc ƒë√¢y"
            latest_snippet = _as_str(reference_old.get("snippet"))

            if fresh_items:
                latest_item = fresh_items[0]
                latest_source = latest_item.get("source") or latest_item.get("url") or "ngu·ªìn m·ªõi"
                latest_date = latest_item.get("date") or "g·∫ßn ƒë√¢y"
                reason = _as_str(
                    f"Th√¥ng tin v·ªÅ '{text_input}' d·ª±a tr√™n ngu·ªìn {old_source} ({old_date}) ƒë√£ c≈©, "
                    f"trong khi c√°c ngu·ªìn m·ªõi nh∆∞ {latest_source} ({latest_date}) cho th·∫•y b·ªëi c·∫£nh ƒë√£ thay ƒë·ªïi. "
                    "Vi·ªác tr√¨nh b√†y nh∆∞ tin n√≥ng d·ªÖ g√¢y hi·ªÉu l·∫ßm."
                )
            else:
                reason = _as_str(
                    f"Th√¥ng tin v·ªÅ '{text_input}' ch·ªâ ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi ngu·ªìn c≈© {old_source} ({old_date}). "
                    "S·∫£n ph·∫©m/s·ª± ki·ªán n√†y ƒë√£ xu·∫•t hi·ªán t·ª´ l√¢u n√™n vi·ªác tr√¨nh b√†y nh∆∞ tin t·ª©c m·ªõi l√† g√¢y hi·ªÉu l·∫ßm."
                )

            return {
                "conclusion": "TIN GI·∫¢",
                "reason": reason,
                "style_analysis": "Tin l·ªói th·ªùi",
                "key_evidence_snippet": latest_snippet,
                "key_evidence_source": _as_str(old_source),
                "evidence_link": _as_str(reference_old.get("url") or reference_old.get("link")),
                "cached": False
            }

        if mentions_product_cycle and fresh_items and not old_items:
            latest_item = fresh_items[0]
            latest_source = latest_item.get("source") or latest_item.get("url") or "ngu·ªìn m·ªõi"
            latest_date = latest_item.get("date") or "g·∫ßn ƒë√¢y"
            reason = _as_str(
                f"Kh√¥ng t√¨m th·∫•y ngu·ªìn g·∫ßn ƒë√¢y x√°c nh·∫≠n '{text_input}', trong khi c√°c s·∫£n ph·∫©m m·ªõi h∆°n ƒë√£ xu·∫•t hi·ªán "
                f"(v√≠ d·ª• {latest_source}, {latest_date}). ƒê√¢y l√† th√¥ng tin c≈© ƒë∆∞·ª£c l·∫∑p l·∫°i khi·∫øn ng∆∞·ªùi ƒë·ªçc hi·ªÉu l·∫ßm b·ªëi c·∫£nh hi·ªán t·∫°i."
            )
            return {
                "conclusion": "TIN GI·∫¢",
                "reason": reason,
                "style_analysis": "Tin l·ªói th·ªùi",
                "key_evidence_snippet": _as_str(latest_item.get("snippet")),
                "key_evidence_source": _as_str(latest_source),
                "evidence_link": _as_str(latest_item.get("url") or latest_item.get("link")),
                "cached": False
            }

        claim_implies_present = any(
            kw in text_lower
            for kw in [
                "hi·ªán nay", "b√¢y gi·ªù", "ƒëang", "s·∫Øp", "v·ª´a", "today", "now", "currently",
                "m·ªõi ƒë√¢y", "ngay l√∫c n√†y", "trong th·ªùi gian t·ªõi"
            ]
        )
        if claim_implies_present and old_items and not fresh_items:
            old_item = old_items[0]
            older_source = old_item.get("source") or old_item.get("url") or "ngu·ªìn c≈©"
            older_date = old_item.get("date") or "tr∆∞·ªõc ƒë√¢y"
            reason = _as_str(
                f"'{text_input}' √°m ch·ªâ th√¥ng tin ƒëang di·ªÖn ra nh∆∞ng ch·ªâ c√≥ ngu·ªìn {older_source} ({older_date}) t·ª´ tr∆∞·ªõc kia. "
                "Vi·ªác d√πng l·∫°i tin c≈© khi·∫øn ng∆∞·ªùi ƒë·ªçc hi·ªÉu sai v·ªÅ t√¨nh tr·∫°ng hi·ªán t·∫°i."
            )
            return {
                "conclusion": "TIN GI·∫¢",
                "reason": reason,
                "style_analysis": "Tin l·ªói th·ªùi",
                "key_evidence_snippet": _as_str(old_item.get("snippet")),
                "key_evidence_source": _as_str(older_source),
                "evidence_link": _as_str(old_item.get("url") or old_item.get("link")),
                "cached": False
            }

        misleading_tokens = [
            "ƒë√£ k·∫øt th√∫c", "ƒë√£ d·ª´ng", "ng·ª´ng √°p d·ª•ng", "kh√¥ng c√≤n √°p d·ª•ng",
            "ƒë√£ h·ªßy", "ƒë√£ ho√£n", "ƒë√£ ƒë√≥ng", "ƒë√£ ng∆∞ng", "no longer", "ended", "discontinued"
        ]
        for item in evidence_items:
            snippet_lower = (item.get("snippet") or "").lower()
            if any(token in snippet_lower for token in misleading_tokens):
                source = item.get("source") or item.get("url") or "ngu·ªìn c·∫≠p nh·∫≠t"
                reason = _as_str(
                    f"'{text_input}' b·ªè qua c·∫≠p nh·∫≠t t·ª´ {source} cho bi·∫øt s·ª± ki·ªán/ch∆∞∆°ng tr√¨nh ƒë√£ k·∫øt th√∫c ho·∫∑c thay ƒë·ªïi "
                    "n√™n th√¥ng tin d·ªÖ g√¢y hi·ªÉu l·∫ßm."
                )
                return {
                    "conclusion": "TIN GI·∫¢",
                    "reason": reason,
                    "style_analysis": "Tin ƒë√£ kh√¥ng c√≤n ƒë√∫ng",
                    "key_evidence_snippet": _as_str(item.get("snippet")),
                    "key_evidence_source": _as_str(source),
                    "evidence_link": _as_str(item.get("url") or item.get("link")),
                    "cached": False
                }

    # FIX: M·∫∑c ƒë·ªãnh TIN TH·∫¨T khi kh√¥ng c√≥ b·∫±ng ch·ª©ng B√ÅC B·ªé (innocent until proven guilty)
    # Tr∆∞·ªõc ƒë√¢y m·∫∑c ƒë·ªãnh TIN GI·∫¢ g√¢y false positive cao
    return {
        "conclusion": "TIN TH·∫¨T",
        "confidence_score": 60,
        "reason": _as_str("Kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng B√ÅC B·ªé th√¥ng tin n√†y. D·ª±a tr√™n nguy√™n t·∫Øc 'innocent until proven guilty'."),
        "debate_log": {
            "red_team_argument": "Kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng ph·∫£n b√°c r√µ r√†ng.",
            "blue_team_argument": "Kh√¥ng c√≥ ngu·ªìn n√†o b√°c b·ªè th√¥ng tin n√†y.",
            "judge_reasoning": "Khi kh√¥ng c√≥ b·∫±ng ch·ª©ng b√°c b·ªè, tin ƒë∆∞·ª£c coi l√† c√≥ th·ªÉ ƒë√∫ng."
        },
        "style_analysis": "",
        "key_evidence_snippet": "",
        "key_evidence_source": "",
        "evidence_link": "",
        "cached": False
    }


def _normalize_agent2_model(model_key: str | None) -> str:
    """Normalize Agent 2 model identifier."""
    if not model_key:
        return "models/gemini-2.5-pro"
    mapping = {
        "gemini_flash": "models/gemini-2.5-flash",
        "gemini flash": "models/gemini-2.5-flash",
        "gemini-2.5-flash": "models/gemini-2.5-flash",
        "models/gemini_flash": "models/gemini-2.5-flash",
        "gemini_pro": "models/gemini-2.5-pro",
        "gemini pro": "models/gemini-2.5-pro",
        "models/gemini-2.5-pro": "models/gemini-2.5-pro",
        "openai/gpt-oss-120b": "openai/gpt-oss-120b",
        "meta-llama/llama-3.3-70b-instruct": "meta-llama/llama-3.3-70b-instruct",
        "qwen/qwen-2.5-72b-instruct": "qwen/qwen-2.5-72b-instruct",
        "gemma-3-1b": "models/gemma-3-1b-it",
        "gemma-3-1b-it": "models/gemma-3-1b-it",
        "gemma-3-2b": "models/gemma-3-4b-it",  # 2B not available, fallback to 4B
        "gemma-3-4b": "models/gemma-3-4b-it",
        "gemma-3-4b-it": "models/gemma-3-4b-it",
        "gemma-3-12b": "models/gemma-3-12b-it",
        "gemma-3-12b-it": "models/gemma-3-12b-it",
        "gemma-3-27b": "models/gemma-3-27b-it",
        "gemma-3-27b-it": "models/gemma-3-27b-it",
        "google/gemma-3-1b": "models/gemma-3-1b-it",
        "google/gemma-3-2b": "models/gemma-3-4b-it",
        "google/gemma-3-4b": "models/gemma-3-4b-it",
        "google/gemma-3-12b": "models/gemma-3-12b-it",
        "google/gemma-3-27b": "models/gemma-3-27b-it",
        "models/gemma-3-1b": "models/gemma-3-1b-it",
        "models/gemma-3-2b": "models/gemma-3-4b-it",
        "models/gemma-3-4b": "models/gemma-3-4b-it",
        "models/gemma-3-12b": "models/gemma-3-12b-it",
        "models/gemma-3-27b": "models/gemma-3-27b-it",
        "models/gemma-3-1b-it": "models/gemma-3-1b-it",
        "models/gemma-3-4b-it": "models/gemma-3-4b-it",
        "models/gemma-3-12b-it": "models/gemma-3-12b-it",
        "models/gemma-3-27b-it": "models/gemma-3-27b-it",
        "models/gemma-3n-e2b-it": "models/gemma-3n-e2b-it",
        "models/gemma-3n-e4b-it": "models/gemma-3n-e4b-it",
    }
    return mapping.get(model_key, model_key)


def _detect_agent2_provider(model_name: str) -> str:
    """Detect provider for Agent 2 model."""
    if not model_name:
        return "gemini"
    lowered = model_name.lower()
    if "gemini" in lowered or "gemma" in lowered or model_name.startswith("models/"):
        return "gemini"
    # All Agent 2 models now use Gemini API
    return "gemini"

async def execute_final_analysis(
    text_input: str,
    evidence_bundle: dict,
    current_date: str,
    model_key: str | None = None,
    flash_mode: bool = False,
    site_query_string: str = "",  # Added for re-search
) -> dict:
    """
    Pipeline: Input ‚Üí Planner ‚Üí Search ‚Üí CRITIC ‚Üí JUDGE ‚Üí (RE-SEARCH n·∫øu c·∫ßn)
    
    1. CRITIC (Bi·ªán l√Ω) - Ph·∫£n bi·ªán m·∫°nh, t√¨m ƒëi·ªÉm y·∫øu trong b·∫±ng ch·ª©ng
    2. JUDGE (Th·∫©m ph√°n) - Ra ph√°n quy·∫øt d·ª±a tr√™n b·∫±ng ch·ª©ng V√Ä √Ω ki·∫øn CRITIC
    3. RE-SEARCH - Ch·ªâ khi JUDGE y√™u c·∫ßu th√™m b·∫±ng ch·ª©ng (Self-Correction)
    """
    if not SYNTHESIS_PROMPT:
        raise ValueError("Synthesis prompt (prompt 2) ch∆∞a ƒë∆∞·ª£c t·∫£i.")
    if not CRITIC_PROMPT:
        print("WARNING: Critic prompt ch∆∞a ƒë∆∞·ª£c t·∫£i, d√πng m·∫∑c ƒë·ªãnh.")

    # Trim evidence before sending to models
    trimmed_bundle = _trim_evidence_bundle(evidence_bundle)
    evidence_bundle_json = json.dumps(trimmed_bundle, indent=2, ensure_ascii=False)

    # =========================================================================
    # PHASE 1: CRITIC AGENT (BI·ªÜN L√ù ƒê·ªêI L·∫¨P)
    # =========================================================================
    critic_report = "Kh√¥ng c√≥ ph·∫£n bi·ªán."
    try:
        print(f"\n[CRITIC] B·∫Øt ƒë·∫ßu ph·∫£n bi·ªán (Model: {model_key})...")
        critic_prompt_filled = CRITIC_PROMPT.replace("{text_input}", text_input)
        critic_prompt_filled = critic_prompt_filled.replace("{evidence_bundle_json}", evidence_bundle_json)
        critic_prompt_filled = critic_prompt_filled.replace("{current_date}", current_date)
        
        critic_report = await call_agent_with_capability_fallback(
            role="CRITIC",
            prompt=critic_prompt_filled,
            temperature=0.7, # C·∫ßn creativity ƒë·ªÉ t√¨m l·ªói
            timeout=60.0
        )
        print(f"[CRITIC] Report:\n{critic_report[:200]}...")
        
    except Exception as e:
        print(f"[CRITIC] G·∫∑p l·ªói: {e}")
        critic_report = "L·ªói khi ch·∫°y Critic Agent."

    # =========================================================================
    # PHASE 2: JUDGE AGENT (TH·∫®M PH√ÅN) - Round 1
    # =========================================================================
    judge_result = {}
    try:
        print(f"\n[JUDGE] B·∫Øt ƒë·∫ßu ph√°n quy·∫øt Round 1...")
        judge_prompt_filled = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
        judge_prompt_filled = judge_prompt_filled.replace("{evidence_bundle_json}", evidence_bundle_json) # D√πng l·∫°i json c≈©
        judge_prompt_filled = judge_prompt_filled.replace("{current_date}", current_date)
        judge_prompt_filled += f"\n\n[√ù KI·∫æN BI·ªÜN L√ù (CRITIC)]:\n{critic_report}"
        
        judge_text = await call_agent_with_capability_fallback(
            role="JUDGE",
            prompt=judge_prompt_filled,
            temperature=0.1, # C·∫ßn strict logic
            timeout=80.0
        )
        
        judge_result = _parse_json_from_text(judge_text)

        # ---------------------------------------------------------------------
        # ADAPTER: Convert New "Cognitive Architecture" JSON to Flat Schema
        # ---------------------------------------------------------------------
        verdict_meta = judge_result.get("verdict_metadata")
        if verdict_meta:
            # CONCLUSION
            judge_result["conclusion"] = verdict_meta.get("conclusion")
            judge_result["confidence_score"] = verdict_meta.get("probability_score")
            
            # REASON (Combine BLUF + Synthesis)
            exec_summary = judge_result.get("executive_summary") or {}
            dialectical = judge_result.get("dialectical_analysis") or {}
            
            bluf = exec_summary.get("bluf")
            synthesis = dialectical.get("synthesis")
            
            combined_reason = ""
            if bluf:
                combined_reason += f"{bluf}\n\n"
            if synthesis:
                combined_reason += f"ANALYSIS: {synthesis}"
            
            judge_result["reason"] = combined_reason.strip() or "No rationale provided."
            
            # DEBATE LOG
            judge_result["debate_log"] = {
                "red_team_argument": dialectical.get("antithesis", "N/A"),
                "blue_team_argument": dialectical.get("thesis", "N/A"),
                "judge_reasoning": dialectical.get("synthesis", "N/A")
            }
            
            # STYLE / WEP
            judge_result["style_analysis"] = verdict_meta.get("wep_label") or "N/A"
            
            # KEY EVIDENCE
            citations = judge_result.get("key_evidence_citations") or []
            if citations and isinstance(citations, list) and len(citations) > 0:
                first_cit = citations[0]
                judge_result["key_evidence_snippet"] = first_cit.get("quote") or "N/A"
                judge_result["key_evidence_source"] = first_cit.get("source") or "N/A"
                judge_result["evidence_link"] = first_cit.get("url") or ""
                
            print(f"[JUDGE] Round 1 (Cognitive Schema): {judge_result.get('conclusion')} ({judge_result.get('confidence_score')}%)")
        else:
            # FIX: Handle FLAT SCHEMA (fallback models may return simpler JSON)
            # Fallback models c√≥ th·ªÉ tr·∫£ v·ªÅ nhi·ªÅu format kh√°c nhau
            
            # 1. T√¨m conclusion t·ª´ nhi·ªÅu field c√≥ th·ªÉ
            if not judge_result.get("conclusion"):
                for key in ["final_conclusion", "verdict", "result", "classification", "Âà§ÂÆö"]:
                    if judge_result.get(key):
                        judge_result["conclusion"] = judge_result[key]
                        break
            
            # 2. T√¨m confidence_score t·ª´ nhi·ªÅu field c√≥ th·ªÉ
            if not judge_result.get("confidence_score"):
                for key in ["probability_score", "confidence", "score", "probability", "certainty", "ƒë·ªô_tin_c·∫≠y"]:
                    val = judge_result.get(key)
                    if val is not None:
                        try:
                            judge_result["confidence_score"] = int(val) if isinstance(val, (int, float)) else int(str(val).replace("%", ""))
                        except:
                            pass
                        break
                        
                # N·∫øu v·∫´n kh√¥ng c√≥, th·ª≠ t√¨m trong nested objects
                if not judge_result.get("confidence_score"):
                    for nested_key in ["metadata", "verdict_info", "analysis"]:
                        nested = judge_result.get(nested_key)
                        if isinstance(nested, dict):
                            for key in ["probability_score", "confidence", "score", "confidence_score"]:
                                val = nested.get(key)
                                if val is not None:
                                    try:
                                        judge_result["confidence_score"] = int(val) if isinstance(val, (int, float)) else int(str(val).replace("%", ""))
                                    except:
                                        pass
                                    break
            
            # 3. T√¨m reason t·ª´ nhi·ªÅu field c√≥ th·ªÉ (m·ªü r·ªông danh s√°ch)
            if not judge_result.get("reason"):
                reason_keys = [
                    "reasoning", "explanation", "rationale", "analysis", 
                    "l√Ω_do", "gi·∫£i_th√≠ch", "bluf", "summary", "message",
                    "judgment", "verdict_reason", "conclusion_reason", 
                    "justification", "evidence_analysis", "finding",
                    "key_judgment", "final_analysis", "assessment"
                ]
                for key in reason_keys:
                    if judge_result.get(key):
                        judge_result["reason"] = str(judge_result[key])
                        print(f"[JUDGE] Found reason in field '{key}'")
                        break
                        
                # N·∫øu v·∫´n kh√¥ng c√≥, th·ª≠ t√¨m trong nested objects
                if not judge_result.get("reason"):
                    nested_searches = [
                        ("executive_summary", ["bluf", "summary", "key_judgment", "message"]),
                        ("analysis", ["reasoning", "explanation", "summary", "text"]),
                        ("verdict_info", ["reason", "explanation", "analysis"]),
                        ("verdict_metadata", ["reason", "explanation", "temporal_reason"]),
                        ("dialectical_analysis", ["synthesis", "thesis", "antithesis"]),
                    ]
                    for nested_key, sub_keys in nested_searches:
                        nested = judge_result.get(nested_key)
                        if isinstance(nested, dict):
                            for key in sub_keys:
                                if nested.get(key):
                                    judge_result["reason"] = str(nested[key])
                                    print(f"[JUDGE] Found reason in '{nested_key}.{key}'")
                                    break
                            if judge_result.get("reason"):
                                break
                
                # FIX: Th·ª≠ l·∫•y t·ª´ temporal_analysis TR∆Ø·ªöC (fallback model th∆∞·ªùng tr·∫£ v·ªÅ field n√†y)
                if not judge_result.get("reason"):
                    temporal = judge_result.get("temporal_analysis")
                    if isinstance(temporal, dict):
                        # ∆Øu ti√™n currency_reason v√¨ ƒë√¢y l√† field ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong schema
                        for key in ["currency_reason", "reason", "explanation", "analysis", "currency_status"]:
                            val = temporal.get(key)
                            if val and isinstance(val, str) and len(val) > 5:
                                # Combine v·ªõi currency_status n·∫øu c√≥ ƒë·ªÉ t·∫°o reason ƒë·∫ßy ƒë·ªß h∆°n
                                currency_status = temporal.get("currency_status", "")
                                if key == "currency_reason":
                                    judge_result["reason"] = f"[{currency_status}] {val}" if currency_status else val
                                else:
                                    judge_result["reason"] = str(val)
                                print(f"[JUDGE] Found reason in 'temporal_analysis.{key}'")
                                break
                    elif isinstance(temporal, str) and len(temporal) > 20:
                        judge_result["reason"] = temporal
                        print(f"[JUDGE] Using 'temporal_analysis' string as reason")
                
                # N·∫øu v·∫´n kh√¥ng c√≥, d√πng wep_label + conclusion l√†m reason
                if not judge_result.get("reason"):
                    wep = judge_result.get("wep_label", "")
                    conclusion = judge_result.get("conclusion", "")
                    if wep:
                        judge_result["reason"] = f"ƒê√°nh gi√°: {wep}. K·∫øt lu·∫≠n: {conclusion}."
                        print(f"[JUDGE] Using wep_label as fallback reason")
                
                # Th·ª≠ l·∫•y b·∫•t k·ª≥ string field n√†o c√≥ ƒë·ªô d√†i > 50 l√†m reason
                if not judge_result.get("reason"):
                    for key, val in judge_result.items():
                        if isinstance(val, str) and len(val) > 50 and key not in ["conclusion", "text_input"]:
                            judge_result["reason"] = val
                            print(f"[JUDGE] Using field '{key}' as reason")
                            break
                
                # CH·ªà log DEBUG n·∫øu sau t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p v·∫´n kh√¥ng t√¨m ƒë∆∞·ª£c reason
                if not judge_result.get("reason"):
                    print(f"[JUDGE] DEBUG: Could not find reason after all attempts. Available keys: {list(judge_result.keys())}")
                    # Fallback cu·ªëi c√πng: t·∫°o reason t·ª´ conclusion
                    judge_result["reason"] = f"K·∫øt lu·∫≠n: {judge_result.get('conclusion', 'N/A')}. Xem b·∫±ng ch·ª©ng chi ti·∫øt b√™n d∆∞·ªõi."
            
            # 4. Log k·∫øt qu·∫£
            if judge_result.get("conclusion"):
                conf = judge_result.get("confidence_score")
                conf_str = f"{conf}%" if conf is not None else "N/A"
                print(f"[JUDGE] Round 1 (Flat Schema): {judge_result.get('conclusion')} ({conf_str})")
            else:
                # JSON parse ƒë∆∞·ª£c nh∆∞ng kh√¥ng c√≥ conclusion h·ª£p l·ªá
                print(f"[JUDGE] WARNING: JSON parsed but no valid conclusion found. Keys: {list(judge_result.keys())}")
                # FIX: LU√îN d√πng heuristic fallback khi kh√¥ng c√≥ conclusion
                print(f"[JUDGE] Fallback to heuristic analyzer...")
                return _heuristic_summarize(text_input, evidence_bundle, current_date)

        # ---------------------------------------------------------------------
    except Exception as e:
        print(f"[JUDGE] G·∫∑p l·ªói Round 1: {e}")
        return _heuristic_summarize(text_input, evidence_bundle, current_date)

    # =========================================================================
    # PHASE 3: SELF-CORRECTION (RE-SEARCH LOOP)
    # =========================================================================
    
    # FIX: Parse confidence an to√†n - default 50 (neutral) thay v√¨ 0 ƒë·ªÉ tr√°nh trigger re-search sai
    confidence = 50  # Neutral default
    raw_confidence = judge_result.get("confidence_score")
    if raw_confidence is not None:
        try:
            confidence = int(raw_confidence)
        except (ValueError, TypeError):
            confidence = 50  # Keep neutral if parse fails
            print(f"[SELF-CORRECTION] Warning: Could not parse confidence '{raw_confidence}', using default 50")
    else:
        print(f"[SELF-CORRECTION] Warning: No confidence_score in judge result, using default 50")
    
    # FIX: needs_more_evidence ph·∫£i l√† True EXPLICIT, kh√¥ng ph·∫£i ch·ªâ v√¨ confidence th·∫•p do parse l·ªói    
    needs_more = judge_result.get("needs_more_evidence", False)
    if not isinstance(needs_more, bool):
        needs_more = str(needs_more).lower() == "true"
    
    # K√≠ch ho·∫°t Re-search n·∫øu:
    # 1. Judge Y√äU C·∫¶U EXPLICIT (needs_more_evidence = True) - ∆∞u ti√™n cao nh·∫•t
    # 2. Ho·∫∑c Confidence < 40 (r·∫•t th·∫•p, kh√¥ng ph·∫£i do parse fail)
    # 3. V√Ä ch∆∞a ph·∫£i l√† tin th·ªùi ti·∫øt (th·ªùi ti·∫øt th∆∞·ªùng check 1 l·∫ßn l√† ƒë·ªß)
    # 4. V√Ä judge_result kh√¥ng r·ªóng (c√≥ k·∫øt qu·∫£ th·ª±c s·ª±)
    is_weather = "th·ªùi ti·∫øt" in judge_result.get("claim_type", "").lower()
    has_valid_result = bool(judge_result.get("conclusion"))
    
    # FIX: Ch·ªâ trigger re-search khi TH·ª∞C S·ª∞ c·∫ßn, kh√¥ng ph·∫£i do parse error
    should_research = (
        needs_more  # Judge y√™u c·∫ßu explicit
        or (confidence < 40 and has_valid_result)  # Confidence th·∫•p th·∫≠t s·ª±
    ) and not is_weather and has_valid_result
    
    if should_research:
        print(f"\n[SELF-CORRECTION] K√≠ch ho·∫°t Re-Search (Confidence: {confidence}%, Needs More: {needs_more}, Has Result: {has_valid_result})")
        
        new_queries = judge_result.get("additional_search_queries", [])
        if not new_queries:
            # Fallback n·∫øu Judge kh√¥ng ƒë∆∞a query
            new_queries = [f"{text_input} s·ª± th·∫≠t", f"{text_input} fact check"]
            
        print(f"[SELF-CORRECTION] Queries m·ªõi: {new_queries}")
        
        if new_queries:
            # Th·ª±c hi·ªán search b·ªï sung
            re_search_plan = {
                "required_tools": [{
                    "tool_name": "search",
                    "parameters": {"queries": new_queries}
                }]
            }
            
            # Execute search
            new_evidence = await execute_tool_plan(re_search_plan, site_query_string, flash_mode)
            
            # FIX: Safe initialization - ƒë·∫£m b·∫£o c√°c layer keys t·ªìn t·∫°i tr∆∞·ªõc khi merge
            for layer_key in ["layer_2_high_trust", "layer_3_general", "layer_4_social_low"]:
                if layer_key not in evidence_bundle:
                    evidence_bundle[layer_key] = []
                if not isinstance(evidence_bundle[layer_key], list):
                    evidence_bundle[layer_key] = []
            
            # Merge v√†o bundle c≈© (now safe)
            evidence_bundle["layer_2_high_trust"].extend(new_evidence.get("layer_2_high_trust", []))
            evidence_bundle["layer_3_general"].extend(new_evidence.get("layer_3_general", []))
            evidence_bundle["layer_4_social_low"].extend(new_evidence.get("layer_4_social_low", []))
            
            # Remove duplicates based on URL
            seen_urls = set()
            for layer in ["layer_2_high_trust", "layer_3_general", "layer_4_social_low"]:
                unique_items = []
                for item in evidence_bundle[layer]:
                    url = item.get("url")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        unique_items.append(item)
                evidence_bundle[layer] = unique_items
                
            print(f"[SELF-CORRECTION] ƒê√£ merge evidence m·ªõi. T·ªïng L2: {len(evidence_bundle['layer_2_high_trust'])}")
            
            # Re-Generate Critic (Nhanh) - Optional, but good for completeness
            # ƒê·ªÉ ti·∫øt ki·ªám th·ªùi gian, c√≥ th·ªÉ b·ªè qua Critic R2 ho·∫∑c ch·∫°y nhanh
            # ·ªû ƒë√¢y ta update l·∫°i Critic Report v·ªõi b·∫±ng ch·ª©ng m·ªõi
            evidence_bundle_json_v2 = json.dumps(_trim_evidence_bundle(evidence_bundle), indent=2, ensure_ascii=False)
            
            # Re-Run Judge Round 2
            print(f"[JUDGE] B·∫Øt ƒë·∫ßu ph√°n quy·∫øt Round 2 (Final)...")
            judge_prompt_filled_v2 = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
            judge_prompt_filled_v2 = judge_prompt_filled_v2.replace("{evidence_bundle_json}", evidence_bundle_json_v2)
            judge_prompt_filled_v2 = judge_prompt_filled_v2.replace("{current_date}", current_date)
            judge_prompt_filled_v2 += f"\n\n[√ù KI·∫æN BI·ªÜN L√ù (CRITIC - ROUND 1)]:\n{critic_report}\n(L∆∞u √Ω: B·∫±ng ch·ª©ng ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√™m sau v√≤ng 1)"
            
            # FIX: L∆∞u k·∫øt qu·∫£ Round 1 l√†m backup
            judge_result_r1_backup = judge_result.copy() if judge_result else {}
            
            try:
                judge_text_v2 = await call_agent_with_capability_fallback(
                    role="JUDGE",
                    prompt=judge_prompt_filled_v2,
                    temperature=0.1,
                    timeout=80.0
                )
                judge_result_r2 = _parse_json_from_text(judge_text_v2)
                
                # ---------------------------------------------------------------------
                # ADAPTER ROUND 2: Convert "Cognitive Architecture" JSON to Flat Schema
                # ---------------------------------------------------------------------
                verdict_meta = judge_result_r2.get("verdict_metadata")
                if verdict_meta:
                    # CONCLUSION
                    judge_result_r2["conclusion"] = verdict_meta.get("conclusion")
                    judge_result_r2["confidence_score"] = verdict_meta.get("probability_score")
                    
                    # REASON (Combine BLUF + Synthesis)
                    exec_summary = judge_result_r2.get("executive_summary") or {}
                    dialectical = judge_result_r2.get("dialectical_analysis") or {}
                    
                    bluf = exec_summary.get("bluf")
                    synthesis = dialectical.get("synthesis")
                    
                    combined_reason = ""
                    if bluf:
                        combined_reason += f"{bluf}\n\n"
                    if synthesis:
                        combined_reason += f"ANALYSIS: {synthesis}"
                    
                    judge_result_r2["reason"] = combined_reason.strip() or "No rationale provided."
                    
                    # DEBATE LOG
                    judge_result_r2["debate_log"] = {
                        "red_team_argument": dialectical.get("antithesis", "N/A"),
                        "blue_team_argument": dialectical.get("thesis", "N/A"),
                        "judge_reasoning": dialectical.get("synthesis", "N/A")
                    }
                    
                    # STYLE / WEP
                    judge_result_r2["style_analysis"] = verdict_meta.get("wep_label") or "N/A"
                    
                    # KEY EVIDENCE
                    citations = judge_result_r2.get("key_evidence_citations") or []
                    if citations and isinstance(citations, list) and len(citations) > 0:
                        first_cit = citations[0]
                        judge_result_r2["key_evidence_snippet"] = first_cit.get("quote") or "N/A"
                        judge_result_r2["key_evidence_source"] = first_cit.get("source") or "N/A"
                        judge_result_r2["evidence_link"] = first_cit.get("url") or ""
                        
                    print(f"[JUDGE] Round 2 (Cognitive Schema): {judge_result_r2.get('conclusion')} ({judge_result_r2.get('confidence_score')}%)")
                else:
                    # FIX: Handle FLAT SCHEMA for Round 2 (same logic as Round 1)
                    
                    # 1. T√¨m conclusion t·ª´ nhi·ªÅu field c√≥ th·ªÉ
                    if not judge_result_r2.get("conclusion"):
                        for key in ["final_conclusion", "verdict", "result", "classification"]:
                            if judge_result_r2.get(key):
                                judge_result_r2["conclusion"] = judge_result_r2[key]
                                break
                    
                    # 2. T√¨m confidence_score t·ª´ nhi·ªÅu field c√≥ th·ªÉ
                    if not judge_result_r2.get("confidence_score"):
                        for key in ["probability_score", "confidence", "score", "probability", "certainty"]:
                            val = judge_result_r2.get(key)
                            if val is not None:
                                try:
                                    judge_result_r2["confidence_score"] = int(val) if isinstance(val, (int, float)) else int(str(val).replace("%", ""))
                                except:
                                    pass
                                break
                    
                    # 3. T√¨m reason t·ª´ nhi·ªÅu field c√≥ th·ªÉ
                    if not judge_result_r2.get("reason"):
                        for key in ["reasoning", "explanation", "rationale", "analysis", "summary", "bluf"]:
                            if judge_result_r2.get(key):
                                judge_result_r2["reason"] = str(judge_result_r2[key])
                                break
                    
                    # 4. Log k·∫øt qu·∫£
                    if judge_result_r2.get("conclusion"):
                        conf = judge_result_r2.get("confidence_score")
                        conf_str = f"{conf}%" if conf is not None else "N/A"
                        print(f"[JUDGE] Round 2 (Flat Schema): {judge_result_r2.get('conclusion')} ({conf_str})")
                    else:
                        print(f"[JUDGE] WARNING Round 2: No valid conclusion. Keys: {list(judge_result_r2.keys())}")
                
                # FIX: Ch·ªâ s·ª≠ d·ª•ng Round 2 n·∫øu c√≥ k·∫øt qu·∫£ h·ª£p l·ªá
                if judge_result_r2.get("conclusion"):
                    judge_result = judge_result_r2
                    judge_result["cached"] = False
                    print(f"[JUDGE] K·∫øt qu·∫£ Round 2: {judge_result.get('conclusion')} ({judge_result.get('confidence_score')}%)")
                    
                    # FIX: ƒê·∫£m b·∫£o reason v√† evidence_link ƒë∆∞·ª£c copy t·ª´ R2
                    if not judge_result.get("reason"):
                        judge_result["reason"] = judge_result_r1_backup.get("reason", "Xem b·∫±ng ch·ª©ng b√™n d∆∞·ªõi.")
                    if not judge_result.get("evidence_link"):
                        judge_result["evidence_link"] = judge_result_r1_backup.get("evidence_link", "")
                else:
                    # Round 2 kh√¥ng c√≥ k·∫øt qu·∫£ h·ª£p l·ªá - gi·ªØ Round 1
                    print(f"[JUDGE] Round 2 failed to produce valid result. Keeping Round 1 result.")
                    judge_result = judge_result_r1_backup
                    
            except Exception as e:
                print(f"[JUDGE] L·ªói Round 2: {e}. Gi·ªØ nguy√™n k·∫øt qu·∫£ Round 1.")
                judge_result = judge_result_r1_backup  # FIX: Ensure we use backup
        else:
             print("[SELF-CORRECTION] Kh√¥ng c√≥ query m·ªõi, b·ªè qua Round 2.")

    # Post-processing normalization
    if judge_result:
        # Map old schema keys if needed (fallback)
        if "final_conclusion" in judge_result and "conclusion" not in judge_result:
            judge_result["conclusion"] = judge_result["final_conclusion"]
            
        judge_result["conclusion"] = normalize_conclusion(judge_result.get("conclusion"))
        return judge_result

    # Fallback final
    return _heuristic_summarize(text_input, evidence_bundle, current_date)
