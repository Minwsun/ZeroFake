# app/agent_synthesizer.py

import os
import json
import re
from dotenv import load_dotenv
from typing import Dict, Any, List

from app.weather import classify_claim
from app.model_clients import (
    call_gemini_model,
    call_groq_chat_completion,
    call_agent_with_capability_fallback,
    ModelClientError,
    RateLimitError,
)
from app.tool_executor import execute_tool_plan  # Import for Re-Search
from app.fact_check import call_google_fact_check, interpret_fact_check_rating, format_fact_check_evidence  # NEW: Fact Check API
from app.search_helper import quick_fact_check, search_google_news, search_wikipedia  # NEW: Direct search for JUDGE/CRITIC

load_dotenv()


GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SYNTHESIS_PROMPT = ""
CRITIC_PROMPT = ""  # NEW: Prompt cho CRITIC agent

# ==============================================================================
# COGNITIVE PIPELINE FLAGS - Quy trÃ¬nh tÆ° duy CRITIC-JUDGE
# ==============================================================================
# SEARCH FLAGS - Cho phÃ©p search khi THá»°C Sá»° cáº§n thiáº¿t
# ==============================================================================
ENABLE_CRITIC_SEARCH = False    # Táº®T - CRITIC chá»‰ tÆ° duy, khÃ´ng search thÃªm
ENABLE_COUNTER_SEARCH = False   # Táº®T - JUDGE chá»‰ tÆ° duy, khÃ´ng search thÃªm
ENABLE_SELF_CORRECTION = False  # Táº®T - KhÃ´ng cÃ³ UNIFIED-RE-SEARCH (tá»‘n thá»i gian)


# CÃ i Ä‘áº·t an toÃ n
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]


WEATHER_SOURCE_KEYWORDS = [
    "weather",
    "forecast",
    "accuweather",
    "windy",
    "meteoblue",
    "ventusky",
    "nchmf",
    "thoitiet",
    "openweathermap",
    "wunderground",
    "metoffice",
    "bom.gov",
]


# ==============================================================================
# SYNTH LOGIC: Äá»ƒ LLM tá»± phÃ¢n loáº¡i claim (khÃ´ng dÃ¹ng pattern cá»©ng)
# ==============================================================================

def _classify_claim_type(text_input: str) -> str:
    """
    SIMPLIFIED: KhÃ´ng dÃ¹ng pattern cá»©ng ná»¯a.
    Tráº£ vá» "AUTO" Ä‘á»ƒ LLM tá»± quyáº¿t Ä‘á»‹nh dá»±a trÃªn context.
    
    LLM sáº½ tá»± phÃ¢n loáº¡i:
    - KNOWLEDGE: Kiáº¿n thá»©c cá»‘ Ä‘á»‹nh (Ä‘á»‹a lÃ½, khoa há»c, Ä‘á»‹nh nghÄ©a)
    - NEWS: Tin tá»©c, sá»± kiá»‡n, tuyÃªn bá»‘
    
    NhÆ° váº­y há»‡ thá»‘ng sáº½ khÃ¡ch quan hÆ¡n vÃ  hoáº¡t Ä‘á»™ng trÃªn má»i trÆ°á»ng há»£p.
    """
    return "AUTO"


def normalize_conclusion(conclusion: str) -> str:
    """
    Normalize conclusion to BINARY classification: TIN THáº¬T or TIN GIáº¢ only.
    
    ğŸ”´ NGUYÃŠN Táº®C Má»šI: PRESUMPTION OF DOUBT
    - Máº·c Ä‘á»‹nh lÃ  TIN GIáº¢ náº¿u khÃ´ng chá»©ng minh Ä‘Æ°á»£c TIN THáº¬T
    - Chá»‰ tráº£ vá» TIN THáº¬T khi cÃ³ keywords chá»‰ Ä‘á»‹nh rÃµ rÃ ng
    """
    if not conclusion:
        return "TIN GIáº¢"  # Máº¶C Äá»ŠNH: KhÃ´ng cÃ³ káº¿t luáº­n = TIN GIáº¢
    
    conclusion_upper = conclusion.upper().strip()
    
    # ğŸŸ¢ CHá»ˆ TIN THáº¬T KHI CÃ“ Dáº¤U HIá»†U RÃ• RÃ€NG
    true_indicators = [
        # English true indicators
        "TRUE NEWS", "TRUE", "REAL", "VERIFIED", "CONFIRMED",
        # Vietnamese true indicators
        "TIN THáº¬T", "TIN THAT", "THáº¬T", "THAT", "ÄÃšNG", "DUNG",
        "XÃC NHáº¬N", "XAC NHAN", "CHÃNH XÃC", "CHINH XAC",
    ]
    
    for indicator in true_indicators:
        if indicator in conclusion_upper:
            return "TIN THáº¬T"
    
    # Máº¶C Äá»ŠNH: KhÃ´ng chá»©ng minh Ä‘Æ°á»£c TIN THáº¬T â†’ TIN GIáº¢
    # Bao gá»“m cáº£ cÃ¡c trÆ°á»ng há»£p: TIN GIáº¢, FAKE, FALSE, UNVERIFIED, etc.
    return "TIN GIáº¢"


# Some legacy fake indicators for reference (deprecated - logic Ä‘áº£o ngÆ°á»£c)
_DEPRECATED_FAKE_INDICATORS = [
    # English fake indicators (new prompts are in English)
    "FAKE NEWS", "FAKE", "FALSE", "UNTRUE", "NOT TRUE",
    # Vietnamese fake indicators
    "TIN GIáº¢", "TIN GIA", "GIáº¢ Máº O",
    "Bá»ŠA Äáº¶T", "BIA DAT", "Lá»ªA Äáº¢O", "LUA DAO", "SCAM",
]


# Product version database for outdated information detection
# Format: product_pattern -> (latest_version, release_year)
PRODUCT_VERSIONS = {
    # Apple iPhone (as of Dec 2025)
    r"iphone\s*(\d+)": {"latest": 17, "year": 2025, "name": "iPhone"},
    # Samsung Galaxy S
    r"galaxy\s*s\s*(\d+)": {"latest": 25, "year": 2025, "name": "Galaxy S"},
    # Samsung Galaxy Note
    r"galaxy\s*note\s*(\d+)": {"latest": 20, "year": 2020, "name": "Galaxy Note"},
    # Google Pixel
    r"pixel\s*(\d+)": {"latest": 9, "year": 2024, "name": "Pixel"},
    # PlayStation
    r"playstation\s*(\d+)|ps\s*(\d+)": {"latest": 5, "year": 2020, "name": "PlayStation"},
    # Xbox (Xbox One=1, Series X=2)
    r"xbox\s*series\s*([xs])": {"latest": "x", "year": 2020, "name": "Xbox Series"},
    # Windows
    r"windows\s*(\d+)": {"latest": 11, "year": 2021, "name": "Windows"},
    # macOS versions
    r"macos\s*(\d+)|mac\s*os\s*(\d+)": {"latest": 15, "year": 2024, "name": "macOS"},
    # MacBook chips
    r"macbook.*m(\d+)": {"latest": 4, "year": 2024, "name": "MacBook M-chip"},
}


def _detect_outdated_product(text_input: str) -> dict | None:
    """
    Detect if the input mentions an outdated product version.
    Returns dict with product info if outdated, None otherwise.
    """
    text_lower = text_input.lower()
    
    for pattern, info in PRODUCT_VERSIONS.items():
        match = re.search(pattern, text_lower)
        if match:
            # Get the version number from match groups
            version_str = None
            for group in match.groups():
                if group:
                    version_str = group
                    break
            
            if version_str:
                try:
                    # Handle numeric versions
                    if version_str.isdigit():
                        mentioned_version = int(version_str)
                        latest_version = info["latest"]
                        
                        if isinstance(latest_version, int) and mentioned_version < latest_version:
                            return {
                                "product": info["name"],
                                "mentioned_version": mentioned_version,
                                "latest_version": latest_version,
                                "latest_year": info["year"],
                                "is_outdated": True,
                                "years_behind": latest_version - mentioned_version
                            }
                except (ValueError, TypeError):
                    pass
    
    return None


# ==============================================================================
# TRUSTED SOURCE DETECTION - Reduce False Positive Rate
# ==============================================================================

TRUSTED_SOURCE_PREFIXES = [
    # International news agencies
    "theo reuters:", "reuters:", "theo ap:", "ap news:", "thÃ´ng tin tá»« ap:",
    "afp:", "theo afp:", 
    # Major broadcasters
    "bbc Ä‘Æ°a tin:", "bbc:", "cnn:", "theo cnn:",
    # Vietnamese trusted sources
    "theo vnexpress:", "vnexpress:", "tuá»•i tráº»:", "thanh niÃªn:", "dÃ¢n trÃ­:",
    "theo nguá»“n tin chÃ­nh thá»©c:", 
    # International newspapers
    "the guardian:", "new york times:", "washington post:", "the economist:",
]

def _has_trusted_source_citation(text: str) -> bool:
    """
    Check if claim begins with a trusted source citation.
    Claims with trusted source prefixes should be given benefit of the doubt.
    
    Returns True if text starts with a trusted source prefix.
    """
    if not text:
        return False
    text_lower = text.lower().strip()
    return any(text_lower.startswith(prefix) for prefix in TRUSTED_SOURCE_PREFIXES)


def _is_common_knowledge(text_input: str) -> bool:
    """
    Detect if the claim is about well-known, easily verifiable facts.
    These are facts that are widely accepted and don't need extensive verification.
    
    SOFT MATCHING: 70-80% match is OK for geographic/sports facts.
    """
    text_lower = text_input.lower()
    
    # ===========================================================================
    # CATEGORY 1: Tech/Company Facts (exact match)
    # ===========================================================================
    tech_patterns = [
        ("chatgpt", "openai"), ("gpt-4", "openai"), ("gpt-3", "openai"),
        ("google", "alphabet"), ("youtube", "google"),
        ("instagram", "meta"), ("whatsapp", "meta"), ("facebook", "meta"),
        ("iphone", "apple"), ("android", "google"),
        ("windows", "microsoft"), ("azure", "microsoft"), ("aws", "amazon"),
    ]
    
    for pattern in tech_patterns:
        if all(keyword in text_lower for keyword in pattern):
            return True
    
    # ===========================================================================
    # CATEGORY 2: Geographic/Population Facts (soft match - 70-80% OK)
    # ===========================================================================
    geo_facts = [
        # Vietnam
        ("viá»‡t nam", "hÃ  ná»™i", "thá»§ Ä‘Ã´"), ("vietnam", "hanoi", "capital"),
        ("viá»‡t nam", "63", "tá»‰nh"), ("viá»‡t nam", "tá»‰nh thÃ nh"),
        ("viá»‡t nam", "dÃ¢n sá»‘", "100"), ("viá»‡t nam", "triá»‡u ngÆ°á»i"),
        ("viá»‡t nam", "diá»‡n tÃ­ch"), ("viá»‡t nam", "kmÂ²"),
        ("viá»‡t nam", "giÃ¡p", "trung quá»‘c"), ("viá»‡t nam", "giÃ¡p", "lÃ o"),
        ("viá»‡t nam", "giÃ¡p", "campuchia"),
        ("fansipan", "cao nháº¥t"), ("fansipan", "3143"),
        ("mekong", "sÃ´ng"), ("mÃª kÃ´ng", "sÃ´ng"),
        # General geography
        ("trÃ¡i Ä‘áº¥t", "quay", "máº·t trá»i"),
        ("nÆ°á»›c", "sÃ´i", "100"), ("nÆ°á»›c sÃ´i", "Ä‘á»™"),
    ]
    
    for pattern in geo_facts:
        matches = sum(1 for kw in pattern if kw in text_lower)
        # Soft match: 70% of keywords is enough
        if matches >= len(pattern) * 0.7:
            return True
    
    # ===========================================================================
    # CATEGORY 3: Major Sports Events (soft match)
    # ===========================================================================
    sports_facts = [
        # World Cup
        ("argentina", "world cup", "2022"), ("messi", "world cup", "2022"),
        ("argentina", "vÃ´ Ä‘á»‹ch", "2022"), ("argentina", "world cup"),
        ("france", "world cup", "2018"), ("phÃ¡p", "world cup", "2018"),
        # Champions League
        ("real madrid", "champions league", "2024"),
        ("real madrid", "champions", "2024"),
        ("real madrid", "vÃ´ Ä‘á»‹ch", "champions"),
        ("inter", "serie a", "2024"), ("napoli", "serie a", "2023"),
        ("manchester city", "premier league"),
        # Transfers
        ("ronaldo", "al-nassr"), ("ronaldo", "al nassr"),
        ("messi", "inter miami"), ("messi", "barcelona"),
        # NBA
        ("nba", "mvp"), ("nba", "champion"),
        # Other sports
        ("taylor swift", "eras tour"),
        ("bts", "nghÄ©a vá»¥", "quÃ¢n sá»±"),
    ]
    
    for pattern in sports_facts:
        matches = sum(1 for kw in pattern if kw in text_lower)
        # Soft match: 70% of keywords is enough
        if matches >= len(pattern) * 0.7:
            return True
    
    # ===========================================================================
    # CATEGORY 4: Historical Events (known to AI)
    # ===========================================================================
    historical_facts = [
        ("facebook", "meta", "2021"),
        ("vinfast", "nasdaq", "2023"), ("vinfast", "ipo"),
        ("who", "covid", "kháº©n cáº¥p"), ("who", "pandemic"),
        ("alibaba", "chia tÃ¡ch"), ("alibaba", "split"),
        ("jimmy carter", "qua Ä‘á»i"), ("jimmy carter", "died"),
    ]
    
    for pattern in historical_facts:
        if all(keyword in text_lower for keyword in pattern):
            return True
    
    return False


def _detect_zombie_news(text_input: str, current_date: str) -> dict | None:
    """
    Detect ZOMBIE NEWS: News about past events presented as if they just happened.
    
    Examples:
    - "Viá»‡t Nam vÃ´ Ä‘á»‹ch AFF Cup 2018 Ä‘Ãªm qua" (AFF 2018 but "last night")
    - "Steve Jobs vá»«a qua Ä‘á»i" (Steve Jobs died in 2011)
    - "Samsung Galaxy Note 7 bá»‹ thu há»“i" (Note 7 was recalled in 2016)
    
    Returns dict with zombie news info if detected, None otherwise.
    """
    import re
    from datetime import datetime
    
    text_lower = text_input.lower()
    
    # Get current year from current_date or system
    try:
        if current_date and len(current_date) >= 4:
            current_year = int(current_date[:4])
        else:
            current_year = datetime.now().year
    except:
        current_year = datetime.now().year
    
    # Words indicating "just happened" / "breaking news" / "recent"
    recency_indicators = [
        "Ä‘Ãªm qua", "sÃ¡ng nay", "vá»«a", "má»›i", "hÃ´m nay", "hÃ´m qua", "tuáº§n nÃ y",
        "breaking", "nÃ³ng", "kháº©n cáº¥p", "má»›i nháº¥t", "cáº­p nháº­t", "tin sá»‘c",
        "vá»«a xáº£y ra", "vá»«a má»›i", "sÃ¡ng sá»›m", "chiá»u nay", "tá»‘i nay",
        "xem ngay", "share ngay", "chia sáº» ngay"
    ]
    
    has_recency_indicator = any(indicator in text_lower for indicator in recency_indicators)
    
    if not has_recency_indicator:
        return None
    
    # Pattern 1: Detect year in the text (e.g., "2018", "2019", etc.)
    # Only consider years that are significantly in the past (at least 1 year ago)
    year_pattern = re.search(r'\b(19\d{2}|20[0-2]\d)\b', text_input)
    if year_pattern:
        mentioned_year = int(year_pattern.group(1))
        years_ago = current_year - mentioned_year
        
        # If the mentioned year is at least 1 year ago, this is zombie news
        if years_ago >= 1:
            return {
                "is_zombie_news": True,
                "mentioned_year": mentioned_year,
                "current_year": current_year,
                "years_ago": years_ago,
                "recency_indicator": next((ind for ind in recency_indicators if ind in text_lower), "unknown")
            }
    
    # Pattern 2: Known past events database (famous events that can't "just happen")
    # These are events that definitively happened in the past and cannot happen again
    known_past_events = [
        # Deaths of famous people
        ("steve jobs", "qua Ä‘á»i", 2011),
        ("steve jobs", "died", 2011),
        ("michael jackson", "qua Ä‘á»i", 2009),
        ("michael jackson", "died", 2009),
        ("kobe bryant", "qua Ä‘á»i", 2020),
        ("kobe bryant", "died", 2020),
        
        # Product recalls/launches that are old
        ("galaxy note 7", "thu há»“i", 2016),
        ("galaxy note 7", "recall", 2016),
        ("galaxy note 7", "chÃ¡y ná»•", 2016),
        
        # Aviation incidents
        ("mh370", "máº¥t tÃ­ch", 2014),
        ("mh370", "missing", 2014),
        
        # Specific tournaments with years (AFF Cup 2018 was in past)
        # Sports events follow: {event} + {year} + recency = zombie
    ]
    
    for keywords_year in known_past_events:
        *keywords, event_year = keywords_year
        if all(kw in text_lower for kw in keywords):
            years_ago = current_year - event_year
            if years_ago >= 1:
                return {
                    "is_zombie_news": True,
                    "mentioned_year": event_year,
                    "current_year": current_year,
                    "years_ago": years_ago,
                    "recency_indicator": next((ind for ind in recency_indicators if ind in text_lower), "unknown"),
                    "known_event": " ".join(keywords)
                }
    return None


# NOTE: _detect_half_truth function REMOVED per user request
# System should be objective and rely on LLM reasoning, not hardcoded patterns
# Real-world scenarios are more complex than patterns can handle


def _is_weather_source(item: Dict[str, Any]) -> bool:
    source = (item.get("source") or item.get("url") or "").lower()
    if not source:
        return False
    return any(keyword in source for keyword in WEATHER_SOURCE_KEYWORDS)


def load_synthesis_prompt(prompt_path="prompts/synthesis_prompt.txt"):
    """Táº£i prompt cho Agent 2 (Synthesizer)"""
    global SYNTHESIS_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            SYNTHESIS_PROMPT = f.read()
        print("INFO: Táº£i Synthesis Prompt thÃ nh cÃ´ng.")
    except Exception as e:
        print(f"Lá»–I: khÃ´ng thá»ƒ táº£i {prompt_path}: {e}")
        raise


def load_critic_prompt(prompt_path="prompts/critic_prompt.txt"):
    """Táº£i prompt cho CRITIC agent (Devil's Advocate)"""
    global CRITIC_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            CRITIC_PROMPT = f.read()
        print("INFO: Táº£i CRITIC Prompt thÃ nh cÃ´ng.")
    except FileNotFoundError:
        # Fallback to default prompt if file not found
        CRITIC_PROMPT = (
            "Báº¡n lÃ  Biá»‡n lÃ½ Ä‘á»‘i láº­p (Devil's Advocate). "
            "HÃ£y chá»‰ ra 3 Ä‘iá»ƒm yáº¿u, mÃ¢u thuáº«n hoáº·c kháº£ nÄƒng Ä‘Ã¢y lÃ  tin cÅ©/satire/tin Ä‘á»“n. "
            "Chá»‰ tráº£ lá»i ngáº¯n gá»n, gay gáº¯t."
        )
        print(f"WARNING: KhÃ´ng tÃ¬m tháº¥y {prompt_path}, dÃ¹ng prompt máº·c Ä‘á»‹nh.")
    except Exception as e:
        print(f"Lá»–I: khÃ´ng thá»ƒ táº£i {prompt_path}: {e}")


def _parse_json_from_text(text: str) -> dict:
    """TrÃ­ch xuáº¥t JSON an toÃ n tá»« text tráº£ vá» cá»§a LLM - IMPROVED VERSION"""
    if not text:
        print("Lá»–I: Agent 2 (Synthesizer) khÃ´ng tÃ¬m tháº¥y JSON.")
        return {}

    cleaned = text.strip()
    
    # Remove <think>...</think> blocks (common in reasoning models)
    cleaned = re.sub(r'<think>.*?</think>', '', cleaned, flags=re.DOTALL)
    cleaned = cleaned.strip()
    
    # Remove Markdown code fences if present
    if cleaned.startswith("```"):
        cleaned = re.sub(r"^```[a-zA-Z0-9_-]*\s*", "", cleaned)
        cleaned = cleaned.rstrip("`").strip()
    
    # METHOD 1: Find JSON by balanced braces
    def find_json_object(s: str) -> str | None:
        start = s.find('{')
        if start == -1:
            return None
        depth = 0
        for i, c in enumerate(s[start:], start):
            if c == '{':
                depth += 1
            elif c == '}':
                depth -= 1
                if depth == 0:
                    return s[start:i+1]
        return None
    
    json_str = find_json_object(cleaned)
    if json_str:
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass  # Continue to fallback
    
    # METHOD 2: Try direct JSON load
    try:
        return json.loads(cleaned)
    except Exception:
        pass
    
    # METHOD 3: FALLBACK - Extract conclusion from raw text
    result = {}
    text_lower = cleaned.lower()
    
    # Extract conclusion - multiple patterns
    if "tin giáº£" in text_lower or "tin gáº£" in text_lower or '"conclusion": "tin giáº£"' in text_lower or "fake" in text_lower:
        result["conclusion"] = "TIN GIáº¢"
    elif "tin tháº­t" in text_lower or "tin that" in text_lower or '"conclusion": "tin tháº­t"' in text_lower or "true news" in text_lower:
        result["conclusion"] = "TIN THáº¬T"
    
    # Extract confidence from multiple patterns
    # Pattern 1: "confidence": 85, "confidence_score": 75, probability_score: 90
    conf_patterns = [
        r'(?:confidence|probability)[_\s]*(?:score)?["\s:]+(\d+)',
        r'"confidence_score"\s*:\s*(\d+)',
        r'"probability_score"\s*:\s*(\d+)',
        r'confidence[:\s]+(\d+)\s*%',
        r'(\d+)\s*%\s*(?:confidence|cháº¯c cháº¯n)',
    ]
    for pattern in conf_patterns:
        conf_match = re.search(pattern, text_lower)
        if conf_match:
            result["confidence_score"] = int(conf_match.group(1))
            break
    
    # If conclusion found but no confidence, default to 70
    if result.get("conclusion") and not result.get("confidence_score"):
        result["confidence_score"] = 70  # Default confidence
    
    # Extract reason
    reason_match = re.search(r'"reason"\s*:\s*"([^"]+)"', cleaned, re.IGNORECASE)
    if reason_match:
        result["reason"] = reason_match.group(1)
    
    if result.get("conclusion"):
        print(f"[JSON FALLBACK] Extracted from raw text: {result.get('conclusion')} ({result.get('confidence_score', 70)}%)")
        return result
    
    print(f"Lá»–I: Agent 2 (Synthesizer) khÃ´ng tÃ¬m tháº¥y JSON. Raw response: {cleaned[:300]}...")
    return {}


# Track if Fact Check API was used (only CRITIC OR JUDGE can use, not both)
_fact_check_used_by = None  # "CRITIC" or "JUDGE" or None


async def _agent_fact_check(agent_name: str, query: str) -> dict:
    """
    Allow CRITIC or JUDGE to call Fact Check API (only one can use per claim).
    Returns: {"used": bool, "results": list, "conclusion": str, "confidence": int}
    """
    global _fact_check_used_by
    
    # Only one agent can use fact check
    if _fact_check_used_by is not None:
        print(f"[FACT-CHECK] {agent_name} skipped - already used by {_fact_check_used_by}")
        return {"used": False, "results": [], "conclusion": "", "confidence": 0}
    
    print(f"[FACT-CHECK] {agent_name} calling Fact Check API for: {query[:50]}...")
    _fact_check_used_by = agent_name
    
    results = await call_google_fact_check(query)
    
    if results:
        # Find highest confidence result
        best_conclusion = ""
        best_confidence = 0
        for r in results:
            conclusion, confidence = interpret_fact_check_rating(r.get("rating", ""))
            if confidence > best_confidence:
                best_conclusion = conclusion
                best_confidence = confidence
        
        print(f"[FACT-CHECK] {agent_name} got {len(results)} results, best: {best_conclusion} ({best_confidence}%)")
        return {
            "used": True,
            "results": results,
            "conclusion": best_conclusion,
            "confidence": best_confidence,
            "evidence_text": format_fact_check_evidence(results)
        }
    
    return {"used": True, "results": [], "conclusion": "", "confidence": 0}


def _reset_fact_check_state():
    """Reset fact check usage tracking for new claim."""
    global _fact_check_used_by
    _fact_check_used_by = None


# ==============================================================================
# FILTER SEARCH RESULT - LLM-based evidence filtering
# ==============================================================================

FILTER_PROMPT = ""

# Cache for filter results (key: claim_hash, value: filtered_bundle)
_filter_cache = {}
_FILTER_CACHE_MAX_SIZE = 500  # Increased from 200 for better cache hit rate

def _get_claim_hash(claim: str, evidence_count: int) -> str:
    """Generate hash for caching filter results."""
    import hashlib
    cache_key = f"{claim.strip().lower()}_{evidence_count}"
    return hashlib.md5(cache_key.encode()).hexdigest()[:16]

def load_filter_prompt(prompt_path="prompts/filter_search_result.txt"):
    """Táº£i prompt cho Filter Search Result agent"""
    global FILTER_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            FILTER_PROMPT = f.read()
        print("INFO: Táº£i Filter Search Result Prompt thÃ nh cÃ´ng.")
    except FileNotFoundError:
        FILTER_PROMPT = (
            "Lá»c cÃ¡c káº¿t quáº£ tÃ¬m kiáº¿m. Giá»¯ láº¡i evidence liÃªn quan Ä‘áº¿n claim. "
            "Loáº¡i bá» spam, quáº£ng cÃ¡o, ná»™i dung khÃ´ng liÃªn quan. "
            "Tráº£ vá» JSON vá»›i filtered array."
        )
        print(f"WARNING: KhÃ´ng tÃ¬m tháº¥y {prompt_path}, dÃ¹ng prompt máº·c Ä‘á»‹nh.")
    except Exception as e:
        print(f"Lá»–I: khÃ´ng thá»ƒ táº£i {prompt_path}: {e}")


async def filter_evidence_with_llm(claim: str, evidence_bundle: dict, current_date: str) -> dict:
    """
    Use LLM to intelligently filter search results before passing to CRITIC/JUDGE.
    
    Model Priority:
    1. Llama 8B (Groq) - Fast & accurate
    2. Gemma 27B - High quality fallback
    3. Gemma 12B - Medium fallback
    4. Gemma 4B - Light fallback
    
    Input: Raw evidence bundle from search
    Output: Filtered evidence bundle (only useful evidence)
    """
    if not FILTER_PROMPT:
        load_filter_prompt()
    
    # Combine all evidence into a single list for filtering
    all_evidence = []
    
    # Layer 2: High trust sources
    for idx, item in enumerate(evidence_bundle.get("layer_2_high_trust", [])):
        all_evidence.append({
            "i": idx,  # Shortened key to save tokens
            "s": item.get("source", ""),
            "t": (item.get("snippet", "") or "")[:400],  # 400 chars for balanced info
        })
    
    # Layer 3: General sources
    l2_count = len(all_evidence)
    for idx, item in enumerate(evidence_bundle.get("layer_3_general", [])):
        all_evidence.append({
            "i": l2_count + idx,
            "s": item.get("source", ""),
            "t": (item.get("snippet", "") or "")[:400],  # 400 chars for balanced info
        })
    
    # Layer 4: Low trust sources (previously blocked)
    l3_count = len(all_evidence)
    for idx, item in enumerate(evidence_bundle.get("layer_4_social_low", [])):
        all_evidence.append({
            "i": l3_count + idx,
            "s": item.get("source", ""),
            "t": (item.get("snippet", "") or "")[:400],  # 400 chars for balanced info
        })
    
    if not all_evidence:
        print("[FILTER] No evidence to filter")
        return evidence_bundle
    
    l2_len = len(evidence_bundle.get("layer_2_high_trust", []))
    l3_len = len(evidence_bundle.get("layer_3_general", []))
    l4_len = len(evidence_bundle.get("layer_4_social_low", []))
    
    # Check cache first
    cache_key = _get_claim_hash(claim, len(all_evidence))
    if cache_key in _filter_cache:
        print(f"[FILTER] Cache HIT for {cache_key[:8]}... - returning cached result")
        return _filter_cache[cache_key]
    
    print(f"[FILTER] Input: {len(all_evidence)} items (L2={l2_len}, L3={l3_len}, L4={l4_len})")
    print(f"[FILTER] Goal: Remove duplicates, keep max 10 best items...")
    
    # Prepare prompt - compact format
    evidence_json = json.dumps(all_evidence, ensure_ascii=False, separators=(',', ':'))
    filter_prompt_filled = FILTER_PROMPT.replace("{claim}", claim)
    filter_prompt_filled = filter_prompt_filled.replace("{search_results}", evidence_json)
    
    filter_response = None
    model_used = None
    
    # Model cascade: Llama 8B (Groq) â†’ Gemma 4B (fastest fallback)
    # Reduced cascade for latency optimization
    models_to_try = [
        ("groq", "llama-3.1-8b-instant"),
        ("gemini", "models/gemma-3-4b-it"),  # Skip 27B/12B for speed
    ]
    
    for provider, model_name in models_to_try:
        try:
            print(f"[FILTER] Trying {provider}/{model_name}...")
            
            if provider == "groq":
                filter_response = await call_groq_chat_completion(
                    model_name=model_name,
                    prompt=filter_prompt_filled,
                    temperature=0.1,
                    timeout=15.0,  # Reduced from 30s
                )
            else:  # gemini
                filter_response = await call_gemini_model(
                    model_name=model_name,
                    prompt=filter_prompt_filled,
                    timeout=20.0,  # Reduced from 45s
                    safety_settings=SAFETY_SETTINGS
                )
            
            if filter_response:
                model_used = f"{provider}/{model_name}"
                print(f"[FILTER] Success with {model_used}")
                break
                
        except Exception as e:
            print(f"[FILTER] {provider}/{model_name} failed: {e}")
            continue
    
    if not filter_response:
        print("[FILTER] All models failed, returning original evidence")
        return evidence_bundle
    
    # Parse response - expect {"filtered": [{"i": 0, "s": "source", "info": "..."}, ...], "removed": [...]}
    try:
        filter_result = _parse_json_from_text(filter_response)
        
        if not filter_result:
            print("[FILTER] Failed to parse JSON, returning original evidence")
            return evidence_bundle
        
        # Get filtered items (support both "filtered" and "keep" keys)
        keep_items = filter_result.get("filtered", []) or filter_result.get("keep", [])
        removed_items = filter_result.get("removed", [])
        
        if not isinstance(keep_items, list):
            print("[FILTER] Invalid format, returning original evidence")
            return evidence_bundle
        
        # Log removed items
        if removed_items:
            print(f"[FILTER] REMOVED: {', '.join(str(r) for r in removed_items[:5])}{'...' if len(removed_items) > 5 else ''}")
        
        # Extract indices and log info
        keep_set = set()
        for item in keep_items:
            if isinstance(item, dict):
                idx = item.get("i")
                source = item.get("s", "")
                info = item.get("info", "") or item.get("r", "")  # Support both "info" and "r"
                if idx is not None:
                    keep_set.add(idx)
                    print(f"[FILTER] âœ“ #{idx} ({source}): {info[:60]}{'...' if len(info) > 60 else ''}")
            elif isinstance(item, (int, float)):
                # Fallback: plain index array
                keep_set.add(int(item))
        
        print(f"[FILTER] Keeping {len(keep_set)}/{len(all_evidence)} items")
        
        # Rebuild evidence bundle from filtered indices
        filtered_bundle = {
            "layer_1_tools": evidence_bundle.get("layer_1_tools", []),  # Keep weather data
            "layer_2_high_trust": [],
            "layer_3_general": [],
            "layer_4_social_low": [],
            "fact_check_verdict": evidence_bundle.get("fact_check_verdict"),  # Keep fact check
        }
        
        # Map filtered indices back to layers
        l2_items = evidence_bundle.get("layer_2_high_trust", [])
        l3_items = evidence_bundle.get("layer_3_general", [])
        l4_items = evidence_bundle.get("layer_4_social_low", [])
        
        for idx, item in enumerate(l2_items):
            if idx in keep_set:
                filtered_bundle["layer_2_high_trust"].append(item)
        
        l2_max = len(l2_items)
        for idx, item in enumerate(l3_items):
            if (l2_max + idx) in keep_set:
                filtered_bundle["layer_3_general"].append(item)
        
        l3_max = l2_max + len(l3_items)
        for idx, item in enumerate(l4_items):
            if (l3_max + idx) in keep_set:
                filtered_bundle["layer_4_social_low"].append(item)
        
        kept_total = (len(filtered_bundle["layer_2_high_trust"]) + 
                      len(filtered_bundle["layer_3_general"]) + 
                      len(filtered_bundle["layer_4_social_low"]))
        
        print(f"[FILTER] Result: L2={len(filtered_bundle['layer_2_high_trust'])}, "
              f"L3={len(filtered_bundle['layer_3_general'])}, "
              f"L4={len(filtered_bundle['layer_4_social_low'])} (total={kept_total})")
        
        # Save to cache (with size limit)
        if len(_filter_cache) >= _FILTER_CACHE_MAX_SIZE:
            # Remove oldest entry (first key)
            oldest_key = next(iter(_filter_cache))
            del _filter_cache[oldest_key]
        _filter_cache[cache_key] = filtered_bundle
        
        return filtered_bundle
        
    except Exception as e:
        print(f"[FILTER] Error parsing response: {e}")
        print("[FILTER] Returning original evidence bundle")
        return evidence_bundle

def _trim_snippet(s: str, max_len: int = 400) -> str:
    """
    Use 400 chars for balanced context.
    """
    if not s:
        return ""
    s = s.replace("\n", " ").strip()
    return s[:max_len]


def _trim_evidence_bundle(bundle: Dict[str, Any], cap_l2: int = 1000, cap_l3: int = 1000, cap_l4: int = 1000, claim_text: str = "") -> Dict[str, Any]:
    """
    OPTIMIZED: Filter evidence by relevance before capping.
    Only include evidence that mentions keywords from the claim.
    This reduces token waste on irrelevant search results.
    """
    if not bundle:
        return {"layer_1_tools": [], "layer_2_high_trust": [], "layer_3_general": [], "layer_4_social_low": []}
    
    # Extract keywords from claim for relevance filtering
    claim_keywords = set()
    if claim_text:
        # Extract words with 3+ chars, excluding common words
        stop_words = {"Ä‘Æ°á»£c", "trong", "vá»›i", "cá»§a", "cho", "ngÆ°á»i", "nhá»¯ng", "theo", "Ä‘ang", "sáº½", "Ä‘Ã£", "nÃ y", "cÃ¡c", "má»™t", "have", "been", "from", "with", "that", "this", "will", "the", "and", "for"}
        words = re.findall(r'\b\w{3,}\b', claim_text.lower())
        claim_keywords = {w for w in words if w not in stop_words}
    
    def is_relevant(item: Dict) -> bool:
        """
        Check if evidence snippet is TRULY relevant to the claim.
        IMPROVED: Requires at least 2 keyword matches OR 50% of keywords.
        This prevents false positives like "Bill Clinton" matching "Bill Gates".
        """
        if not claim_keywords:
            return True  # No filtering if no claim provided
        
        snippet = (item.get("snippet") or "").lower()
        title = (item.get("title") or "").lower()
        url = (item.get("url") or "").lower()
        combined = snippet + " " + title + " " + url
        
        # Count how many keywords match
        matched_keywords = [kw for kw in claim_keywords if kw in combined]
        match_count = len(matched_keywords)
        
        # STRICTER MATCHING:
        # - If claim has 3+ keywords: need at least 2 matches
        # - If claim has 1-2 keywords: need at least 1 match
        min_required = 2 if len(claim_keywords) >= 3 else 1
        
        return match_count >= min_required

    
    out = {
        "layer_1_tools": [],
        "layer_2_high_trust": [],
        "layer_3_general": [],
        "layer_4_social_low": []
    }
    
    # Lá»›p 1: OpenWeather API data (always include)
    for it in (bundle.get("layer_1_tools") or []):
        out["layer_1_tools"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date"),
            "weather_data": it.get("weather_data")
        })
    
    # Lá»›p 2: RE-ENABLED FILTER - Lá»c theo relevance Ä‘á»ƒ trÃ¡nh nháº§m láº«n (Bill Gates vs Bill Clinton)
    all_l2 = bundle.get("layer_2_high_trust") or []
    for it in all_l2[:cap_l2]:
        if is_relevant(it):
            out["layer_2_high_trust"].append({
                "source": it.get("source"),
                "url": it.get("url"),
                "snippet": _trim_snippet(it.get("snippet")),
                "rank_score": it.get("rank_score"),
                "date": it.get("date")
            })
    
    # Lá»›p 3: RE-ENABLED FILTER - Lá»c theo relevance
    all_l3 = bundle.get("layer_3_general") or []
    for it in all_l3[:cap_l3]:
        if is_relevant(it):
            out["layer_3_general"].append({
                "source": it.get("source"),
                "url": it.get("url"),
                "snippet": _trim_snippet(it.get("snippet")),
                "rank_score": it.get("rank_score"),
                "date": it.get("date")
            })
    
    # Lá»›p 4: RE-ENABLED FILTER - Lá»c theo relevance
    all_l4 = bundle.get("layer_4_social_low") or []
    for it in all_l4[:cap_l4]:
        if is_relevant(it):
            out["layer_4_social_low"].append({
                "source": it.get("source"),
                "url": it.get("url"),
                "snippet": _trim_snippet(it.get("snippet")),
                "rank_score": it.get("rank_score"),
                "date": it.get("date")
            })

    
    # Log sá»‘ lÆ°á»£ng evidence (khÃ´ng filter ná»¯a)
    total_evidence = len(all_l2) + len(all_l3) + len(all_l4)
    if total_evidence > 0:
        print(f"[EVIDENCE] Total: {total_evidence} items (L2={len(all_l2)}, L3={len(all_l3)}, L4={len(all_l4)})")
    else:
        print(f"[EVIDENCE] No evidence found")
    
    return out


def _as_str(x: Any) -> str:
    try:
        return x if isinstance(x, str) else ("" if x is None else str(x))
    except Exception:
        return ""


def _heuristic_summarize(text_input: str, bundle: Dict[str, Any], current_date: str) -> Dict[str, Any]:
    """
    Logic dá»± phÃ²ng khi LLM tháº¥t báº¡i.
    
    NGUYÃŠN Táº®C: PRESUMPTION OF TRUTH
    - Máº·c Ä‘á»‹nh lÃ  TIN THáº¬T náº¿u khÃ´ng cÃ³ báº±ng chá»©ng BÃC Bá»
    - Chá»‰ TIN GIáº¢ khi: evidence BÃC Bá» trá»±c tiáº¿p hoáº·c sáº£n pháº©m lá»—i thá»i
    """
    l1 = bundle.get("layer_1_tools") or []
    l2 = bundle.get("layer_2_high_trust") or []
    l3 = bundle.get("layer_3_general") or []

    try:
        claim = classify_claim(text_input)
    except Exception:
        claim = {"is_weather": False}

    is_weather_claim = claim.get("is_weather", False)
    text_lower = text_input.lower()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY 0: Sá»± tháº­t hiá»ƒn nhiÃªn (Common Knowledge)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if _is_common_knowledge(text_input):
        debate_log = {
            "red_team_argument": "TÃ´i khÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng bÃ¡c bá» sá»± tháº­t khoa há»c/ká»¹ thuáº­t nÃ y.",
            "blue_team_argument": "ÄÃ¢y lÃ  sá»± tháº­t Ä‘Ã£ Ä‘Æ°á»£c khoa há»c/cá»™ng Ä‘á»“ng cÃ´ng nháº­n rá»™ng rÃ£i.",
            "judge_reasoning": "Blue Team tháº¯ng. ÄÃ¢y lÃ  kiáº¿n thá»©c phá»• thÃ´ng Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c nháº­n."
        }
        return {
            "conclusion": "TIN THáº¬T",
            "confidence_score": 99,
            "reason": "ÄÃ¢y lÃ  sá»± tháº­t khoa há»c/ká»¹ thuáº­t Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng nháº­n rá»™ng rÃ£i.",
            "debate_log": debate_log,
            "key_evidence_snippet": "Kiáº¿n thá»©c phá»• thÃ´ng",
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "",
            "cached": False
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # NOTE: Pattern-based detection REMOVED for objectivity
    # LLM will decide based on evidence, not hardcoded patterns
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY 0.5: Trusted Source Citations (NEW - Reduce False Positive)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if _has_trusted_source_citation(text_input):
        # Check if evidence CONTRADICTS the claim
        combined_evidence = " ".join(
            [item.get("snippet", "") for item in l2 + l3]
        ).lower()
        
        # Only mark as fake if CONTRADICTING evidence found
        contradiction_keywords = ["sai sá»± tháº­t", "bÃ¡c bá»", "debunked", "fake", "false", "khÃ´ng chÃ­nh xÃ¡c", "incorrect"]
        has_contradiction = any(kw in combined_evidence for kw in contradiction_keywords)
        
        if not has_contradiction:
            # Extract source name from text
            source_match = None
            for prefix in TRUSTED_SOURCE_PREFIXES:
                if text_input.lower().strip().startswith(prefix):
                    source_match = prefix.replace("theo ", "").replace(":", "").replace("Ä‘Æ°a tin", "").strip().title()
                    break
            
            debate_log = {
                "red_team_argument": "TÃ´i khÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng cá»¥ thá»ƒ bÃ¡c bá» tin nÃ y.",
                "blue_team_argument": f"Claim trÃ­ch dáº«n nguá»“n uy tÃ­n ({source_match}). KhÃ´ng cÃ³ pháº£n chá»©ng â†’ nÃªn tin.",
                "judge_reasoning": f"Blue Team tháº¯ng. Claim cÃ³ nguá»“n {source_match} vÃ  khÃ´ng tÃ¬m tháº¥y pháº£n chá»©ng."
            }
            return {
                "conclusion": "TIN THáº¬T",
                "confidence_score": 90,  # Boosted from 75 to 90 for trusted sources
                "reason": f"Claim trÃ­ch dáº«n nguá»“n uy tÃ­n ({source_match}) vÃ  khÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng bÃ¡c bá».",
                "debate_log": debate_log,
                "key_evidence_snippet": f"Nguá»“n: {source_match}",
                "key_evidence_source": source_match or "",
                "evidence_link": "",
                "style_analysis": "Tin cÃ³ nguá»“n uy tÃ­n, Æ°u tiÃªn TIN THáº¬T",
                "cached": False
            }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY 2: PhÃ¡t hiá»‡n sáº£n pháº©m Lá»–I THá»œI (Outdated Product)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    outdated_info = _detect_outdated_product(text_input)
    if outdated_info and outdated_info.get("is_outdated"):
        product = outdated_info["product"]
        mentioned = outdated_info["mentioned_version"]
        latest = outdated_info["latest_version"]
        latest_year = outdated_info["latest_year"]
        
        # Build Adversarial Dialectic debate
        debate_log = {
            "red_team_argument": _as_str(
                f"ThÃ´ng tin nÃ y SAI! {product} {mentioned} lÃ  phiÃªn báº£n cÅ©. "
                f"Hiá»‡n táº¡i Ä‘Ã£ cÃ³ {product} {latest} (ra máº¯t nÄƒm {latest_year}). "
                f"Viá»‡c Ä‘Äƒng tin vá» {product} {mentioned} nhÆ° tin má»›i lÃ  SAI Sá»° THáº¬T."
            ),
            "blue_team_argument": _as_str(
                f"ÄÃºng lÃ  {product} {mentioned} Ä‘Ã£ ra máº¯t tháº­t. "
                f"Tuy nhiÃªn, Ä‘Ã¢y lÃ  thÃ´ng tin lá»—i thá»i. TÃ´i thá»«a nháº­n thua cuá»™c."
            ),
            "judge_reasoning": _as_str(
                f"Red Team tháº¯ng. {product} {mentioned} lÃ  phiÃªn báº£n cÅ©. "
                f"Hiá»‡n táº¡i Ä‘Ã£ cÃ³ {product} {latest}. Tin lá»—i thá»i = TIN GIáº¢."
            )
        }
        
        return {
            "conclusion": "TIN GIáº¢",
            "confidence_score": 95,
            "reason": _as_str(
                f"{product} {mentioned} Ä‘Ã£ lá»—i thá»i. "
                f"Hiá»‡n táº¡i Ä‘Ã£ cÃ³ {product} {latest} (nÄƒm {latest_year}). "
                f"Tin vá» sáº£n pháº©m cÅ© = TIN GIáº¢."
            ),
            "debate_log": debate_log,
            "key_evidence_snippet": _as_str(f"{product} {latest} ra máº¯t nÄƒm {latest_year}"),
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "ThÃ´ng tin lá»—i thá»i Ä‘Æ°á»£c trÃ¬nh bÃ y nhÆ° tin má»›i",
            "cached": False
        }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY 3: PhÃ¡t hiá»‡n ZOMBIE NEWS (tin cÅ© trÃ¬nh bÃ y nhÆ° tin má»›i)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    zombie_info = _detect_zombie_news(text_input, current_date)
    if zombie_info and zombie_info.get("is_zombie_news"):
        mentioned_year = zombie_info["mentioned_year"]
        years_ago = zombie_info["years_ago"]
        recency_indicator = zombie_info.get("recency_indicator", "vá»«a xáº£y ra")
        known_event = zombie_info.get("known_event", "")
        
        # Build Adversarial Dialectic debate
        debate_log = {
            "red_team_argument": _as_str(
                f"ÄÃ¢y lÃ  ZOMBIE NEWS! Sá»± kiá»‡n nÄƒm {mentioned_year} ({years_ago} nÄƒm trÆ°á»›c) "
                f"nhÆ°ng Ä‘Æ°á»£c trÃ¬nh bÃ y nhÆ° vá»«a xáº£y ra ('{recency_indicator}'). "
                f"ÄÃ¢y lÃ  thá»§ thuáº­t clickbait phá»• biáº¿n Ä‘á»ƒ lá»«a ngÆ°á»i Ä‘á»c."
            ),
            "blue_team_argument": _as_str(
                f"ÄÃºng lÃ  sá»± kiá»‡n nÄƒm {mentioned_year} Ä‘Ã£ xáº£y ra tháº­t. "
                f"NhÆ°ng viá»‡c dÃ¹ng ngÃ´n ngá»¯ '{recency_indicator}' lÃ  gÃ¢y hiá»ƒu láº§m. TÃ´i thua."
            ),
            "judge_reasoning": _as_str(
                f"Red Team tháº¯ng. Sá»± kiá»‡n nÄƒm {mentioned_year} KHÃ”NG THá»‚ '{recency_indicator}' Ä‘Æ°á»£c. "
                f"ÄÃ¢y lÃ  tin cÅ© Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng = ZOMBIE NEWS = TIN GIáº¢."
            )
        }
        
        return {
            "conclusion": "TIN GIáº¢",
            "confidence_score": 95,
            "reason": _as_str(
                f"ZOMBIE NEWS: Sá»± kiá»‡n nÄƒm {mentioned_year} ({years_ago} nÄƒm trÆ°á»›c) "
                f"Ä‘Æ°á»£c trÃ¬nh bÃ y nhÆ° vá»«a xáº£y ra ('{recency_indicator}'). "
                f"ÄÃ¢y lÃ  tin cÅ© Ä‘Æ°á»£c láº·p láº¡i Ä‘á»ƒ lá»«a ngÆ°á»i Ä‘á»c."
            ),
            "debate_log": debate_log,
            "key_evidence_snippet": _as_str(f"Sá»± kiá»‡n xáº£y ra nÄƒm {mentioned_year}, khÃ´ng pháº£i '{recency_indicator}'"),
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "ZOMBIE NEWS - Tin cÅ© trÃ¬nh bÃ y nhÆ° tin má»›i",
            "cached": False
        }

    # Æ¯u tiÃªn Lá»›p 1 (OpenWeather API) cho tin thá»i tiáº¿t
    if is_weather_claim and l1:
        weather_item = l1[0]
        weather_data = weather_item.get("weather_data", {})
        if weather_data:
            # So sÃ¡nh Ä‘iá»u kiá»‡n thá»i tiáº¿t
            main_condition = weather_data.get("main", "").lower()
            description = weather_data.get("description", "").lower()
            
            # Kiá»ƒm tra mÆ°a
            if "mÆ°a" in text_lower or "rain" in text_lower:
                if "rain" in main_condition or "rain" in description:
                    # Kiá»ƒm tra má»©c Ä‘á»™ mÆ°a
                    if "mÆ°a to" in text_lower or "mÆ°a lá»›n" in text_lower or "heavy rain" in text_lower:
                        if "heavy" in description or "torrential" in description:
                            return {
                                "conclusion": "TIN THáº¬T",
                                "reason": _as_str(f"Heuristic: OpenWeather API xÃ¡c nháº­n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}Â°C) cho {weather_data.get('location')} ngÃ y {weather_data.get('date')}."),
                                "style_analysis": "",
                                "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                                "key_evidence_source": _as_str(weather_item.get("source")),
                                "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                                "cached": False
                            }
                    else:
                        # MÆ°a thÆ°á»ng
                        return {
                            "conclusion": "TIN THáº¬T",
                            "reason": _as_str(f"Heuristic: OpenWeather API xÃ¡c nháº­n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}Â°C) cho {weather_data.get('location')} ngÃ y {weather_data.get('date')}."),
                            "style_analysis": "",
                            "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                            "key_evidence_source": _as_str(weather_item.get("source")),
                            "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                            "cached": False
                        }
            # Kiá»ƒm tra náº¯ng
            elif "náº¯ng" in text_lower or "sunny" in text_lower or "clear" in text_lower:
                if "clear" in main_condition or "sunny" in description:
                    return {
                        "conclusion": "TIN THáº¬T",
                        "reason": _as_str(f"Heuristic: OpenWeather API xÃ¡c nháº­n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}Â°C) cho {weather_data.get('location')} ngÃ y {weather_data.get('date')}."),
                        "style_analysis": "",
                        "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                        "key_evidence_source": _as_str(weather_item.get("source")),
                        "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                        "cached": False
                    }
            # Náº¿u khÃ´ng khá»›p Ä‘iá»u kiá»‡n cá»¥ thá»ƒ, váº«n tráº£ vá» dá»¯ liá»‡u tá»« OpenWeather
            return {
                "conclusion": "TIN THáº¬T",
                "reason": _as_str(f"Heuristic: OpenWeather API cung cáº¥p dá»¯ liá»‡u thá»i tiáº¿t {weather_item.get('source')} - {description} ({weather_data.get('temperature')}Â°C) cho {weather_data.get('location')} ngÃ y {weather_data.get('date')}."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                "key_evidence_source": _as_str(weather_item.get("source")),
                "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                "cached": False
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY 2: Kiá»ƒm tra nguá»“n L2 CÃ“ LIÃŠN QUAN Ä‘áº¿n claim
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ quan trá»ng tá»« claim Ä‘á»ƒ kiá»ƒm tra relevance
    person_keywords = []
    org_location_keywords = []
    
    # TÃ¬m tÃªn ngÆ°á»i (viáº¿t hoa, thÆ°á»ng lÃ  tá»« Ä‘áº§u tiÃªn)
    name_pattern = re.compile(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b')
    names = name_pattern.findall(text_input)
    person_keywords.extend([n.lower() for n in names])
    
    # TÃ¬m tÃªn tá»• chá»©c/CLB/Ä‘á»‹a Ä‘iá»ƒm
    org_patterns = [
        (r'clb\s+(\w+\s*\w*)', 'clb'),
        (r'fc\s+(\w+\s*\w*)', 'fc'),
        (r'Ä‘á»™i\s+(\w+\s*\w*)', 'Ä‘á»™i'),
    ]
    for pat, prefix in org_patterns:
        match = re.search(pat, text_lower)
        if match:
            org_location_keywords.append(match.group(1).strip())
    
    # ThÃªm cÃ¡c Ä‘á»‹a danh phá»• biáº¿n
    location_names = ["hÃ  ná»™i", "ha noi", "hanoi", "sÃ i gÃ²n", "saigon", "ho chi minh", 
                      "viá»‡t nam", "vietnam", "barca", "barcelona", "inter miami", "real madrid"]
    for loc in location_names:
        if loc in text_lower:
            org_location_keywords.append(loc)
    
    # Kiá»ƒm tra L2 sources cÃ³ liÃªn quan THá»°C Sá»° khÃ´ng
    # Äá»‘i vá»›i claim vá» ngÆ°á»i + tá»• chá»©c: Cáº¦N KHá»šP Cáº¢ HAI
    relevant_l2 = []
    has_person_org_claim = len(person_keywords) > 0 and len(org_location_keywords) > 0
    
    for item in l2:
        snippet = (item.get("snippet") or "").lower()
        title = (item.get("title") or "").lower()
        combined = snippet + " " + title
        
        if has_person_org_claim:
            # Claim cÃ³ cáº£ ngÆ°á»i + tá»• chá»©c -> cáº§n khá»›p Cáº¢ HAI
            has_person = any(kw in combined for kw in person_keywords if kw and len(kw) > 2)
            has_org = any(kw in combined for kw in org_location_keywords if kw and len(kw) > 2)
            
            if has_person and has_org:
                relevant_l2.append(item)
        else:
            # Claim Ä‘Æ¡n giáº£n -> chá»‰ cáº§n khá»›p 1 keyword
            is_relevant = False
            all_keywords = person_keywords + org_location_keywords
            for kw in all_keywords:
                if kw and len(kw) > 2 and kw in combined:
                    is_relevant = True
                    break
            if is_relevant:
                relevant_l2.append(item)
    
    # Giáº£m yÃªu cáº§u tá»« 2 xuá»‘ng 1: Chá»‰ cáº§n 1 nguá»“n uy tÃ­n LIÃŠN QUAN THá»°C Sá»° Ä‘á»ƒ há»— trá»£ TIN THáº¬T
    if len(relevant_l2) >= 1:
        top = relevant_l2[0]
        return {
            "conclusion": "TIN THáº¬T",
            "debate_log": {
                "red_team_argument": "TÃ´i khÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng bÃ¡c bá».",
                "blue_team_argument": _as_str(f"CÃ³ Ã­t nháº¥t 1 nguá»“n uy tÃ­n xÃ¡c nháº­n: {top.get('source')}."),
                "judge_reasoning": "Blue Team tháº¯ng vá»›i báº±ng chá»©ng tá»« nguá»“n uy tÃ­n."
            },
            "confidence_score": 85,
            "reason": _as_str(f"CÃ³ nguá»“n uy tÃ­n xÃ¡c nháº­n thÃ´ng tin nÃ y ({top.get('source')})."),
            "style_analysis": "",
            "key_evidence_snippet": _as_str(top.get("snippet")),
            "key_evidence_source": _as_str(top.get("source")),
            "evidence_link": _as_str(top.get("url") or top.get("link")),
            "cached": False
        }
    
    # ÄÃƒ XÃ“A: Block Ä‘Ã¡nh TIN GIáº¢ khi "cÃ³ L2 nhÆ°ng khÃ´ng liÃªn quan"
    # ÄÃ¢y lÃ  logic SAI: KhÃ´ng cÃ³ evidence â‰  Tin giáº£
    # Theo IFCN: Presumption of Truth - chá»‰ TIN GIáº¢ khi cÃ³ Báº°NG CHá»¨NG BÃC Bá»


    if is_weather_claim and l2:
        weather_sources = [item for item in l2 if _is_weather_source(item)]
        if weather_sources:
            top = weather_sources[0]
            return {
                "conclusion": "TIN THáº¬T",
                "reason": _as_str(f"Heuristic (weather): Dá»±a trÃªn nguá»“n dá»± bÃ¡o thá»i tiáº¿t {top.get('source')} ({top.get('date') or 'N/A'})."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(top.get("snippet")),
                "key_evidence_source": _as_str(top.get("source")),
                "evidence_link": _as_str(top.get("url") or top.get("link")),
                "cached": False
            }

    if is_weather_claim:
        layer3 = bundle.get("layer_3_general") or []
        weather_layer3 = [item for item in layer3 if _is_weather_source(item)]
        if weather_layer3:
            top = weather_layer3[0]
            return {
                "conclusion": "TIN THáº¬T",
                "reason": _as_str(f"Heuristic (weather): Dá»±a trÃªn trang dá»± bÃ¡o {top.get('source')} cho Ä‘á»‹a Ä‘iá»ƒm Ä‘Æ°á»£c nÃªu."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(top.get("snippet")),
                "key_evidence_source": _as_str(top.get("source")),
                "evidence_link": _as_str(top.get("url") or top.get("link")),
                "cached": False
            }

    # PhÃ¡t hiá»‡n thÃ´ng tin gÃ¢y hiá»ƒu láº§m do Ä‘Ã£ cÅ© (Ä‘áº·c biá»‡t vá»›i sáº£n pháº©m/phiÃªn báº£n)
    if not is_weather_claim:
        evidence_items = l2 + l3
        old_items = [item for item in evidence_items if item.get("is_old")]
        fresh_items = [item for item in evidence_items if item.get("is_old") is False]

        marketing_keywords = [
            "giáº£m giÃ¡", "khuyáº¿n mÃ£i", "sale", "ra máº¯t", "má»Ÿ bÃ¡n", "Ä‘áº·t trÆ°á»›c",
            "phiÃªn báº£n", "model", "tháº¿ há»‡", "Ä‘á»i", "nÃ¢ng cáº¥p", "lÃªn ká»‡", "Æ°u Ä‘Ã£i",
            "launch", "promotion"
        ]
        product_pattern = re.compile(r"(iphone|ipad|macbook|galaxy|pixel|surface|playstation|xbox|sony|samsung|apple|oppo|xiaomi|huawei|vinfast)\s?[0-9a-z]{1,4}", re.IGNORECASE)
        mentions_product_cycle = any(kw in text_lower for kw in marketing_keywords) or bool(product_pattern.search(text_input))

        if old_items and (fresh_items or mentions_product_cycle):
            reference_old = old_items[0]
            old_source = reference_old.get("source") or reference_old.get("url") or "nguá»“n cÅ©"
            old_date = reference_old.get("date") or "trÆ°á»›c Ä‘Ã¢y"
            latest_snippet = _as_str(reference_old.get("snippet"))

            if fresh_items:
                latest_item = fresh_items[0]
                latest_source = latest_item.get("source") or latest_item.get("url") or "nguá»“n má»›i"
                latest_date = latest_item.get("date") or "gáº§n Ä‘Ã¢y"
                reason = _as_str(
                    f"ThÃ´ng tin vá» '{text_input}' dá»±a trÃªn nguá»“n {old_source} ({old_date}) Ä‘Ã£ cÅ©, "
                    f"trong khi cÃ¡c nguá»“n má»›i nhÆ° {latest_source} ({latest_date}) cho tháº¥y bá»‘i cáº£nh Ä‘Ã£ thay Ä‘á»•i. "
                    "Viá»‡c trÃ¬nh bÃ y nhÆ° tin nÃ³ng dá»… gÃ¢y hiá»ƒu láº§m."
                )
            else:
                reason = _as_str(
                    f"ThÃ´ng tin vá» '{text_input}' chá»‰ Ä‘Æ°á»£c há»— trá»£ bá»Ÿi nguá»“n cÅ© {old_source} ({old_date}). "
                    "Sáº£n pháº©m/sá»± kiá»‡n nÃ y Ä‘Ã£ xuáº¥t hiá»‡n tá»« lÃ¢u nÃªn viá»‡c trÃ¬nh bÃ y nhÆ° tin tá»©c má»›i lÃ  gÃ¢y hiá»ƒu láº§m."
                )

            return {
                "conclusion": "TIN GIáº¢",
                "reason": reason,
                "style_analysis": "Tin lá»—i thá»i",
                "key_evidence_snippet": latest_snippet,
                "key_evidence_source": _as_str(old_source),
                "evidence_link": _as_str(reference_old.get("url") or reference_old.get("link")),
                "cached": False
            }

        if mentions_product_cycle and fresh_items and not old_items:
            latest_item = fresh_items[0]
            latest_source = latest_item.get("source") or latest_item.get("url") or "nguá»“n má»›i"
            latest_date = latest_item.get("date") or "gáº§n Ä‘Ã¢y"
            reason = _as_str(
                f"KhÃ´ng tÃ¬m tháº¥y nguá»“n gáº§n Ä‘Ã¢y xÃ¡c nháº­n '{text_input}', trong khi cÃ¡c sáº£n pháº©m má»›i hÆ¡n Ä‘Ã£ xuáº¥t hiá»‡n "
                f"(vÃ­ dá»¥ {latest_source}, {latest_date}). ÄÃ¢y lÃ  thÃ´ng tin cÅ© Ä‘Æ°á»£c láº·p láº¡i khiáº¿n ngÆ°á»i Ä‘á»c hiá»ƒu láº§m bá»‘i cáº£nh hiá»‡n táº¡i."
            )
            return {
                "conclusion": "TIN GIáº¢",
                "reason": reason,
                "style_analysis": "Tin lá»—i thá»i",
                "key_evidence_snippet": _as_str(latest_item.get("snippet")),
                "key_evidence_source": _as_str(latest_source),
                "evidence_link": _as_str(latest_item.get("url") or latest_item.get("link")),
                "cached": False
            }

        claim_implies_present = any(
            kw in text_lower
            for kw in [
                "hiá»‡n nay", "bÃ¢y giá»", "Ä‘ang", "sáº¯p", "vá»«a", "today", "now", "currently",
                "má»›i Ä‘Ã¢y", "ngay lÃºc nÃ y", "trong thá»i gian tá»›i"
            ]
        )
        if claim_implies_present and old_items and not fresh_items:
            old_item = old_items[0]
            older_source = old_item.get("source") or old_item.get("url") or "nguá»“n cÅ©"
            older_date = old_item.get("date") or "trÆ°á»›c Ä‘Ã¢y"
            reason = _as_str(
                f"'{text_input}' Ã¡m chá»‰ thÃ´ng tin Ä‘ang diá»…n ra nhÆ°ng chá»‰ cÃ³ nguá»“n {older_source} ({older_date}) tá»« trÆ°á»›c kia. "
                "Viá»‡c dÃ¹ng láº¡i tin cÅ© khiáº¿n ngÆ°á»i Ä‘á»c hiá»ƒu sai vá» tÃ¬nh tráº¡ng hiá»‡n táº¡i."
            )
            return {
                "conclusion": "TIN GIáº¢",
                "reason": reason,
                "style_analysis": "Tin lá»—i thá»i",
                "key_evidence_snippet": _as_str(old_item.get("snippet")),
                "key_evidence_source": _as_str(older_source),
                "evidence_link": _as_str(old_item.get("url") or old_item.get("link")),
                "cached": False
            }

        misleading_tokens = [
            "Ä‘Ã£ káº¿t thÃºc", "Ä‘Ã£ dá»«ng", "ngá»«ng Ã¡p dá»¥ng", "khÃ´ng cÃ²n Ã¡p dá»¥ng",
            "Ä‘Ã£ há»§y", "Ä‘Ã£ hoÃ£n", "Ä‘Ã£ Ä‘Ã³ng", "Ä‘Ã£ ngÆ°ng", "no longer", "ended", "discontinued"
        ]
        for item in evidence_items:
            snippet_lower = (item.get("snippet") or "").lower()
            if any(token in snippet_lower for token in misleading_tokens):
                source = item.get("source") or item.get("url") or "nguá»“n cáº­p nháº­t"
                reason = _as_str(
                    f"'{text_input}' bá» qua cáº­p nháº­t tá»« {source} cho biáº¿t sá»± kiá»‡n/chÆ°Æ¡ng trÃ¬nh Ä‘Ã£ káº¿t thÃºc hoáº·c thay Ä‘á»•i "
                    "nÃªn thÃ´ng tin dá»… gÃ¢y hiá»ƒu láº§m."
                )
                return {
                    "conclusion": "TIN GIáº¢",
                    "reason": reason,
                    "style_analysis": "Tin Ä‘Ã£ khÃ´ng cÃ²n Ä‘Ãºng",
                    "key_evidence_snippet": _as_str(item.get("snippet")),
                    "key_evidence_source": _as_str(source),
                    "evidence_link": _as_str(item.get("url") or item.get("link")),
                    "cached": False
                }

    # FIX: Máº·c Ä‘á»‹nh TIN THáº¬T khi khÃ´ng cÃ³ báº±ng chá»©ng BÃC Bá» (innocent until proven guilty)
    # TrÆ°á»›c Ä‘Ã¢y máº·c Ä‘á»‹nh TIN GIáº¢ gÃ¢y false positive cao
    return {
        "conclusion": "TIN THáº¬T",
        "confidence_score": 60,
        "reason": _as_str("KhÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng BÃC Bá» thÃ´ng tin nÃ y. Dá»±a trÃªn nguyÃªn táº¯c 'innocent until proven guilty'."),
        "debate_log": {
            "red_team_argument": "KhÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng pháº£n bÃ¡c rÃµ rÃ ng.",
            "blue_team_argument": "KhÃ´ng cÃ³ nguá»“n nÃ o bÃ¡c bá» thÃ´ng tin nÃ y.",
            "judge_reasoning": "Khi khÃ´ng cÃ³ báº±ng chá»©ng bÃ¡c bá», tin Ä‘Æ°á»£c coi lÃ  cÃ³ thá»ƒ Ä‘Ãºng."
        },
        "style_analysis": "",
        "key_evidence_snippet": "",
        "key_evidence_source": "",
        "evidence_link": "",
        "cached": False
    }


def _normalize_agent2_model(model_key: str | None) -> str:
    """Normalize Agent 2 model identifier."""
    if not model_key:
        return "models/gemini-2.5-pro"
    mapping = {
        "gemini_flash": "models/gemini-2.5-flash",
        "gemini flash": "models/gemini-2.5-flash",
        "gemini-2.5-flash": "models/gemini-2.5-flash",
        "models/gemini_flash": "models/gemini-2.5-flash",
        "gemini_pro": "models/gemini-2.5-pro",
        "gemini pro": "models/gemini-2.5-pro",
        "models/gemini-2.5-pro": "models/gemini-2.5-pro",
        "openai/gpt-oss-120b": "openai/gpt-oss-120b",
        "meta-llama/llama-3.3-70b-instruct": "meta-llama/llama-3.3-70b-instruct",
        "qwen/qwen-2.5-72b-instruct": "qwen/qwen-2.5-72b-instruct",
        "gemma-3-1b": "models/gemma-3-1b-it",
        "gemma-3-1b-it": "models/gemma-3-1b-it",
        "gemma-3-2b": "models/gemma-3-4b-it",  # 2B not available, fallback to 4B
        "gemma-3-4b": "models/gemma-3-4b-it",
        "gemma-3-4b-it": "models/gemma-3-4b-it",
        "gemma-3-12b": "models/gemma-3-12b-it",
        "gemma-3-12b-it": "models/gemma-3-12b-it",
        "gemma-3-27b": "models/gemma-3-27b-it",
        "gemma-3-27b-it": "models/gemma-3-27b-it",
        "google/gemma-3-1b": "models/gemma-3-1b-it",
        "google/gemma-3-2b": "models/gemma-3-4b-it",
        "google/gemma-3-4b": "models/gemma-3-4b-it",
        "google/gemma-3-12b": "models/gemma-3-12b-it",
        "google/gemma-3-27b": "models/gemma-3-27b-it",
        "models/gemma-3-1b": "models/gemma-3-1b-it",
        "models/gemma-3-2b": "models/gemma-3-4b-it",
        "models/gemma-3-4b": "models/gemma-3-4b-it",
        "models/gemma-3-12b": "models/gemma-3-12b-it",
        "models/gemma-3-27b": "models/gemma-3-27b-it",
        "models/gemma-3-1b-it": "models/gemma-3-1b-it",
        "models/gemma-3-4b-it": "models/gemma-3-4b-it",
        "models/gemma-3-12b-it": "models/gemma-3-12b-it",
        "models/gemma-3-27b-it": "models/gemma-3-27b-it",
        "models/gemma-3n-e2b-it": "models/gemma-3n-e2b-it",
        "models/gemma-3n-e4b-it": "models/gemma-3n-e4b-it",
    }
    return mapping.get(model_key, model_key)


def _detect_agent2_provider(model_name: str) -> str:
    """Detect provider for Agent 2 model."""
    if not model_name:
        return "gemini"
    lowered = model_name.lower()
    if "gemini" in lowered or "gemma" in lowered or model_name.startswith("models/"):
        return "gemini"
    # All Agent 2 models now use Gemini API
    return "gemini"

async def execute_final_analysis(
    text_input: str,
    evidence_bundle: dict,
    current_date: str,
    model_key: str | None = None,
    flash_mode: bool = False,
    site_query_string: str = "",  # Added for re-search
    skip_critic: bool = False,    # NEW: Skip CRITIC when Fact Check has verdict
) -> dict:
    """
    Pipeline: SYNTH â†’ CRITIC (optional) â†’ JUDGE
    
    When skip_critic=True (Fact Check has verdict):
    - Skip CRITIC phase
    - Go directly to JUDGE with Fact Check + Search evidence
    
    SYNTH Logic:
    - KNOWLEDGE claims: Agent cÃ³ quyá»n tá»± quyáº¿t dá»±a trÃªn kiáº¿n thá»©c
    - NEWS claims: Báº¯t buá»™c pháº£i cÃ³ evidence
    
    Optimizations applied:
    - Reduced evidence bundle size (3/3/2 items)
    - Reduced snippet length (200 chars)
    - Reduced timeouts (30s/40s)
    - Simplified prompts
    """
    if not SYNTHESIS_PROMPT:
        raise ValueError("Synthesis prompt (prompt 2) chÆ°a Ä‘Æ°á»£c táº£i.")
    if not CRITIC_PROMPT:
        print("WARNING: Critic prompt chÆ°a Ä‘Æ°á»£c táº£i, dÃ¹ng máº·c Ä‘á»‹nh.")

    # Reset fact check state for new claim (only CRITIC or JUDGE can use, not both)
    _reset_fact_check_state()

    # =========================================================================
    # PHASE 0: FILTER EVIDENCE vá»›i Gemma 12B
    # Lá»c thÃ´ng minh cÃ¡c káº¿t quáº£ tÃ¬m kiáº¿m trÆ°á»›c khi Ä‘Æ°a cho CRITIC/JUDGE
    # =========================================================================
    print(f"\n[PIPELINE] Phase 0: Filtering evidence with Gemma 12B...")
    filtered_evidence_bundle = await filter_evidence_with_llm(text_input, evidence_bundle, current_date)

    # =========================================================================
    # SYNTH: Äá»ƒ LLM tá»± phÃ¢n loáº¡i claim (khÃ´ng dÃ¹ng pattern cá»©ng)
    # =========================================================================
    claim_type = _classify_claim_type(text_input)
    print(f"\n[SYNTH] Claim type: {claim_type}")
    
    # AUTO: Äá»ƒ LLM tá»± quyáº¿t Ä‘á»‹nh dá»±a trÃªn context
    synth_instruction = (
        "\n\n[SYNTH INSTRUCTION]\n"
        "HÃ£y Tá»° PHÃ‚N LOáº I claim nÃ y:\n"
        "- KNOWLEDGE: Kiáº¿n thá»©c cá»‘ Ä‘á»‹nh (Ä‘á»‹a lÃ½, khoa há»c, Ä‘á»‹nh nghÄ©a) â†’ CÃ³ thá»ƒ tá»± suy luáº­n\n"
        "- NEWS: Tin tá»©c, sá»± kiá»‡n, tuyÃªn bá»‘ â†’ Cáº§n evidence\n\n"
        "Sau Ä‘Ã³ Ã¡p dá»¥ng:\n"
        "- Náº¿u KNOWLEDGE: Tá»± quyáº¿t dá»±a trÃªn kiáº¿n thá»©c ná»™i táº¡i\n"
        "- Náº¿u NEWS: Báº¯t buá»™c cÃ³ evidence Ä‘á»ƒ káº¿t luáº­n\n"
        "- Náº¿u khÃ´ng cÃ³ evidence bÃ¡c bá» â†’ PRESUMPTION OF TRUTH (TIN THáº¬T)\n"
    )
    print(f"[SYNTH] LLM sáº½ tá»± phÃ¢n loáº¡i vÃ  quyáº¿t Ä‘á»‹nh")

    # Trim evidence before sending to models (using FILTERED bundle)
    trimmed_bundle = _trim_evidence_bundle(filtered_evidence_bundle, claim_text=text_input)
    
    # DEBUG: Log weather data
    weather_items = trimmed_bundle.get("layer_1_tools", [])
    if weather_items:
        print(f"[WEATHERâ†’JUDGE] Found {len(weather_items)} weather items in evidence:")
        for item in weather_items:
            print(f"  â†’ {item.get('source')}: {item.get('snippet', '')[:100]}...")
    
    evidence_bundle_json = json.dumps(trimmed_bundle, indent=2, ensure_ascii=False)

    # =========================================================================
    # PHASE 1: CRITIC AGENT (BIá»†N LÃ Äá»I Láº¬P)
    # Skip if Fact Check already has verdict - use Fact Check as "critic"
    # =========================================================================
    critic_report = "KhÃ´ng cÃ³ pháº£n biá»‡n."
    critic_parsed = {}
    
    # Check if Fact Check has verdict (preserved in filtered bundle)
    fact_check_verdict = filtered_evidence_bundle.get("fact_check_verdict")
    
    if skip_critic and fact_check_verdict:
        # Use Fact Check verdict as the CRITIC report
        fc_conclusion = fact_check_verdict.get("conclusion", "")
        fc_confidence = fact_check_verdict.get("confidence", 0)
        fc_source = fact_check_verdict.get("source", "Fact Check")
        fc_url = fact_check_verdict.get("url", "")
        
        print(f"\n[SKIP CRITIC] Using Fact Check verdict from {fc_source}")
        critic_report = f"""[FACT CHECK VERDICT]
Source: {fc_source}
Verdict: {fc_conclusion} (Confidence: {fc_confidence}%)
URL: {fc_url}

This claim has been fact-checked by {fc_source}. The verdict is {fc_conclusion}."""
        
        critic_parsed = {
            "issues_found": fc_conclusion == "TIN GIáº¢",
            "issue_type": "FACT_CHECK_DEBUNKED" if fc_conclusion == "TIN GIáº¢" else "FACT_CHECK_VERIFIED",
            "fact_check_source": fc_source,
            "fact_check_url": fc_url
        }
        print(f"[SKIP CRITIC] Fact Check verdict added to evidence")
    
    # Skip CRITIC for KNOWLEDGE claims with Wikipedia evidence
    elif claim_type == "KNOWLEDGE":
        # Check if Wikipedia evidence exists
        has_wikipedia = any(
            "wikipedia" in (item.get("source", "") or "").lower()
            for layer in ["layer_2_high_trust", "layer_3_general"]
            for item in filtered_evidence_bundle.get(layer, [])
        )
        if has_wikipedia:
            print(f"\n[SKIP CRITIC] KNOWLEDGE claim with Wikipedia evidence - skipping CRITIC")
            critic_report = "[AUTO-SKIP] Knowledge claim verified by Wikipedia. No adversarial analysis needed."
            critic_parsed = {
                "issues_found": False,
                "issue_type": "KNOWLEDGE_VERIFIED",
                "skip_reason": "Wikipedia evidence for knowledge claim"
            }
    # Skip CRITIC for common knowledge (LATENCY OPTIMIZATION)
    elif _is_common_knowledge(text_input):
        print(f"\n[SKIP CRITIC] Common knowledge detected - skipping CRITIC (saves ~40s)")
        critic_report = "[AUTO-SKIP] Common knowledge. No adversarial analysis needed."
        critic_parsed = {
            "issues_found": False,
            "issue_type": "COMMON_KNOWLEDGE",
            "skip_reason": "Common knowledge claim"
        }
    else:
        # Normal CRITIC flow
        try:
            print(f"\n[CRITIC] Báº¯t Ä‘áº§u pháº£n biá»‡n...")
            critic_prompt_filled = CRITIC_PROMPT.replace("{text_input}", text_input)
            critic_prompt_filled = critic_prompt_filled.replace("{evidence_bundle_json}", evidence_bundle_json)
            critic_prompt_filled = critic_prompt_filled.replace("{current_date}", current_date)
            
            critic_report = await call_agent_with_capability_fallback(
                role="CRITIC",
                prompt=critic_prompt_filled,
                temperature=0.5,
                timeout=60.0  # Reduced from 120s for latency optimization
            )
            print(f"[CRITIC] Report: {critic_report[:150]}...")
            
            # Parse CRITIC response Ä‘á»ƒ kiá»ƒm tra counter_search_needed
            critic_parsed = _parse_json_from_text(critic_report)
            
            # NEW SCHEMA: Kiá»ƒm tra issues_found trá»±c tiáº¿p (khÃ´ng qua conclusion.issues_found)
            critic_issues = critic_parsed.get("issues_found", False)
            if not critic_issues:
                # Fallback: check old schema
                conclusion_obj = critic_parsed.get("conclusion", {})
                if isinstance(conclusion_obj, dict):
                    critic_issues = conclusion_obj.get("issues_found", False)
            
            issue_type = critic_parsed.get("issue_type", "NONE")
            if not issue_type or issue_type == "NONE":
                conclusion_obj = critic_parsed.get("conclusion", {})
                if isinstance(conclusion_obj, dict):
                    issue_type = conclusion_obj.get("issue_type", "NONE")
            
            # Log moved to after counter-search condition check
            
        except Exception as e:
            print(f"[CRITIC] Gáº·p lá»—i: {e}")
            critic_report = "Lá»—i khi cháº¡y Critic Agent."

    # =========================================================================
    # PHASE 1.5: CRITIC COUNTER-SEARCH (náº¿u CRITIC cáº§n search thÃªm)
    # KÃ­ch hoáº¡t khi:
    # - CRITIC phÃ¡t hiá»‡n váº¥n Ä‘á» cáº§n verify (issues_found=True AND issue_type != NONE)
    # - HOáº¶C CRITIC thiáº¿u báº±ng chá»©ng Ä‘á»ƒ pháº£n biá»‡n (evidence_verdict = INSUFFICIENT)
    # =========================================================================
    
    # CRITIC output schema: adversarial_findings.issues_found, adversarial_findings.issue_type
    adv_findings = critic_parsed.get("adversarial_findings", {})
    if isinstance(adv_findings, dict):
        critic_issues = adv_findings.get("issues_found", False)
        issue_type = adv_findings.get("issue_type", "NONE")
    else:
        # Fallback to top-level (old schema)
        critic_issues = critic_parsed.get("issues_found", False)
        issue_type = critic_parsed.get("issue_type", "NONE")
    
    # Check if evidence is insufficient (CRITIC cáº§n thÃªm báº±ng chá»©ng Ä‘á»ƒ pháº£n biá»‡n)
    evidence_assessment = critic_parsed.get("evidence_assessment", {})
    evidence_verdict = evidence_assessment.get("evidence_verdict", "UNKNOWN") if isinstance(evidence_assessment, dict) else "UNKNOWN"
    evidence_insufficient = evidence_verdict in ["INSUFFICIENT", "IRRELEVANT"]
    
    print(f"[CRITIC] Issues found: {critic_issues}, Type: {issue_type}, Evidence: {evidence_verdict}")
    
    # Counter-search CHá»ˆ KHI THá»°C Sá»° Cáº¦N THIáº¾T:
    # 1. Flag ENABLE_CRITIC_SEARCH = True
    # 2. VÃ€ má»™t trong cÃ¡c Ä‘iá»u kiá»‡n sau:
    #    a) Evidence lÃ  INSUFFICIENT hoáº·c IRRELEVANT (khÃ´ng cÃ³ gÃ¬ Ä‘á»ƒ pháº£n biá»‡n)
    #    b) CRITIC phÃ¡t hiá»‡n váº¥n Ä‘á» Cá»¤ THá»‚ (khÃ´ng pháº£i NONE/UNVERIFIED chung chung)
    # 3. VÃ€ CRITIC cÃ³ tráº£ vá» counter_search_queries
    
    should_counter_search = False
    if ENABLE_CRITIC_SEARCH:
        has_critical_issue = critic_issues and issue_type not in ["NONE", "UNVERIFIED"]  # Váº¥n Ä‘á» cá»¥ thá»ƒ nhÆ° ZOMBIE, SCAM
        evidence_truly_missing = evidence_verdict in ["INSUFFICIENT", "IRRELEVANT"]
        has_valid_queries = bool(critic_parsed.get("counter_search_queries"))
        
        # CHá»ˆ search khi: (cÃ³ váº¥n Ä‘á» cá»¥ thá»ƒ HOáº¶C thiáº¿u evidence) VÃ€ cÃ³ queries
        should_counter_search = (has_critical_issue or evidence_truly_missing) and has_valid_queries
        
        if should_counter_search:
            print(f"[CRITIC-SEARCH] KÃ­ch hoáº¡t vÃ¬: issue={issue_type}, evidence={evidence_verdict}")
        else:
            print(f"[CRITIC-SEARCH] Bá» qua - khÃ´ng Ä‘á»§ Ä‘iá»u kiá»‡n")
    
    if should_counter_search:
        counter_queries = critic_parsed.get("counter_search_queries", [])
        if counter_queries:
            print(f"\n[CRITIC-SEARCH] CRITIC yÃªu cáº§u search thÃªm: {counter_queries}")
            try:
                from app.search import call_google_search
                
                critic_counter_evidence = []
                for query in counter_queries[:2]:  # Giá»›i háº¡n 2 queries
                    results = call_google_search(query, "")
                    critic_counter_evidence.extend(results[:5])
                    if len(critic_counter_evidence) >= 5:
                        break
                
                if critic_counter_evidence:
                    print(f"[CRITIC-SEARCH] TÃ¬m tháº¥y {len(critic_counter_evidence)} evidence má»›i")
                    # Merge vÃ o evidence bundle
                    if "layer_2_high_trust" not in evidence_bundle:
                        evidence_bundle["layer_2_high_trust"] = []
                    evidence_bundle["layer_2_high_trust"].extend(critic_counter_evidence[:3])
                    
                    # Update evidence_bundle_json cho JUDGE
                    trimmed_bundle = _trim_evidence_bundle(evidence_bundle, claim_text=text_input)
                    evidence_bundle_json = json.dumps(trimmed_bundle, indent=2, ensure_ascii=False)
                    
            except Exception as e:
                print(f"[CRITIC-SEARCH] Lá»—i search: {e}")

    # =========================================================================
    # PHASE 2: JUDGE AGENT (THáº¨M PHÃN) - Round 1
    # =========================================================================
    judge_result = {}
    try:
        print(f"\n[JUDGE] Báº¯t Ä‘áº§u phÃ¡n quyáº¿t Round 1...")
        judge_prompt_filled = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
        judge_prompt_filled = judge_prompt_filled.replace("{evidence_bundle_json}", evidence_bundle_json)
        judge_prompt_filled = judge_prompt_filled.replace("{current_date}", current_date)
        
        # Add SYNTH instruction and CRITIC report
        judge_prompt_filled += synth_instruction
        judge_prompt_filled += f"\n\n[Ã KIáº¾N BIá»†N LÃ (CRITIC)]:\n{critic_report}"
        
        judge_text = await call_agent_with_capability_fallback(
            role="JUDGE",
            prompt=judge_prompt_filled,
            temperature=0.1,  # Strict logic
            timeout=120.0  # TÄƒng lÃªn 120s theo yÃªu cáº§u user
        )
        
        judge_result = _parse_json_from_text(judge_text)

        # ---------------------------------------------------------------------
        # ADAPTER: Convert to Flat Schema (Support BOTH old and new schemas)
        # ---------------------------------------------------------------------
        
        # NEW SCHEMA (simpler): conclusion, confidence_score at top level
        if not judge_result.get("conclusion"):
            # Try verdict_metadata (old schema)
            verdict_meta = judge_result.get("verdict_metadata")
            if verdict_meta and isinstance(verdict_meta, dict):
                judge_result["conclusion"] = verdict_meta.get("conclusion")
                judge_result["confidence_score"] = verdict_meta.get("probability_score")
        
        # NEW SCHEMA: key_evidence -> key_evidence_snippet, key_evidence_source
        key_ev = judge_result.get("key_evidence")
        if key_ev and isinstance(key_ev, dict):
            judge_result["key_evidence_snippet"] = key_ev.get("quote", "N/A")
            judge_result["key_evidence_source"] = key_ev.get("source", "N/A")
        
        # NEW SCHEMA: critic_response -> debate_log
        critic_resp = judge_result.get("critic_response")
        if critic_resp and isinstance(critic_resp, dict):
            judge_result["debate_log"] = {
                "critic_found_issues": critic_resp.get("critic_found_issues", False),
                "judge_agrees": critic_resp.get("judge_agrees", True),
                "judge_reasoning": critic_resp.get("judge_reasoning", "N/A")
            }
        
        # Fallback for reason - more comprehensive extraction
        if not judge_result.get("reason"):
            # Try alternate field names first
            for key in ["reasoning", "explanation", "rationale", "analysis", "summary"]:
                if judge_result.get(key):
                    judge_result["reason"] = str(judge_result[key])
                    break
            
            # Try extracting from thinking_process
            if not judge_result.get("reason"):
                thinking = judge_result.get("thinking_process")
                if thinking and isinstance(thinking, dict):
                    logical_reasoning = thinking.get("step3_logical_reasoning") or thinking.get("logical_reasoning")
                    if logical_reasoning:
                        judge_result["reason"] = str(logical_reasoning)
                    elif thinking.get("key_factors"):
                        factors = thinking.get("key_factors", [])
                        if isinstance(factors, list) and factors:
                            judge_result["reason"] = "; ".join(str(f) for f in factors[:3])
            
            # Try extracting from key_evidence
            if not judge_result.get("reason"):
                key_ev = judge_result.get("key_evidence")
                if key_ev and isinstance(key_ev, dict):
                    quote = key_ev.get("quote", "")
                    source = key_ev.get("source", "")
                    if quote and source:
                        judge_result["reason"] = f"Theo {source}: \"{quote[:200]}...\""
            
            # Final fallback: generate reason from conclusion with claim input
            if not judge_result.get("reason"):
                conclusion = judge_result.get("conclusion", "")
                verdict_meta = judge_result.get("verdict_metadata", {})
                verdict_type = verdict_meta.get("verdict_type", "") if isinstance(verdict_meta, dict) else ""
                
                # Truncate claim for display (max 100 chars)
                claim_display = text_input[:100] + "..." if len(text_input) > 100 else text_input
                
                if conclusion == "TIN GIáº¢":
                    if verdict_type == "ZOMBIE_NEWS":
                        judge_result["reason"] = f"ÄÃ¢y lÃ  tin cÅ© Ä‘Æ°á»£c trÃ¬nh bÃ y nhÆ° tin má»›i (Zombie News): \"{claim_display}\""
                    elif verdict_type == "SCAM":
                        judge_result["reason"] = f"Ná»™i dung cÃ³ dáº¥u hiá»‡u lá»«a Ä‘áº£o/scam: \"{claim_display}\""
                    elif verdict_type == "UNVERIFIED":
                        judge_result["reason"] = f"KhÃ´ng cÃ³ thÃ´ng tin nÃ o Ä‘á» cáº­p Ä‘áº¿n \"{claim_display}\""
                    else:
                        judge_result["reason"] = f"KhÃ´ng cÃ³ thÃ´ng tin nÃ o Ä‘á» cáº­p Ä‘áº¿n \"{claim_display}\""
                elif conclusion == "TIN THáº¬T":
                    judge_result["reason"] = f"ThÃ´ng tin Ä‘Æ°á»£c xÃ¡c nháº­n tá»« nguá»“n Ä‘Ã¡ng tin cáº­y: \"{claim_display}\""
                else:
                    # Náº¿u khÃ´ng xÃ¡c minh Ä‘Æ°á»£c â†’ TIN GIáº¢ (theo yÃªu cáº§u user)
                    judge_result["conclusion"] = "TIN GIáº¢"
                    judge_result["reason"] = f"KhÃ´ng cÃ³ thÃ´ng tin nÃ o Ä‘á» cáº­p Ä‘áº¿n \"{claim_display}\""
                    judge_result["confidence_score"] = 60  # Medium confidence for unverified
        
        # Final log
        if judge_result.get("conclusion"):
            conf = judge_result.get("confidence_score", "N/A")
            print(f"[JUDGE] Round 1: {judge_result.get('conclusion')} ({conf}%)")
        else:
            print(f"[JUDGE] WARNING: No valid conclusion. Fallback to heuristic.")
        # ---------------------------------------------------------------------
    except Exception as e:
        print(f"[JUDGE] Gáº·p lá»—i Round 1: {e}")
        return _heuristic_summarize(text_input, evidence_bundle, current_date)


    # =========================================================================
    # PHASE 2.5: COUNTER-SEARCH (TÃ¬m dáº«n chá»©ng Báº¢O Vá»† claim trÆ°á»›c khi káº¿t luáº­n TIN GIáº¢)
    # =========================================================================
    # Náº¿u JUDGE Round 1 káº¿t luáº­n TIN GIáº¢ â†’ Search thÃªm Ä‘á»ƒ tÃ¬m dáº«n chá»©ng á»§ng há»™ claim
    # ÄÃ¢y lÃ  cÆ¡ há»™i "pháº£n biá»‡n láº¡i CRITIC" báº±ng báº±ng chá»©ng má»›i
    
    conclusion_r1 = normalize_conclusion(judge_result.get("conclusion", ""))
    confidence_r1 = 50
    try:
        conf_raw = judge_result.get("confidence_score")
        if conf_raw is not None:
            confidence_r1 = int(conf_raw)
    except:
        pass
    
    # Track queries already searched (avoid duplicates)
    searched_queries = set()
    # Add queries from original search (estimated from text_input)
    searched_queries.add(text_input.lower().strip())
    # Add queries from CRITIC if any
    critic_queries = critic_parsed.get("counter_search_queries", [])
    for q in critic_queries:
        if q:
            searched_queries.add(q.lower().strip())
    
    # SMART TRIGGER: Counter-search CHá»ˆ khi JUDGE thá»±c sá»± khÃ´ng cháº¯c cháº¯n (<70%)
    judge_uncertain = confidence_r1 < 70  # Tháº¥p hÆ¡n 70% má»›i search thÃªm
    needs_more_evidence = judge_result.get("needs_more_evidence", False)
    if isinstance(needs_more_evidence, str):
        needs_more_evidence = needs_more_evidence.lower() == "true"
    
    should_counter_search = (
        ENABLE_COUNTER_SEARCH 
        and conclusion_r1 == "TIN GIáº¢" 
        and judge_uncertain  # CHá»ˆ khi confidence tháº¥p (<70%)
    )
    
    if should_counter_search:
        print(f"\n[COUNTER-SEARCH] JUDGE ngá» ngá»i (confidence={confidence_r1}%) â†’ TÃ¬m dáº«n chá»©ng Báº¢O Vá»† claim...")
        
        try:
            from app.search import call_google_search, _is_international_event, _extract_english_query
            
            # IMPROVED: Multi-language counter queries
            counter_queries = []
            
            # 1. Vietnamese confirmation query
            counter_queries.append(f"{text_input} tin tá»©c chÃ­nh thá»‘ng")
            
            # 2. English for international events (key improvement)
            if _is_international_event(text_input):
                en_text = _extract_english_query(text_input)
                if en_text and len(en_text) > 10:
                    counter_queries.append(f"{en_text} confirmed official")
                    counter_queries.append(f"{en_text} news Reuters AP")
            else:
                counter_queries.append(f"{text_input} Reuters AFP BBC")
            
            # FILTER: Remove queries similar to already searched (avoid redundant search)
            def is_similar(q: str, searched: set) -> bool:
                q_lower = q.lower().strip()
                for s in searched:
                    # Skip if query is substring of searched or vice versa
                    if q_lower in s or s in q_lower:
                        return True
                    # Skip if >70% word overlap
                    q_words = set(q_lower.split())
                    s_words = set(s.split())
                    if q_words and s_words:
                        overlap = len(q_words & s_words) / max(len(q_words), len(s_words))
                        if overlap > 0.7:
                            return True
                return False
            
            unique_counter_queries = [q for q in counter_queries if not is_similar(q, searched_queries)]
            
            if not unique_counter_queries:
                print(f"[COUNTER-SEARCH] Bá» qua - queries Ä‘Ã£ Ä‘Æ°á»£c search trÆ°á»›c Ä‘Ã³")
            else:
                counter_evidence = []
                for query in unique_counter_queries[:2]:  # Chá»‰ 2 queries Ä‘á»ƒ nhanh
                    searched_queries.add(query.lower().strip())  # Track new query
                    results = call_google_search(query, "")
                    counter_evidence.extend(results[:5])
                    if len(counter_evidence) >= 5:
                        break
            
                if not counter_evidence:
                    print(f"[COUNTER-SEARCH] KhÃ´ng tÃ¬m tháº¥y dáº«n chá»©ng má»›i")
                else:
                    print(f"[COUNTER-SEARCH] TÃ¬m tháº¥y {len(counter_evidence)} dáº«n chá»©ng cÃ³ thá»ƒ á»§ng há»™ claim")
                    
                    # Táº¡o evidence bundle má»›i vá»›i counter-evidence
                    counter_bundle = {
                        "layer_1_tools": evidence_bundle.get("layer_1_tools", []),
                        "layer_2_high_trust": counter_evidence[:5],
                        "layer_3_general": evidence_bundle.get("layer_3_general", []),
                        "layer_4_social_low": []
                    }
                    counter_evidence_json = json.dumps(_trim_evidence_bundle(counter_bundle, claim_text=text_input), indent=2, ensure_ascii=False)
                    
                    # JUDGE Round 1.5: Xem xÃ©t láº¡i vá»›i dáº«n chá»©ng má»›i
                    print(f"[JUDGE] Round 1.5: Xem xÃ©t láº¡i vá»›i dáº«n chá»©ng má»›i...")
                    
                    counter_prompt = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
                    counter_prompt = counter_prompt.replace("{evidence_bundle_json}", counter_evidence_json)
                    counter_prompt = counter_prompt.replace("{current_date}", current_date)
                    counter_prompt += f"""

[COUNTER-SEARCH EVIDENCE - QUAN TRá»ŒNG]
ÄÃ£ tÃ¬m thÃªm dáº«n chá»©ng tá»« nguá»“n tin chÃ­nh thá»‘ng. HÃ£y xem xÃ©t láº¡i káº¿t luáº­n.

[NGUYÃŠN Táº®C Báº®T BUá»˜C - ANTI-HALLUCINATION]
1. Báº N Báº®T BUá»˜C pháº£i dá»±a vÃ o evidence trong bundle, KHÃ”NG ÄÆ¯á»¢C tá»± suy diá»…n
2. Náº¿u evidence má»›i XÃC NHáº¬N claim (cÃ³ nguá»“n uy tÃ­n Ä‘Æ°a tin) â†’ Báº®T BUá»˜C TIN THáº¬T
3. "KhÃ´ng tÃ¬m tháº¥y evidence" â‰  TIN GIáº¢ (Innocent until proven guilty)
4. CHá»ˆ káº¿t luáº­n TIN GIáº¢ náº¿u cÃ³ báº±ng chá»©ng BÃC Bá» TRá»°C TIáº¾P claim
5. Tin quá»‘c táº¿ cÃ³ thá»ƒ Ä‘Æ°á»£c Reuters/AP/BBC Ä‘Æ°a tin trÆ°á»›c bÃ¡o VN

[CRITIC FEEDBACK TRÆ¯á»šC ÄÃ“]
{critic_report}
"""
                    
                    counter_text = await call_agent_with_capability_fallback(
                        role="JUDGE",
                        prompt=counter_prompt,
                        temperature=0.1,
                        timeout=25.0
                    )
                    
                    counter_result = _parse_json_from_text(counter_text)
                    
                    # Parse káº¿t quáº£
                    if counter_result.get("verdict_metadata"):
                        counter_conclusion = counter_result["verdict_metadata"].get("conclusion")
                        counter_confidence = counter_result["verdict_metadata"].get("probability_score")
                    else:
                        counter_conclusion = counter_result.get("conclusion")
                        counter_confidence = counter_result.get("confidence_score")
                    
                    counter_conclusion = normalize_conclusion(counter_conclusion or "")
                    
                    print(f"[JUDGE] Round 1.5: {counter_conclusion} ({counter_confidence}%)")
                    
                    # Náº¿u Counter-Search Ä‘á»•i Ã½ â†’ Cáº­p nháº­t judge_result
                    if counter_conclusion == "TIN THáº¬T":
                        print(f"[COUNTER-SEARCH] âœ… Counter-evidence Ä‘Ã£ thay Ä‘á»•i káº¿t luáº­n: TIN GIáº¢ â†’ TIN THáº¬T")
                        judge_result["conclusion"] = "TIN THáº¬T"
                        judge_result["confidence_score"] = counter_confidence or 75
                        judge_result["reason"] = (judge_result.get("reason", "") + 
                            f"\n\n[COUNTER-SEARCH] Sau khi tÃ¬m thÃªm dáº«n chá»©ng, claim Ä‘Æ°á»£c xÃ¡c nháº­n lÃ  TIN THáº¬T.")
                    else:
                        print(f"[COUNTER-SEARCH] âŒ Counter-evidence khÃ´ng thay Ä‘á»•i káº¿t luáº­n, giá»¯ TIN GIáº¢")
                
        except Exception as e:
            print(f"[COUNTER-SEARCH] Lá»—i: {e}")

    # =========================================================================
    # PHASE 3: SELF-CORRECTION (RE-SEARCH LOOP)
    # =========================================================================
    
    # FIX: Parse confidence an toÃ n - default 50 (neutral) thay vÃ¬ 0 Ä‘á»ƒ trÃ¡nh trigger re-search sai
    confidence = 50  # Neutral default
    raw_confidence = judge_result.get("confidence_score")
    if raw_confidence is not None:
        try:
            confidence = int(raw_confidence)
        except (ValueError, TypeError):
            confidence = 50  # Keep neutral if parse fails
            print(f"[SELF-CORRECTION] Warning: Could not parse confidence '{raw_confidence}', using default 50")
    else:
        print(f"[SELF-CORRECTION] Warning: No confidence_score in judge result, using default 50")
    
    # FIX: needs_more_evidence pháº£i lÃ  True EXPLICIT, khÃ´ng pháº£i chá»‰ vÃ¬ confidence tháº¥p do parse lá»—i    
    needs_more = judge_result.get("needs_more_evidence", False)
    if not isinstance(needs_more, bool):
        needs_more = str(needs_more).lower() == "true"
    
    # KÃ­ch hoáº¡t Re-search náº¿u:
    # 1. Judge YÃŠU Cáº¦U EXPLICIT (needs_more_evidence = True) - Æ°u tiÃªn cao nháº¥t
    # 2. Hoáº·c Confidence < 40 (ráº¥t tháº¥p, khÃ´ng pháº£i do parse fail)
    # 3. VÃ€ chÆ°a pháº£i lÃ  tin thá»i tiáº¿t (thá»i tiáº¿t thÆ°á»ng check 1 láº§n lÃ  Ä‘á»§)
    # 4. VÃ€ judge_result khÃ´ng rá»—ng (cÃ³ káº¿t quáº£ thá»±c sá»±)
    is_weather = "thá»i tiáº¿t" in judge_result.get("claim_type", "").lower()
    has_valid_result = bool(judge_result.get("conclusion"))
    
    # FIX: Chá»‰ trigger re-search khi THá»°C Sá»° cáº§n, khÃ´ng pháº£i do parse error
    # =========================================================================
    # PHASE 3: UNIFIED RE-SEARCH & CORRECTION
    # =========================================================================
    # SPEED & ACCURACY OPTIMIZATION: Gá»™p Counter-Search vÃ  Self-Correction.
    # KÃ­ch hoáº¡t Re-search náº¿u:
    # 1. JUDGE Round 1 káº¿t luáº­n TIN GIáº¢ (TÃ¬m dáº«n chá»©ng Báº¢O Vá»†)
    # 2. Hoáº·c JUDGE yÃªu cáº§u explicit (needs_more_evidence = True)
    # 3. Hoáº·c Confidence ráº¥t tháº¥p (< 40%)
    # 4. HOáº¶C CÃ³ sá»± mÃ¢u thuáº«n lá»›n giá»¯a CRITIC vÃ  JUDGE (Adversarial Mismatch)
    
    conclusion_r1 = normalize_conclusion(judge_result.get("conclusion", ""))
    confidence_r1 = 50
    try:
        conf_val = judge_result.get("confidence_score")
        if conf_val is not None:
            confidence_r1 = int(conf_val)
    except:
        pass
        
    needs_more_r1 = judge_result.get("needs_more_evidence", False)
    if not isinstance(needs_more_r1, bool):
        needs_more_r1 = str(needs_more_r1).lower() == "true"
        
    critic_conclusion = critic_parsed.get("conclusion", {})
    critic_found_issues = critic_conclusion.get("issues_found", False) if isinstance(critic_conclusion, dict) else False
    # Máº«u thuáº«n: CRITIC báº£o OK nhÆ°ng JUDGE báº£o SAI, hoáº·c ngÆ°á»£c láº¡i
    adversarial_mismatch = (critic_found_issues and conclusion_r1 == "TIN THáº¬T") or (not critic_found_issues and conclusion_r1 == "TIN GIáº¢")
    
    is_weather = "thá»i tiáº¿t" in judge_result.get("claim_type", "").lower()
    
    # =========================================================================
    # LATENCY OPTIMIZATION: Skip Round 2 if confidence > 85%
    # High confidence means JUDGE is already sure, no need for re-search
    # =========================================================================
    high_confidence_skip = confidence_r1 >= 85
    if high_confidence_skip:
        print(f"[LATENCY-SKIP] Confidence {confidence_r1}% >= 85%, skipping re-search phase")
    
    should_unified_research = (
        ENABLE_SELF_CORRECTION and 
        not high_confidence_skip and (  # NEW: Skip if high confidence
            (conclusion_r1 == "TIN GIáº¢" and ENABLE_COUNTER_SEARCH) # Phase 2.5 logic
            or needs_more_r1 # Phase 3 logic
            or confidence_r1 < 40 # Phase 3 logic
            or adversarial_mismatch # New logic
        ) and not is_weather
    )
    
    if should_unified_research:
        print(f"\n[UNIFIED-RE-SEARCH] KÃ­ch hoáº¡t (REASON: {'TIN GIáº¢' if conclusion_r1 == 'TIN GIáº¢' else 'Needs More' if needs_more_r1 else 'Low Conf' if confidence_r1 < 40 else 'Adversarial Mismatch'})")
        
        # Thu tháº­p táº¥t cáº£ queries tiá»m nÄƒng
        unified_queries = []
        
        # 1. Queries tá»« JUDGE
        unified_queries.extend(judge_result.get("additional_search_queries", []))
        unified_queries.extend(judge_result.get("verification_search_queries", []))
        
        # 2. Náº¿u lÃ  TIN GIáº¢, thÃªm cÃ¡c queries mang tÃ­nh "báº£o vá»‡" (Support Search)
        if conclusion_r1 == "TIN GIáº¢":
            # IMPROVED: Multi-language support
            from app.search import _is_international_event, _extract_english_query
            
            unified_queries.append(f"{text_input} tin tá»©c chÃ­nh thá»‘ng")
            
            if _is_international_event(text_input):
                en_text = _extract_english_query(text_input)
                if en_text and len(en_text) > 10:
                    unified_queries.append(f"{en_text} confirmed Reuters AP")
                    unified_queries.append(f"{en_text} official news")
            else:
                unified_queries.append(f"{text_input} official news")
            
        # 3. Fallback queries
        if not unified_queries:
            unified_queries = [f"{text_input} fact check", f"{text_input} news"]
            
        # Unique and limit queries (giá»›i háº¡n 3 queries Ä‘á»ƒ nhanh)
        unique_queries = []
        for q in unified_queries:
            if q and q not in unique_queries:
                unique_queries.append(q)
        unique_queries = unique_queries[:3]
        
        print(f"[UNIFIED-RE-SEARCH] Queries: {unique_queries}")
        
        try:
            # Execute search
            re_search_plan = {
                "required_tools": [{
                    "tool_name": "search",
                    "parameters": {"queries": unique_queries}
                }]
            }
            new_evidence = await execute_tool_plan(re_search_plan, site_query_string, flash_mode)
            
            # Merge evidence (safe initialization)
            for layer in ["layer_2_high_trust", "layer_3_general", "layer_4_social_low"]:
                if layer not in evidence_bundle: evidence_bundle[layer] = []
                evidence_bundle[layer].extend(new_evidence.get(layer, []))
            
            # Remove duplicates by URL
            seen_urls = {item.get("url") or item.get("link") for item in (evidence_bundle.get("layer_2_high_trust") or [])}
            # Trim evidence
            trimmed_bundle_v2 = _trim_evidence_bundle(evidence_bundle, claim_text=text_input)
            evidence_bundle_json_v2 = json.dumps(trimmed_bundle_v2, indent=2, ensure_ascii=False)
            
            # Re-Run JUDGE Round 2
            print(f"\n[JUDGE] Báº¯t Ä‘áº§u phÃ¡n quyáº¿t Round 2 (Final)...")
            judge_prompt_v2 = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
            judge_prompt_v2 = judge_prompt_v2.replace("{evidence_bundle_json}", evidence_bundle_json_v2)
            judge_prompt_v2 = judge_prompt_v2.replace("{current_date}", current_date)
            judge_prompt_v2 += f"\n\n[Ã KIáº¾N CRITIC & Káº¾T QUáº¢ R1]:\nCRITIC: {critic_report}\nR1 CONCLUSION: {conclusion_r1} ({confidence_r1}%)\n\n[INSTRUCTION]: HÃ£y xem xÃ©t báº±ng chá»©ng má»›i Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ Ä‘Æ°a ra káº¿t luáº­n cuá»‘i cÃ¹ng chÃ­nh xÃ¡c nháº¥t."
            
            judge_result_r1_backup = judge_result.copy()
            
            judge_text_v2 = await call_agent_with_capability_fallback(
                role="JUDGE",
                prompt=judge_prompt_v2,
                temperature=0.1,
                timeout=80.0
            )
            
            judge_result_r2 = _parse_json_from_text(judge_text_v2)
            
            # Adapter Round 2
            verdict_meta_v2 = judge_result_r2.get("verdict_metadata")
            if verdict_meta_v2:
                judge_result_r2["conclusion"] = verdict_meta_v2.get("conclusion")
                judge_result_r2["confidence_score"] = verdict_meta_v2.get("probability_score")
                
                exec_summary = judge_result_r2.get("executive_summary") or {}
                dialectical = judge_result_r2.get("dialectical_analysis") or {}
                synthesis = dialectical.get("synthesis") or exec_summary.get("bluf")
                
                combined_reason = ""
                citations = judge_result_r2.get("key_evidence_citations") or []
                if citations:
                    cite = citations[0]
                    combined_reason = f"Cáº­p nháº­t báº±ng chá»©ng má»›i tá»« {cite.get('source')}: \"{cite.get('quote', '')[:100]}...\". "
                
                judge_result_r2["reason"] = (combined_reason + (synthesis or "")).strip()

            else:
                # Fallback flat schema R2
                if not judge_result_r2.get("conclusion"):
                    judge_result_r2["conclusion"] = judge_result_r2.get("final_conclusion") or judge_result_r2.get("verdict")
                if not judge_result_r2.get("reason"):
                    judge_result_r2["reason"] = judge_result_r2.get("reasoning") or judge_result_r2.get("explanation")
            
            # Cáº­p nháº­t káº¿t quáº£ náº¿u R2 há»£p lá»‡
            if judge_result_r2.get("conclusion"):
                judge_result = judge_result_r2
                judge_result["cached"] = False
                print(f"[JUDGE] Round 2 Success: {judge_result.get('conclusion')} ({judge_result.get('confidence_score')}%)")
            else:
                print("[JUDGE] Round 2 failed or invalid, keeping Round 1 results.")
                judge_result = judge_result_r1_backup
                
        except Exception as e:
            print(f"[UNIFIED-RE-SEARCH] Error: {e}")
            judge_result = judge_result_r1_backup
    else:
        print("[SELF-CORRECTION] KhÃ´ng kÃ­ch hoáº¡t cÃ¡c vÃ²ng phá»¥ (Fast Lane).")

    # =========================================================================
    # POST-PROCESSING: TRUSTED SOURCE OVERRIDE (Reduce False Positive Rate)
    # =========================================================================
    # If claim has trusted source prefix (AP, Reuters, BBC, VnExpress) and 
    # JUDGE returned TIN GIáº¢ but no strong contradiction found â†’ Override to TIN THáº¬T
    
    if judge_result and _has_trusted_source_citation(text_input):
        current_conclusion = normalize_conclusion(judge_result.get("conclusion", ""))
        reason_text = (judge_result.get("reason") or "").lower()
        
        # Check if there's a STRONG contradiction in the reason
        strong_contradiction_keywords = [
            "bÃ¡c bá»", "debunked", "sai sá»± tháº­t", "fake", "hoax", "lá»«a Ä‘áº£o",
            "khÃ´ng tá»“n táº¡i", "khÃ´ng xÃ¡c nháº­n", "khÃ´ng cÃ³ tháº­t", "contrary evidence"
        ]
        has_strong_contradiction = any(kw in reason_text for kw in strong_contradiction_keywords)
        
        if current_conclusion == "TIN GIáº¢" and not has_strong_contradiction:
            # Extract source name for logging
            source_name = "Trusted Source"
            for prefix in TRUSTED_SOURCE_PREFIXES:
                if text_input.lower().strip().startswith(prefix):
                    source_name = prefix.replace("theo ", "").replace(":", "").replace("Ä‘Æ°a tin", "").strip().title()
                    break
            
            print(f"[TRUSTED-SOURCE-OVERRIDE] Claim cÃ³ nguá»“n {source_name}, khÃ´ng cÃ³ pháº£n chá»©ng máº¡nh â†’ Override TIN GIáº¢ â†’ TIN THáº¬T")
            judge_result["conclusion"] = "TIN THáº¬T"
            judge_result["confidence_score"] = 85  # HIGH confidence for trusted source
            judge_result["reason"] = (
                f"Claim trÃ­ch dáº«n nguá»“n uy tÃ­n ({source_name}). "
                f"KhÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng bÃ¡c bá» cá»¥ thá»ƒ nÃªn Æ°u tiÃªn TIN THáº¬T. "
                f"[Gá»‘c: {judge_result.get('reason', '')}]"
            )
        elif current_conclusion == "TIN THáº¬T":
            # BOOST: If LLM already returned TIN THáº¬T for trusted source, boost confidence
            current_conf = judge_result.get("confidence_score", 70)
            # Boost by 15% for trusted source, max 90%
            boosted_conf = min(current_conf + 15, 90)
            if boosted_conf > current_conf:
                print(f"[TRUSTED-SOURCE-BOOST] Claim cÃ³ nguá»“n uy tÃ­n, boost confidence {current_conf}% â†’ {boosted_conf}%")
                judge_result["confidence_score"] = boosted_conf
    
    # Post-processing normalization
    if judge_result:
        # Map old schema keys if needed (fallback)
        if "final_conclusion" in judge_result and "conclusion" not in judge_result:
            judge_result["conclusion"] = judge_result["final_conclusion"]
            
        judge_result["conclusion"] = normalize_conclusion(judge_result.get("conclusion"))
        
        # =========================================================================
        # FIX: Ensure evidence_link is populated from evidence bundle
        # =========================================================================
        if not judge_result.get("evidence_link"):
            # Extract first evidence URL from bundle
            for layer in ["layer_2_high_trust", "layer_3_general", "layer_1_tools", "layer_4_social_low"]:
                items = evidence_bundle.get(layer, [])
                for item in items:
                    url = item.get("url") or item.get("link")
                    if url:
                        judge_result["evidence_link"] = url
                        break
                if judge_result.get("evidence_link"):
                    break
        
        return judge_result

    # Fallback final
    return _heuristic_summarize(text_input, evidence_bundle, current_date)

