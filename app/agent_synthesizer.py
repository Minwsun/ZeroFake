# app/agent_synthesizer.py

import os
import json
import re
from dotenv import load_dotenv
from typing import Dict, Any, List

from app.weather import classify_claim
from app.model_clients import (
    call_gemini_model,
    call_agent_with_capability_fallback,
    ModelClientError,
    RateLimitError,
)
from app.tool_executor import execute_tool_plan  # Import for Re-Search

load_dotenv()


GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SYNTHESIS_PROMPT = ""
CRITIC_PROMPT = ""  # NEW: Prompt cho CRITIC agent

# ==============================================================================
# COGNITIVE PIPELINE FLAGS - Quy tr√¨nh t∆∞ duy CRITIC-JUDGE
# ==============================================================================
# COUNTER-SEARCH: Khi JUDGE k·∫øt lu·∫≠n TIN GI·∫¢, search th√™m ƒë·ªÉ "b·∫£o v·ªá" claim
# SELF-CORRECTION: Re-search khi JUDGE y√™u c·∫ßu ho·∫∑c confidence th·∫•p
ENABLE_COUNTER_SEARCH = True   # B·∫≠t ƒë·ªÉ JUDGE c√≥ th·ªÉ ph·∫£n bi·ªán l·∫°i CRITIC
ENABLE_SELF_CORRECTION = True  # B·∫≠t ƒë·ªÉ JUDGE c√≥ th·ªÉ search verify khi c·∫ßn


# C√†i ƒë·∫∑t an to√†n
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]


WEATHER_SOURCE_KEYWORDS = [
    "weather",
    "forecast",
    "accuweather",
    "windy",
    "meteoblue",
    "ventusky",
    "nchmf",
    "thoitiet",
    "openweathermap",
    "wunderground",
    "metoffice",
    "bom.gov",
]


# ==============================================================================
# SYNTH LOGIC: ƒê·ªÉ LLM t·ª± ph√¢n lo·∫°i claim (kh√¥ng d√πng pattern c·ª©ng)
# ==============================================================================

def _classify_claim_type(text_input: str) -> str:
    """
    SIMPLIFIED: Kh√¥ng d√πng pattern c·ª©ng n·ªØa.
    Tr·∫£ v·ªÅ "AUTO" ƒë·ªÉ LLM t·ª± quy·∫øt ƒë·ªãnh d·ª±a tr√™n context.
    
    LLM s·∫Ω t·ª± ph√¢n lo·∫°i:
    - KNOWLEDGE: Ki·∫øn th·ª©c c·ªë ƒë·ªãnh (ƒë·ªãa l√Ω, khoa h·ªçc, ƒë·ªãnh nghƒ©a)
    - NEWS: Tin t·ª©c, s·ª± ki·ªán, tuy√™n b·ªë
    
    Nh∆∞ v·∫≠y h·ªá th·ªëng s·∫Ω kh√°ch quan h∆°n v√† ho·∫°t ƒë·ªông tr√™n m·ªçi tr∆∞·ªùng h·ª£p.
    """
    return "AUTO"


def normalize_conclusion(conclusion: str) -> str:
    """
    Normalize conclusion to BINARY classification: TIN TH·∫¨T or TIN GI·∫¢ only.
    
    üü¢ NGUY√äN T·∫ÆC M·ªöI: PRESUMPTION OF TRUTH
    - M·∫∑c ƒë·ªãnh l√† TIN TH·∫¨T n·∫øu kh√¥ng c√≥ d·∫•u hi·ªáu TIN GI·∫¢ r√µ r√†ng
    - Ch·ªâ tr·∫£ v·ªÅ TIN GI·∫¢ khi c√≥ keywords ch·ªâ ƒë·ªãnh r√µ r√†ng
    """
    if not conclusion:
        return "TIN TH·∫¨T"  # ƒê·ªîI: M·∫∑c ƒë·ªãnh TIN TH·∫¨T n·∫øu kh√¥ng c√≥ k·∫øt lu·∫≠n
    
    conclusion_upper = conclusion.upper().strip()
    
    # üî¥ CH·ªà TIN GI·∫¢ KHI C√ì D·∫§U HI·ªÜU R√ï R√ÄNG
    fake_indicators = [
        # Vietnamese fake indicators
        "TIN GI·∫¢", "TIN GIA", "GI·∫¢ M·∫†O", "FAKE", "FALSE",
        "B·ªäA ƒê·∫∂T", "BIA DAT", "L·ª™A ƒê·∫¢O", "LUA DAO", "SCAM",
        "ZOMBIE", "OUTDATED", "L·ªñI TH·ªúI", "LOI THOI",
        "KH√îNG ƒê√öNG", "KHONG DUNG", "SAI S·ª∞ TH·∫¨T", "SAI SU THAT",
        "KH√îNG C√ì C∆† S·ªû", "KHONG CO CO SO", "V√î CƒÇN C·ª®", "VO CAN CU",
        "ALMOST CERTAINLY FALSE", "HIGHLY UNLIKELY",
        "B√ÅC B·ªé", "BAC BO", "KH√îNG X√ÅC NH·∫¨N", "KHONG XAC NHAN",
        # Y t·∫ø sai
        "Y T·∫æ SAI", "Y TE SAI", "MISLEADING",
        # S·ªë li·ªáu phi th·ª±c t·∫ø
        "PHI TH·ª∞C T·∫æ", "PHI THUC TE", "UNREALISTIC",
        # G√ÇY HI·ªÇU L·∫¶M - v·∫´n coi l√† TIN GI·∫¢
        "G√ÇY HI·ªÇU L·∫¶M", "GAY HIEU LAM",
    ]
    
    # N·∫øu c√≥ b·∫•t k·ª≥ indicator TIN GI·∫¢ n√†o -> TIN GI·∫¢
    for indicator in fake_indicators:
        if indicator in conclusion_upper:
            return "TIN GI·∫¢"
    
    # üü¢ T·∫§T C·∫¢ C√ÅC TR∆Ø·ªúNG H·ª¢P KH√ÅC -> TIN TH·∫¨T
    # Bao g·ªìm: TIN TH·∫¨T, CH∆ØA KI·ªÇM CH·ª®NG, TRUE, PROBABLE, LIKELY, etc.
    return "TIN TH·∫¨T"


# Product version database for outdated information detection
# Format: product_pattern -> (latest_version, release_year)
PRODUCT_VERSIONS = {
    # Apple iPhone (as of Dec 2025)
    r"iphone\s*(\d+)": {"latest": 17, "year": 2025, "name": "iPhone"},
    # Samsung Galaxy S
    r"galaxy\s*s\s*(\d+)": {"latest": 25, "year": 2025, "name": "Galaxy S"},
    # Samsung Galaxy Note
    r"galaxy\s*note\s*(\d+)": {"latest": 20, "year": 2020, "name": "Galaxy Note"},
    # Google Pixel
    r"pixel\s*(\d+)": {"latest": 9, "year": 2024, "name": "Pixel"},
    # PlayStation
    r"playstation\s*(\d+)|ps\s*(\d+)": {"latest": 5, "year": 2020, "name": "PlayStation"},
    # Xbox (Xbox One=1, Series X=2)
    r"xbox\s*series\s*([xs])": {"latest": "x", "year": 2020, "name": "Xbox Series"},
    # Windows
    r"windows\s*(\d+)": {"latest": 11, "year": 2021, "name": "Windows"},
    # macOS versions
    r"macos\s*(\d+)|mac\s*os\s*(\d+)": {"latest": 15, "year": 2024, "name": "macOS"},
    # MacBook chips
    r"macbook.*m(\d+)": {"latest": 4, "year": 2024, "name": "MacBook M-chip"},
}


def _detect_outdated_product(text_input: str) -> dict | None:
    """
    Detect if the input mentions an outdated product version.
    Returns dict with product info if outdated, None otherwise.
    """
    text_lower = text_input.lower()
    
    for pattern, info in PRODUCT_VERSIONS.items():
        match = re.search(pattern, text_lower)
        if match:
            # Get the version number from match groups
            version_str = None
            for group in match.groups():
                if group:
                    version_str = group
                    break
            
            if version_str:
                try:
                    # Handle numeric versions
                    if version_str.isdigit():
                        mentioned_version = int(version_str)
                        latest_version = info["latest"]
                        
                        if isinstance(latest_version, int) and mentioned_version < latest_version:
                            return {
                                "product": info["name"],
                                "mentioned_version": mentioned_version,
                                "latest_version": latest_version,
                                "latest_year": info["year"],
                                "is_outdated": True,
                                "years_behind": latest_version - mentioned_version
                            }
                except (ValueError, TypeError):
                    pass
    
    return None


def _is_common_knowledge(text_input: str) -> bool:
    """
    Detect if the claim is about well-known, easily verifiable facts.
    These are facts that are widely accepted and don't need extensive verification.
    """
    text_lower = text_input.lower()
    
    # Well-known tech facts
    common_knowledge_patterns = [
        # Company ownership/development
        ("chatgpt", "openai"),
        ("gpt-4", "openai"),
        ("gpt-3", "openai"),
        ("google", "alphabet"),
        ("youtube", "google"),
        ("instagram", "meta"),
        ("whatsapp", "meta"),
        ("facebook", "meta"),
        ("iphone", "apple"),
        ("android", "google"),
        ("windows", "microsoft"),
        ("azure", "microsoft"),
        ("aws", "amazon"),
        
        # Historical events that are well-documented
        ("facebook", "meta", "2021"),
        ("messi", "world cup", "2022"),
        ("argentina", "world cup", "2022"),
    ]
    
    for pattern in common_knowledge_patterns:
        if all(keyword in text_lower for keyword in pattern):
            return True
    
    return False


def _detect_zombie_news(text_input: str, current_date: str) -> dict | None:
    """
    Detect ZOMBIE NEWS: News about past events presented as if they just happened.
    
    Examples:
    - "Vi·ªát Nam v√¥ ƒë·ªãch AFF Cup 2018 ƒë√™m qua" (AFF 2018 but "last night")
    - "Steve Jobs v·ª´a qua ƒë·ªùi" (Steve Jobs died in 2011)
    - "Samsung Galaxy Note 7 b·ªã thu h·ªìi" (Note 7 was recalled in 2016)
    
    Returns dict with zombie news info if detected, None otherwise.
    """
    import re
    from datetime import datetime
    
    text_lower = text_input.lower()
    
    # Get current year from current_date or system
    try:
        if current_date and len(current_date) >= 4:
            current_year = int(current_date[:4])
        else:
            current_year = datetime.now().year
    except:
        current_year = datetime.now().year
    
    # Words indicating "just happened" / "breaking news" / "recent"
    recency_indicators = [
        "ƒë√™m qua", "s√°ng nay", "v·ª´a", "m·ªõi", "h√¥m nay", "h√¥m qua", "tu·∫ßn n√†y",
        "breaking", "n√≥ng", "kh·∫©n c·∫•p", "m·ªõi nh·∫•t", "c·∫≠p nh·∫≠t", "tin s·ªëc",
        "v·ª´a x·∫£y ra", "v·ª´a m·ªõi", "s√°ng s·ªõm", "chi·ªÅu nay", "t·ªëi nay",
        "xem ngay", "share ngay", "chia s·∫ª ngay"
    ]
    
    has_recency_indicator = any(indicator in text_lower for indicator in recency_indicators)
    
    if not has_recency_indicator:
        return None
    
    # Pattern 1: Detect year in the text (e.g., "2018", "2019", etc.)
    # Only consider years that are significantly in the past (at least 1 year ago)
    year_pattern = re.search(r'\b(19\d{2}|20[0-2]\d)\b', text_input)
    if year_pattern:
        mentioned_year = int(year_pattern.group(1))
        years_ago = current_year - mentioned_year
        
        # If the mentioned year is at least 1 year ago, this is zombie news
        if years_ago >= 1:
            return {
                "is_zombie_news": True,
                "mentioned_year": mentioned_year,
                "current_year": current_year,
                "years_ago": years_ago,
                "recency_indicator": next((ind for ind in recency_indicators if ind in text_lower), "unknown")
            }
    
    # Pattern 2: Known past events database (famous events that can't "just happen")
    # These are events that definitively happened in the past and cannot happen again
    known_past_events = [
        # Deaths of famous people
        ("steve jobs", "qua ƒë·ªùi", 2011),
        ("steve jobs", "died", 2011),
        ("michael jackson", "qua ƒë·ªùi", 2009),
        ("michael jackson", "died", 2009),
        ("kobe bryant", "qua ƒë·ªùi", 2020),
        ("kobe bryant", "died", 2020),
        
        # Product recalls/launches that are old
        ("galaxy note 7", "thu h·ªìi", 2016),
        ("galaxy note 7", "recall", 2016),
        ("galaxy note 7", "ch√°y n·ªï", 2016),
        
        # Aviation incidents
        ("mh370", "m·∫•t t√≠ch", 2014),
        ("mh370", "missing", 2014),
        
        # Specific tournaments with years (AFF Cup 2018 was in past)
        # Sports events follow: {event} + {year} + recency = zombie
    ]
    
    for keywords_year in known_past_events:
        *keywords, event_year = keywords_year
        if all(kw in text_lower for kw in keywords):
            years_ago = current_year - event_year
            if years_ago >= 1:
                return {
                    "is_zombie_news": True,
                    "mentioned_year": event_year,
                    "current_year": current_year,
                    "years_ago": years_ago,
                    "recency_indicator": next((ind for ind in recency_indicators if ind in text_lower), "unknown"),
                    "known_event": " ".join(keywords)
                }
    return None


def _is_weather_source(item: Dict[str, Any]) -> bool:
    source = (item.get("source") or item.get("url") or "").lower()
    if not source:
        return False
    return any(keyword in source for keyword in WEATHER_SOURCE_KEYWORDS)


def load_synthesis_prompt(prompt_path="prompts/synthesis_prompt.txt"):
    """T·∫£i prompt cho Agent 2 (Synthesizer)"""
    global SYNTHESIS_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            SYNTHESIS_PROMPT = f.read()
        print("INFO: T·∫£i Synthesis Prompt th√†nh c√¥ng.")
    except Exception as e:
        print(f"L·ªñI: kh√¥ng th·ªÉ t·∫£i {prompt_path}: {e}")
        raise


def load_critic_prompt(prompt_path="prompts/critic_prompt.txt"):
    """T·∫£i prompt cho CRITIC agent (Devil's Advocate)"""
    global CRITIC_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            CRITIC_PROMPT = f.read()
        print("INFO: T·∫£i CRITIC Prompt th√†nh c√¥ng.")
    except FileNotFoundError:
        # Fallback to default prompt if file not found
        CRITIC_PROMPT = (
            "B·∫°n l√† Bi·ªán l√Ω ƒë·ªëi l·∫≠p (Devil's Advocate). "
            "H√£y ch·ªâ ra 3 ƒëi·ªÉm y·∫øu, m√¢u thu·∫´n ho·∫∑c kh·∫£ nƒÉng ƒë√¢y l√† tin c≈©/satire/tin ƒë·ªìn. "
            "Ch·ªâ tr·∫£ l·ªùi ng·∫Øn g·ªçn, gay g·∫Øt."
        )
        print(f"WARNING: Kh√¥ng t√¨m th·∫•y {prompt_path}, d√πng prompt m·∫∑c ƒë·ªãnh.")
    except Exception as e:
        print(f"L·ªñI: kh√¥ng th·ªÉ t·∫£i {prompt_path}: {e}")


def _parse_json_from_text(text: str) -> dict:
    """Tr√≠ch xu·∫•t JSON an to√†n t·ª´ text tr·∫£ v·ªÅ c·ªßa LLM"""
    if not text:
        print("L·ªñI: Agent 2 (Synthesizer) kh√¥ng t√¨m th·∫•y JSON.")
        return {}

    cleaned = text.strip()
    # Remove Markdown code fences if present
    if cleaned.startswith("```"):
        cleaned = re.sub(r"^```[a-zA-Z0-9_-]*\s*", "", cleaned)
        cleaned = cleaned.rstrip("`").strip()

    match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            print(f"L·ªñI: Agent 2 (Synthesizer) tr·∫£ v·ªÅ JSON kh√¥ng h·ª£p l·ªá. Text: {cleaned[:300]}...")
            return {}
    # Try direct JSON load if regex failed
    try:
        return json.loads(cleaned)
    except Exception:
        print(f"L·ªñI: Agent 2 (Synthesizer) kh√¥ng t√¨m th·∫•y JSON. Raw response: {cleaned[:300]}...")
        return {}


def _trim_snippet(s: str, max_len: int = 200) -> str:
    """
    OPTIMIZED: Gi·∫£m max_len t·ª´ 500 xu·ªëng 200 ƒë·ªÉ ti·∫øt ki·ªám token.
    V·ªõi 3 evidence items * 200 chars = 600 chars thay v√¨ 10 * 500 = 5000 chars.
    Ti·∫øt ki·ªám ~90% token cho evidence.
    """
    if not s:
        return ""
    s = s.replace("\n", " ").strip()
    return s[:max_len]


def _trim_evidence_bundle(bundle: Dict[str, Any], cap_l2: int = 3, cap_l3: int = 3, cap_l4: int = 2) -> Dict[str, Any]:
    """
    OPTIMIZED: Gi·∫£m cap t·ª´ 10/10/5 xu·ªëng 3/3/2 ƒë·ªÉ ti·∫øt ki·ªám token.
    T·ªïng: 8 evidence items thay v√¨ 25 items.
    M·ª•c ti√™u: Gi·∫£m latency t·ª´ ~70s xu·ªëng ~25s.
    """
    if not bundle:
        return {"layer_1_tools": [], "layer_2_high_trust": [], "layer_3_general": [], "layer_4_social_low": []}
    out = {
        "layer_1_tools": [], # OpenWeather API data
        "layer_2_high_trust": [],
        "layer_3_general": [],
        "layer_4_social_low": []
    }
    
    # L·ªõp 1: OpenWeather API data (quan tr·ªçng cho tin th·ªùi ti·∫øt)
    for it in (bundle.get("layer_1_tools") or []):
        out["layer_1_tools"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date"),
            "weather_data": it.get("weather_data")  # Gi·ªØ nguy√™n d·ªØ li·ªáu g·ªëc t·ª´ OpenWeather
        })
    
    # L·ªõp 2
    for it in (bundle.get("layer_2_high_trust") or [])[:cap_l2]:
        out["layer_2_high_trust"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    # L·ªõp 3
    for it in (bundle.get("layer_3_general") or [])[:cap_l3]:
        out["layer_3_general"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    # L·ªõp 4
    for it in (bundle.get("layer_4_social_low") or [])[:cap_l4]:
        out["layer_4_social_low"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    return out


def _as_str(x: Any) -> str:
    try:
        return x if isinstance(x, str) else ("" if x is None else str(x))
    except Exception:
        return ""


def _heuristic_summarize(text_input: str, bundle: Dict[str, Any], current_date: str) -> Dict[str, Any]:
    """
    Logic d·ª± ph√≤ng khi LLM th·∫•t b·∫°i.
    
    NGUY√äN T·∫ÆC: PRESUMPTION OF TRUTH
    - M·∫∑c ƒë·ªãnh l√† TIN TH·∫¨T n·∫øu kh√¥ng c√≥ b·∫±ng ch·ª©ng B√ÅC B·ªé
    - Ch·ªâ TIN GI·∫¢ khi: evidence B√ÅC B·ªé tr·ª±c ti·∫øp ho·∫∑c s·∫£n ph·∫©m l·ªói th·ªùi
    """
    l1 = bundle.get("layer_1_tools") or []
    l2 = bundle.get("layer_2_high_trust") or []
    l3 = bundle.get("layer_3_general") or []

    try:
        claim = classify_claim(text_input)
    except Exception:
        claim = {"is_weather": False}

    is_weather_claim = claim.get("is_weather", False)
    text_lower = text_input.lower()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PRIORITY 0: S·ª± th·∫≠t hi·ªÉn nhi√™n (Common Knowledge)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if _is_common_knowledge(text_input):
        debate_log = {
            "red_team_argument": "T√¥i kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng b√°c b·ªè s·ª± th·∫≠t khoa h·ªçc/k·ªπ thu·∫≠t n√†y.",
            "blue_team_argument": "ƒê√¢y l√† s·ª± th·∫≠t ƒë√£ ƒë∆∞·ª£c khoa h·ªçc/c·ªông ƒë·ªìng c√¥ng nh·∫≠n r·ªông r√£i.",
            "judge_reasoning": "Blue Team th·∫Øng. ƒê√¢y l√† ki·∫øn th·ª©c ph·ªï th√¥ng ƒë√£ ƒë∆∞·ª£c x√°c nh·∫≠n."
        }
        return {
            "conclusion": "TIN TH·∫¨T",
            "confidence_score": 99,
            "reason": "ƒê√¢y l√† s·ª± th·∫≠t khoa h·ªçc/k·ªπ thu·∫≠t ƒë√£ ƒë∆∞·ª£c c√¥ng nh·∫≠n r·ªông r√£i.",
            "debate_log": debate_log,
            "key_evidence_snippet": "Ki·∫øn th·ª©c ph·ªï th√¥ng",
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "",
            "cached": False
        }
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PRIORITY 2: Ph√°t hi·ªán s·∫£n ph·∫©m L·ªñI TH·ªúI (Outdated Product)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    outdated_info = _detect_outdated_product(text_input)
    if outdated_info and outdated_info.get("is_outdated"):
        product = outdated_info["product"]
        mentioned = outdated_info["mentioned_version"]
        latest = outdated_info["latest_version"]
        latest_year = outdated_info["latest_year"]
        
        # Build Adversarial Dialectic debate
        debate_log = {
            "red_team_argument": _as_str(
                f"Th√¥ng tin n√†y SAI! {product} {mentioned} l√† phi√™n b·∫£n c≈©. "
                f"Hi·ªán t·∫°i ƒë√£ c√≥ {product} {latest} (ra m·∫Øt nƒÉm {latest_year}). "
                f"Vi·ªác ƒëƒÉng tin v·ªÅ {product} {mentioned} nh∆∞ tin m·ªõi l√† SAI S·ª∞ TH·∫¨T."
            ),
            "blue_team_argument": _as_str(
                f"ƒê√∫ng l√† {product} {mentioned} ƒë√£ ra m·∫Øt th·∫≠t. "
                f"Tuy nhi√™n, ƒë√¢y l√† th√¥ng tin l·ªói th·ªùi. T√¥i th·ª´a nh·∫≠n thua cu·ªôc."
            ),
            "judge_reasoning": _as_str(
                f"Red Team th·∫Øng. {product} {mentioned} l√† phi√™n b·∫£n c≈©. "
                f"Hi·ªán t·∫°i ƒë√£ c√≥ {product} {latest}. Tin l·ªói th·ªùi = TIN GI·∫¢."
            )
        }
        
        return {
            "conclusion": "TIN GI·∫¢",
            "confidence_score": 95,
            "reason": _as_str(
                f"{product} {mentioned} ƒë√£ l·ªói th·ªùi. "
                f"Hi·ªán t·∫°i ƒë√£ c√≥ {product} {latest} (nƒÉm {latest_year}). "
                f"Tin v·ªÅ s·∫£n ph·∫©m c≈© = TIN GI·∫¢."
            ),
            "debate_log": debate_log,
            "key_evidence_snippet": _as_str(f"{product} {latest} ra m·∫Øt nƒÉm {latest_year}"),
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "Th√¥ng tin l·ªói th·ªùi ƒë∆∞·ª£c tr√¨nh b√†y nh∆∞ tin m·ªõi",
            "cached": False
        }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PRIORITY 3: Ph√°t hi·ªán ZOMBIE NEWS (tin c≈© tr√¨nh b√†y nh∆∞ tin m·ªõi)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    zombie_info = _detect_zombie_news(text_input, current_date)
    if zombie_info and zombie_info.get("is_zombie_news"):
        mentioned_year = zombie_info["mentioned_year"]
        years_ago = zombie_info["years_ago"]
        recency_indicator = zombie_info.get("recency_indicator", "v·ª´a x·∫£y ra")
        known_event = zombie_info.get("known_event", "")
        
        # Build Adversarial Dialectic debate
        debate_log = {
            "red_team_argument": _as_str(
                f"ƒê√¢y l√† ZOMBIE NEWS! S·ª± ki·ªán nƒÉm {mentioned_year} ({years_ago} nƒÉm tr∆∞·ªõc) "
                f"nh∆∞ng ƒë∆∞·ª£c tr√¨nh b√†y nh∆∞ v·ª´a x·∫£y ra ('{recency_indicator}'). "
                f"ƒê√¢y l√† th·ªß thu·∫≠t clickbait ph·ªï bi·∫øn ƒë·ªÉ l·ª´a ng∆∞·ªùi ƒë·ªçc."
            ),
            "blue_team_argument": _as_str(
                f"ƒê√∫ng l√† s·ª± ki·ªán nƒÉm {mentioned_year} ƒë√£ x·∫£y ra th·∫≠t. "
                f"Nh∆∞ng vi·ªác d√πng ng√¥n ng·ªØ '{recency_indicator}' l√† g√¢y hi·ªÉu l·∫ßm. T√¥i thua."
            ),
            "judge_reasoning": _as_str(
                f"Red Team th·∫Øng. S·ª± ki·ªán nƒÉm {mentioned_year} KH√îNG TH·ªÇ '{recency_indicator}' ƒë∆∞·ª£c. "
                f"ƒê√¢y l√† tin c≈© ƒë∆∞·ª£c t√°i s·ª≠ d·ª•ng = ZOMBIE NEWS = TIN GI·∫¢."
            )
        }
        
        return {
            "conclusion": "TIN GI·∫¢",
            "confidence_score": 95,
            "reason": _as_str(
                f"ZOMBIE NEWS: S·ª± ki·ªán nƒÉm {mentioned_year} ({years_ago} nƒÉm tr∆∞·ªõc) "
                f"ƒë∆∞·ª£c tr√¨nh b√†y nh∆∞ v·ª´a x·∫£y ra ('{recency_indicator}'). "
                f"ƒê√¢y l√† tin c≈© ƒë∆∞·ª£c l·∫∑p l·∫°i ƒë·ªÉ l·ª´a ng∆∞·ªùi ƒë·ªçc."
            ),
            "debate_log": debate_log,
            "key_evidence_snippet": _as_str(f"S·ª± ki·ªán x·∫£y ra nƒÉm {mentioned_year}, kh√¥ng ph·∫£i '{recency_indicator}'"),
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "ZOMBIE NEWS - Tin c≈© tr√¨nh b√†y nh∆∞ tin m·ªõi",
            "cached": False
        }

    # ∆Øu ti√™n L·ªõp 1 (OpenWeather API) cho tin th·ªùi ti·∫øt
    if is_weather_claim and l1:
        weather_item = l1[0]
        weather_data = weather_item.get("weather_data", {})
        if weather_data:
            # So s√°nh ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt
            main_condition = weather_data.get("main", "").lower()
            description = weather_data.get("description", "").lower()
            
            # Ki·ªÉm tra m∆∞a
            if "m∆∞a" in text_lower or "rain" in text_lower:
                if "rain" in main_condition or "rain" in description:
                    # Ki·ªÉm tra m·ª©c ƒë·ªô m∆∞a
                    if "m∆∞a to" in text_lower or "m∆∞a l·ªõn" in text_lower or "heavy rain" in text_lower:
                        if "heavy" in description or "torrential" in description:
                            return {
                                "conclusion": "TIN TH·∫¨T",
                                "reason": _as_str(f"Heuristic: OpenWeather API x√°c nh·∫≠n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}¬∞C) cho {weather_data.get('location')} ng√†y {weather_data.get('date')}."),
                                "style_analysis": "",
                                "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                                "key_evidence_source": _as_str(weather_item.get("source")),
                                "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                                "cached": False
                            }
                    else:
                        # M∆∞a th∆∞·ªùng
                        return {
                            "conclusion": "TIN TH·∫¨T",
                            "reason": _as_str(f"Heuristic: OpenWeather API x√°c nh·∫≠n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}¬∞C) cho {weather_data.get('location')} ng√†y {weather_data.get('date')}."),
                            "style_analysis": "",
                            "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                            "key_evidence_source": _as_str(weather_item.get("source")),
                            "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                            "cached": False
                        }
            # Ki·ªÉm tra n·∫Øng
            elif "n·∫Øng" in text_lower or "sunny" in text_lower or "clear" in text_lower:
                if "clear" in main_condition or "sunny" in description:
                    return {
                        "conclusion": "TIN TH·∫¨T",
                        "reason": _as_str(f"Heuristic: OpenWeather API x√°c nh·∫≠n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}¬∞C) cho {weather_data.get('location')} ng√†y {weather_data.get('date')}."),
                        "style_analysis": "",
                        "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                        "key_evidence_source": _as_str(weather_item.get("source")),
                        "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                        "cached": False
                    }
            # N·∫øu kh√¥ng kh·ªõp ƒëi·ªÅu ki·ªán c·ª• th·ªÉ, v·∫´n tr·∫£ v·ªÅ d·ªØ li·ªáu t·ª´ OpenWeather
            return {
                "conclusion": "TIN TH·∫¨T",
                "reason": _as_str(f"Heuristic: OpenWeather API cung c·∫•p d·ªØ li·ªáu th·ªùi ti·∫øt {weather_item.get('source')} - {description} ({weather_data.get('temperature')}¬∞C) cho {weather_data.get('location')} ng√†y {weather_data.get('date')}."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                "key_evidence_source": _as_str(weather_item.get("source")),
                "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                "cached": False
            }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PRIORITY 2: Ki·ªÉm tra ngu·ªìn L2 C√ì LI√äN QUAN ƒë·∫øn claim
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ quan tr·ªçng t·ª´ claim ƒë·ªÉ ki·ªÉm tra relevance
    person_keywords = []
    org_location_keywords = []
    
    # T√¨m t√™n ng∆∞·ªùi (vi·∫øt hoa, th∆∞·ªùng l√† t·ª´ ƒë·∫ßu ti√™n)
    name_pattern = re.compile(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b')
    names = name_pattern.findall(text_input)
    person_keywords.extend([n.lower() for n in names])
    
    # T√¨m t√™n t·ªï ch·ª©c/CLB/ƒë·ªãa ƒëi·ªÉm
    org_patterns = [
        (r'clb\s+(\w+\s*\w*)', 'clb'),
        (r'fc\s+(\w+\s*\w*)', 'fc'),
        (r'ƒë·ªôi\s+(\w+\s*\w*)', 'ƒë·ªôi'),
    ]
    for pat, prefix in org_patterns:
        match = re.search(pat, text_lower)
        if match:
            org_location_keywords.append(match.group(1).strip())
    
    # Th√™m c√°c ƒë·ªãa danh ph·ªï bi·∫øn
    location_names = ["h√† n·ªôi", "ha noi", "hanoi", "s√†i g√≤n", "saigon", "ho chi minh", 
                      "vi·ªát nam", "vietnam", "barca", "barcelona", "inter miami", "real madrid"]
    for loc in location_names:
        if loc in text_lower:
            org_location_keywords.append(loc)
    
    # Ki·ªÉm tra L2 sources c√≥ li√™n quan TH·ª∞C S·ª∞ kh√¥ng
    # ƒê·ªëi v·ªõi claim v·ªÅ ng∆∞·ªùi + t·ªï ch·ª©c: C·∫¶N KH·ªöP C·∫¢ HAI
    relevant_l2 = []
    has_person_org_claim = len(person_keywords) > 0 and len(org_location_keywords) > 0
    
    for item in l2:
        snippet = (item.get("snippet") or "").lower()
        title = (item.get("title") or "").lower()
        combined = snippet + " " + title
        
        if has_person_org_claim:
            # Claim c√≥ c·∫£ ng∆∞·ªùi + t·ªï ch·ª©c -> c·∫ßn kh·ªõp C·∫¢ HAI
            has_person = any(kw in combined for kw in person_keywords if kw and len(kw) > 2)
            has_org = any(kw in combined for kw in org_location_keywords if kw and len(kw) > 2)
            
            if has_person and has_org:
                relevant_l2.append(item)
        else:
            # Claim ƒë∆°n gi·∫£n -> ch·ªâ c·∫ßn kh·ªõp 1 keyword
            is_relevant = False
            all_keywords = person_keywords + org_location_keywords
            for kw in all_keywords:
                if kw and len(kw) > 2 and kw in combined:
                    is_relevant = True
                    break
            if is_relevant:
                relevant_l2.append(item)
    
    # Gi·∫£m y√™u c·∫ßu t·ª´ 2 xu·ªëng 1: Ch·ªâ c·∫ßn 1 ngu·ªìn uy t√≠n LI√äN QUAN TH·ª∞C S·ª∞ ƒë·ªÉ h·ªó tr·ª£ TIN TH·∫¨T
    if len(relevant_l2) >= 1:
        top = relevant_l2[0]
        return {
            "conclusion": "TIN TH·∫¨T",
            "debate_log": {
                "red_team_argument": "T√¥i kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng b√°c b·ªè.",
                "blue_team_argument": _as_str(f"C√≥ √≠t nh·∫•t 1 ngu·ªìn uy t√≠n x√°c nh·∫≠n: {top.get('source')}."),
                "judge_reasoning": "Blue Team th·∫Øng v·ªõi b·∫±ng ch·ª©ng t·ª´ ngu·ªìn uy t√≠n."
            },
            "confidence_score": 85,
            "reason": _as_str(f"C√≥ ngu·ªìn uy t√≠n x√°c nh·∫≠n th√¥ng tin n√†y ({top.get('source')})."),
            "style_analysis": "",
            "key_evidence_snippet": _as_str(top.get("snippet")),
            "key_evidence_source": _as_str(top.get("source")),
            "evidence_link": _as_str(top.get("url") or top.get("link")),
            "cached": False
        }
    
    # ƒê√É X√ìA: Block ƒë√°nh TIN GI·∫¢ khi "c√≥ L2 nh∆∞ng kh√¥ng li√™n quan"
    # ƒê√¢y l√† logic SAI: Kh√¥ng c√≥ evidence ‚â† Tin gi·∫£
    # Theo IFCN: Presumption of Truth - ch·ªâ TIN GI·∫¢ khi c√≥ B·∫∞NG CH·ª®NG B√ÅC B·ªé


    if is_weather_claim and l2:
        weather_sources = [item for item in l2 if _is_weather_source(item)]
        if weather_sources:
            top = weather_sources[0]
            return {
                "conclusion": "TIN TH·∫¨T",
                "reason": _as_str(f"Heuristic (weather): D·ª±a tr√™n ngu·ªìn d·ª± b√°o th·ªùi ti·∫øt {top.get('source')} ({top.get('date') or 'N/A'})."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(top.get("snippet")),
                "key_evidence_source": _as_str(top.get("source")),
                "evidence_link": _as_str(top.get("url") or top.get("link")),
                "cached": False
            }

    if is_weather_claim:
        layer3 = bundle.get("layer_3_general") or []
        weather_layer3 = [item for item in layer3 if _is_weather_source(item)]
        if weather_layer3:
            top = weather_layer3[0]
            return {
                "conclusion": "TIN TH·∫¨T",
                "reason": _as_str(f"Heuristic (weather): D·ª±a tr√™n trang d·ª± b√°o {top.get('source')} cho ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c n√™u."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(top.get("snippet")),
                "key_evidence_source": _as_str(top.get("source")),
                "evidence_link": _as_str(top.get("url") or top.get("link")),
                "cached": False
            }

    # Ph√°t hi·ªán th√¥ng tin g√¢y hi·ªÉu l·∫ßm do ƒë√£ c≈© (ƒë·∫∑c bi·ªát v·ªõi s·∫£n ph·∫©m/phi√™n b·∫£n)
    if not is_weather_claim:
        evidence_items = l2 + l3
        old_items = [item for item in evidence_items if item.get("is_old")]
        fresh_items = [item for item in evidence_items if item.get("is_old") is False]

        marketing_keywords = [
            "gi·∫£m gi√°", "khuy·∫øn m√£i", "sale", "ra m·∫Øt", "m·ªü b√°n", "ƒë·∫∑t tr∆∞·ªõc",
            "phi√™n b·∫£n", "model", "th·∫ø h·ªá", "ƒë·ªùi", "n√¢ng c·∫•p", "l√™n k·ªá", "∆∞u ƒë√£i",
            "launch", "promotion"
        ]
        product_pattern = re.compile(r"(iphone|ipad|macbook|galaxy|pixel|surface|playstation|xbox|sony|samsung|apple|oppo|xiaomi|huawei|vinfast)\s?[0-9a-z]{1,4}", re.IGNORECASE)
        mentions_product_cycle = any(kw in text_lower for kw in marketing_keywords) or bool(product_pattern.search(text_input))

        if old_items and (fresh_items or mentions_product_cycle):
            reference_old = old_items[0]
            old_source = reference_old.get("source") or reference_old.get("url") or "ngu·ªìn c≈©"
            old_date = reference_old.get("date") or "tr∆∞·ªõc ƒë√¢y"
            latest_snippet = _as_str(reference_old.get("snippet"))

            if fresh_items:
                latest_item = fresh_items[0]
                latest_source = latest_item.get("source") or latest_item.get("url") or "ngu·ªìn m·ªõi"
                latest_date = latest_item.get("date") or "g·∫ßn ƒë√¢y"
                reason = _as_str(
                    f"Th√¥ng tin v·ªÅ '{text_input}' d·ª±a tr√™n ngu·ªìn {old_source} ({old_date}) ƒë√£ c≈©, "
                    f"trong khi c√°c ngu·ªìn m·ªõi nh∆∞ {latest_source} ({latest_date}) cho th·∫•y b·ªëi c·∫£nh ƒë√£ thay ƒë·ªïi. "
                    "Vi·ªác tr√¨nh b√†y nh∆∞ tin n√≥ng d·ªÖ g√¢y hi·ªÉu l·∫ßm."
                )
            else:
                reason = _as_str(
                    f"Th√¥ng tin v·ªÅ '{text_input}' ch·ªâ ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi ngu·ªìn c≈© {old_source} ({old_date}). "
                    "S·∫£n ph·∫©m/s·ª± ki·ªán n√†y ƒë√£ xu·∫•t hi·ªán t·ª´ l√¢u n√™n vi·ªác tr√¨nh b√†y nh∆∞ tin t·ª©c m·ªõi l√† g√¢y hi·ªÉu l·∫ßm."
                )

            return {
                "conclusion": "TIN GI·∫¢",
                "reason": reason,
                "style_analysis": "Tin l·ªói th·ªùi",
                "key_evidence_snippet": latest_snippet,
                "key_evidence_source": _as_str(old_source),
                "evidence_link": _as_str(reference_old.get("url") or reference_old.get("link")),
                "cached": False
            }

        if mentions_product_cycle and fresh_items and not old_items:
            latest_item = fresh_items[0]
            latest_source = latest_item.get("source") or latest_item.get("url") or "ngu·ªìn m·ªõi"
            latest_date = latest_item.get("date") or "g·∫ßn ƒë√¢y"
            reason = _as_str(
                f"Kh√¥ng t√¨m th·∫•y ngu·ªìn g·∫ßn ƒë√¢y x√°c nh·∫≠n '{text_input}', trong khi c√°c s·∫£n ph·∫©m m·ªõi h∆°n ƒë√£ xu·∫•t hi·ªán "
                f"(v√≠ d·ª• {latest_source}, {latest_date}). ƒê√¢y l√† th√¥ng tin c≈© ƒë∆∞·ª£c l·∫∑p l·∫°i khi·∫øn ng∆∞·ªùi ƒë·ªçc hi·ªÉu l·∫ßm b·ªëi c·∫£nh hi·ªán t·∫°i."
            )
            return {
                "conclusion": "TIN GI·∫¢",
                "reason": reason,
                "style_analysis": "Tin l·ªói th·ªùi",
                "key_evidence_snippet": _as_str(latest_item.get("snippet")),
                "key_evidence_source": _as_str(latest_source),
                "evidence_link": _as_str(latest_item.get("url") or latest_item.get("link")),
                "cached": False
            }

        claim_implies_present = any(
            kw in text_lower
            for kw in [
                "hi·ªán nay", "b√¢y gi·ªù", "ƒëang", "s·∫Øp", "v·ª´a", "today", "now", "currently",
                "m·ªõi ƒë√¢y", "ngay l√∫c n√†y", "trong th·ªùi gian t·ªõi"
            ]
        )
        if claim_implies_present and old_items and not fresh_items:
            old_item = old_items[0]
            older_source = old_item.get("source") or old_item.get("url") or "ngu·ªìn c≈©"
            older_date = old_item.get("date") or "tr∆∞·ªõc ƒë√¢y"
            reason = _as_str(
                f"'{text_input}' √°m ch·ªâ th√¥ng tin ƒëang di·ªÖn ra nh∆∞ng ch·ªâ c√≥ ngu·ªìn {older_source} ({older_date}) t·ª´ tr∆∞·ªõc kia. "
                "Vi·ªác d√πng l·∫°i tin c≈© khi·∫øn ng∆∞·ªùi ƒë·ªçc hi·ªÉu sai v·ªÅ t√¨nh tr·∫°ng hi·ªán t·∫°i."
            )
            return {
                "conclusion": "TIN GI·∫¢",
                "reason": reason,
                "style_analysis": "Tin l·ªói th·ªùi",
                "key_evidence_snippet": _as_str(old_item.get("snippet")),
                "key_evidence_source": _as_str(older_source),
                "evidence_link": _as_str(old_item.get("url") or old_item.get("link")),
                "cached": False
            }

        misleading_tokens = [
            "ƒë√£ k·∫øt th√∫c", "ƒë√£ d·ª´ng", "ng·ª´ng √°p d·ª•ng", "kh√¥ng c√≤n √°p d·ª•ng",
            "ƒë√£ h·ªßy", "ƒë√£ ho√£n", "ƒë√£ ƒë√≥ng", "ƒë√£ ng∆∞ng", "no longer", "ended", "discontinued"
        ]
        for item in evidence_items:
            snippet_lower = (item.get("snippet") or "").lower()
            if any(token in snippet_lower for token in misleading_tokens):
                source = item.get("source") or item.get("url") or "ngu·ªìn c·∫≠p nh·∫≠t"
                reason = _as_str(
                    f"'{text_input}' b·ªè qua c·∫≠p nh·∫≠t t·ª´ {source} cho bi·∫øt s·ª± ki·ªán/ch∆∞∆°ng tr√¨nh ƒë√£ k·∫øt th√∫c ho·∫∑c thay ƒë·ªïi "
                    "n√™n th√¥ng tin d·ªÖ g√¢y hi·ªÉu l·∫ßm."
                )
                return {
                    "conclusion": "TIN GI·∫¢",
                    "reason": reason,
                    "style_analysis": "Tin ƒë√£ kh√¥ng c√≤n ƒë√∫ng",
                    "key_evidence_snippet": _as_str(item.get("snippet")),
                    "key_evidence_source": _as_str(source),
                    "evidence_link": _as_str(item.get("url") or item.get("link")),
                    "cached": False
                }

    # FIX: M·∫∑c ƒë·ªãnh TIN TH·∫¨T khi kh√¥ng c√≥ b·∫±ng ch·ª©ng B√ÅC B·ªé (innocent until proven guilty)
    # Tr∆∞·ªõc ƒë√¢y m·∫∑c ƒë·ªãnh TIN GI·∫¢ g√¢y false positive cao
    return {
        "conclusion": "TIN TH·∫¨T",
        "confidence_score": 60,
        "reason": _as_str("Kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng B√ÅC B·ªé th√¥ng tin n√†y. D·ª±a tr√™n nguy√™n t·∫Øc 'innocent until proven guilty'."),
        "debate_log": {
            "red_team_argument": "Kh√¥ng t√¨m th·∫•y b·∫±ng ch·ª©ng ph·∫£n b√°c r√µ r√†ng.",
            "blue_team_argument": "Kh√¥ng c√≥ ngu·ªìn n√†o b√°c b·ªè th√¥ng tin n√†y.",
            "judge_reasoning": "Khi kh√¥ng c√≥ b·∫±ng ch·ª©ng b√°c b·ªè, tin ƒë∆∞·ª£c coi l√† c√≥ th·ªÉ ƒë√∫ng."
        },
        "style_analysis": "",
        "key_evidence_snippet": "",
        "key_evidence_source": "",
        "evidence_link": "",
        "cached": False
    }


def _normalize_agent2_model(model_key: str | None) -> str:
    """Normalize Agent 2 model identifier."""
    if not model_key:
        return "models/gemini-2.5-pro"
    mapping = {
        "gemini_flash": "models/gemini-2.5-flash",
        "gemini flash": "models/gemini-2.5-flash",
        "gemini-2.5-flash": "models/gemini-2.5-flash",
        "models/gemini_flash": "models/gemini-2.5-flash",
        "gemini_pro": "models/gemini-2.5-pro",
        "gemini pro": "models/gemini-2.5-pro",
        "models/gemini-2.5-pro": "models/gemini-2.5-pro",
        "openai/gpt-oss-120b": "openai/gpt-oss-120b",
        "meta-llama/llama-3.3-70b-instruct": "meta-llama/llama-3.3-70b-instruct",
        "qwen/qwen-2.5-72b-instruct": "qwen/qwen-2.5-72b-instruct",
        "gemma-3-1b": "models/gemma-3-1b-it",
        "gemma-3-1b-it": "models/gemma-3-1b-it",
        "gemma-3-2b": "models/gemma-3-4b-it",  # 2B not available, fallback to 4B
        "gemma-3-4b": "models/gemma-3-4b-it",
        "gemma-3-4b-it": "models/gemma-3-4b-it",
        "gemma-3-12b": "models/gemma-3-12b-it",
        "gemma-3-12b-it": "models/gemma-3-12b-it",
        "gemma-3-27b": "models/gemma-3-27b-it",
        "gemma-3-27b-it": "models/gemma-3-27b-it",
        "google/gemma-3-1b": "models/gemma-3-1b-it",
        "google/gemma-3-2b": "models/gemma-3-4b-it",
        "google/gemma-3-4b": "models/gemma-3-4b-it",
        "google/gemma-3-12b": "models/gemma-3-12b-it",
        "google/gemma-3-27b": "models/gemma-3-27b-it",
        "models/gemma-3-1b": "models/gemma-3-1b-it",
        "models/gemma-3-2b": "models/gemma-3-4b-it",
        "models/gemma-3-4b": "models/gemma-3-4b-it",
        "models/gemma-3-12b": "models/gemma-3-12b-it",
        "models/gemma-3-27b": "models/gemma-3-27b-it",
        "models/gemma-3-1b-it": "models/gemma-3-1b-it",
        "models/gemma-3-4b-it": "models/gemma-3-4b-it",
        "models/gemma-3-12b-it": "models/gemma-3-12b-it",
        "models/gemma-3-27b-it": "models/gemma-3-27b-it",
        "models/gemma-3n-e2b-it": "models/gemma-3n-e2b-it",
        "models/gemma-3n-e4b-it": "models/gemma-3n-e4b-it",
    }
    return mapping.get(model_key, model_key)


def _detect_agent2_provider(model_name: str) -> str:
    """Detect provider for Agent 2 model."""
    if not model_name:
        return "gemini"
    lowered = model_name.lower()
    if "gemini" in lowered or "gemma" in lowered or model_name.startswith("models/"):
        return "gemini"
    # All Agent 2 models now use Gemini API
    return "gemini"

async def execute_final_analysis(
    text_input: str,
    evidence_bundle: dict,
    current_date: str,
    model_key: str | None = None,
    flash_mode: bool = False,
    site_query_string: str = "",  # Added for re-search
) -> dict:
    """
    Pipeline OPTIMIZED: SYNTH ‚Üí CRITIC ‚Üí JUDGE
    
    SYNTH Logic:
    - KNOWLEDGE claims: Agent c√≥ quy·ªÅn t·ª± quy·∫øt d·ª±a tr√™n ki·∫øn th·ª©c
    - NEWS claims: B·∫Øt bu·ªôc ph·∫£i c√≥ evidence
    
    Optimizations applied:
    - Reduced evidence bundle size (3/3/2 items)
    - Reduced snippet length (200 chars)
    - Reduced timeouts (30s/40s)
    - Simplified prompts
    """
    if not SYNTHESIS_PROMPT:
        raise ValueError("Synthesis prompt (prompt 2) ch∆∞a ƒë∆∞·ª£c t·∫£i.")
    if not CRITIC_PROMPT:
        print("WARNING: Critic prompt ch∆∞a ƒë∆∞·ª£c t·∫£i, d√πng m·∫∑c ƒë·ªãnh.")

    # =========================================================================
    # SYNTH: ƒê·ªÉ LLM t·ª± ph√¢n lo·∫°i claim (kh√¥ng d√πng pattern c·ª©ng)
    # =========================================================================
    claim_type = _classify_claim_type(text_input)
    print(f"\n[SYNTH] Claim type: {claim_type}")
    
    # AUTO: ƒê·ªÉ LLM t·ª± quy·∫øt ƒë·ªãnh d·ª±a tr√™n context
    synth_instruction = (
        "\n\n[SYNTH INSTRUCTION]\n"
        "H√£y T·ª∞ PH√ÇN LO·∫†I claim n√†y:\n"
        "- KNOWLEDGE: Ki·∫øn th·ª©c c·ªë ƒë·ªãnh (ƒë·ªãa l√Ω, khoa h·ªçc, ƒë·ªãnh nghƒ©a) ‚Üí C√≥ th·ªÉ t·ª± suy lu·∫≠n\n"
        "- NEWS: Tin t·ª©c, s·ª± ki·ªán, tuy√™n b·ªë ‚Üí C·∫ßn evidence\n\n"
        "Sau ƒë√≥ √°p d·ª•ng:\n"
        "- N·∫øu KNOWLEDGE: T·ª± quy·∫øt d·ª±a tr√™n ki·∫øn th·ª©c n·ªôi t·∫°i\n"
        "- N·∫øu NEWS: B·∫Øt bu·ªôc c√≥ evidence ƒë·ªÉ k·∫øt lu·∫≠n\n"
        "- N·∫øu kh√¥ng c√≥ evidence b√°c b·ªè ‚Üí PRESUMPTION OF TRUTH (TIN TH·∫¨T)\n"
    )
    print(f"[SYNTH] LLM s·∫Ω t·ª± ph√¢n lo·∫°i v√† quy·∫øt ƒë·ªãnh")

    # Trim evidence before sending to models
    trimmed_bundle = _trim_evidence_bundle(evidence_bundle)
    evidence_bundle_json = json.dumps(trimmed_bundle, indent=2, ensure_ascii=False)

    # =========================================================================
    # PHASE 1: CRITIC AGENT (BI·ªÜN L√ù ƒê·ªêI L·∫¨P)
    # =========================================================================
    critic_report = "Kh√¥ng c√≥ ph·∫£n bi·ªán."
    critic_parsed = {}
    try:
        print(f"\n[CRITIC] B·∫Øt ƒë·∫ßu ph·∫£n bi·ªán...")
        critic_prompt_filled = CRITIC_PROMPT.replace("{text_input}", text_input)
        critic_prompt_filled = critic_prompt_filled.replace("{evidence_bundle_json}", evidence_bundle_json)
        critic_prompt_filled = critic_prompt_filled.replace("{current_date}", current_date)
        
        critic_report = await call_agent_with_capability_fallback(
            role="CRITIC",
            prompt=critic_prompt_filled,
            temperature=0.5,
            timeout=120.0  # TƒÉng l√™n 120s theo y√™u c·∫ßu user
        )
        print(f"[CRITIC] Report: {critic_report[:150]}...")
        
        # Parse CRITIC response ƒë·ªÉ ki·ªÉm tra counter_search_needed
        critic_parsed = _parse_json_from_text(critic_report)
        
        # NEW SCHEMA: Ki·ªÉm tra issues_found tr·ª±c ti·∫øp (kh√¥ng qua conclusion.issues_found)
        critic_issues = critic_parsed.get("issues_found", False)
        if not critic_issues:
            # Fallback: check old schema
            conclusion_obj = critic_parsed.get("conclusion", {})
            if isinstance(conclusion_obj, dict):
                critic_issues = conclusion_obj.get("issues_found", False)
        
        issue_type = critic_parsed.get("issue_type", "NONE")
        if not issue_type or issue_type == "NONE":
            conclusion_obj = critic_parsed.get("conclusion", {})
            if isinstance(conclusion_obj, dict):
                issue_type = conclusion_obj.get("issue_type", "NONE")
        
        print(f"[CRITIC] Issues found: {critic_issues}, Type: {issue_type}")
        
    except Exception as e:
        print(f"[CRITIC] G·∫∑p l·ªói: {e}")
        critic_report = "L·ªói khi ch·∫°y Critic Agent."

    # =========================================================================
    # PHASE 1.5: CRITIC COUNTER-SEARCH (n·∫øu CRITIC y√™u c·∫ßu search th√™m)
    # =========================================================================
    if critic_parsed.get("counter_search_needed", False):
        counter_queries = critic_parsed.get("counter_search_queries", [])
        if counter_queries:
            print(f"\n[CRITIC-SEARCH] CRITIC y√™u c·∫ßu search th√™m: {counter_queries}")
            try:
                from app.search import call_google_search
                
                critic_counter_evidence = []
                for query in counter_queries[:2]:  # Gi·ªõi h·∫°n 2 queries
                    results = call_google_search(query, "")
                    critic_counter_evidence.extend(results[:5])
                    if len(critic_counter_evidence) >= 5:
                        break
                
                if critic_counter_evidence:
                    print(f"[CRITIC-SEARCH] T√¨m th·∫•y {len(critic_counter_evidence)} evidence m·ªõi")
                    # Merge v√†o evidence bundle
                    if "layer_2_high_trust" not in evidence_bundle:
                        evidence_bundle["layer_2_high_trust"] = []
                    evidence_bundle["layer_2_high_trust"].extend(critic_counter_evidence[:3])
                    
                    # Update evidence_bundle_json cho JUDGE
                    trimmed_bundle = _trim_evidence_bundle(evidence_bundle)
                    evidence_bundle_json = json.dumps(trimmed_bundle, indent=2, ensure_ascii=False)
                    
            except Exception as e:
                print(f"[CRITIC-SEARCH] L·ªói search: {e}")

    # =========================================================================
    # PHASE 2: JUDGE AGENT (TH·∫®M PH√ÅN) - Round 1
    # =========================================================================
    judge_result = {}
    try:
        print(f"\n[JUDGE] B·∫Øt ƒë·∫ßu ph√°n quy·∫øt Round 1...")
        judge_prompt_filled = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
        judge_prompt_filled = judge_prompt_filled.replace("{evidence_bundle_json}", evidence_bundle_json)
        judge_prompt_filled = judge_prompt_filled.replace("{current_date}", current_date)
        
        # Add SYNTH instruction and CRITIC report
        judge_prompt_filled += synth_instruction
        judge_prompt_filled += f"\n\n[√ù KI·∫æN BI·ªÜN L√ù (CRITIC)]:\n{critic_report}"
        
        judge_text = await call_agent_with_capability_fallback(
            role="JUDGE",
            prompt=judge_prompt_filled,
            temperature=0.1,  # Strict logic
            timeout=120.0  # TƒÉng l√™n 120s theo y√™u c·∫ßu user
        )
        
        judge_result = _parse_json_from_text(judge_text)

        # ---------------------------------------------------------------------
        # ADAPTER: Convert to Flat Schema (Support BOTH old and new schemas)
        # ---------------------------------------------------------------------
        
        # NEW SCHEMA (simpler): conclusion, confidence_score at top level
        if not judge_result.get("conclusion"):
            # Try verdict_metadata (old schema)
            verdict_meta = judge_result.get("verdict_metadata")
            if verdict_meta and isinstance(verdict_meta, dict):
                judge_result["conclusion"] = verdict_meta.get("conclusion")
                judge_result["confidence_score"] = verdict_meta.get("probability_score")
        
        # NEW SCHEMA: key_evidence -> key_evidence_snippet, key_evidence_source
        key_ev = judge_result.get("key_evidence")
        if key_ev and isinstance(key_ev, dict):
            judge_result["key_evidence_snippet"] = key_ev.get("quote", "N/A")
            judge_result["key_evidence_source"] = key_ev.get("source", "N/A")
        
        # NEW SCHEMA: critic_response -> debate_log
        critic_resp = judge_result.get("critic_response")
        if critic_resp and isinstance(critic_resp, dict):
            judge_result["debate_log"] = {
                "critic_found_issues": critic_resp.get("critic_found_issues", False),
                "judge_agrees": critic_resp.get("judge_agrees", True),
                "judge_reasoning": critic_resp.get("judge_reasoning", "N/A")
            }
        
        # Fallback for reason
        if not judge_result.get("reason"):
            for key in ["reasoning", "explanation", "rationale", "analysis", "summary"]:
                if judge_result.get(key):
                    judge_result["reason"] = str(judge_result[key])
                    break
        
        # Final log
        if judge_result.get("conclusion"):
            conf = judge_result.get("confidence_score", "N/A")
            print(f"[JUDGE] Round 1: {judge_result.get('conclusion')} ({conf}%)")
        else:
            print(f"[JUDGE] WARNING: No valid conclusion. Fallback to heuristic.")
        # ---------------------------------------------------------------------
    except Exception as e:
        print(f"[JUDGE] G·∫∑p l·ªói Round 1: {e}")
        return _heuristic_summarize(text_input, evidence_bundle, current_date)


    # =========================================================================
    # PHASE 2.5: COUNTER-SEARCH (T√¨m d·∫´n ch·ª©ng B·∫¢O V·ªÜ claim tr∆∞·ªõc khi k·∫øt lu·∫≠n TIN GI·∫¢)
    # =========================================================================
    # N·∫øu JUDGE Round 1 k·∫øt lu·∫≠n TIN GI·∫¢ ‚Üí Search th√™m ƒë·ªÉ t√¨m d·∫´n ch·ª©ng ·ªßng h·ªô claim
    # ƒê√¢y l√† c∆° h·ªôi "ph·∫£n bi·ªán l·∫°i CRITIC" b·∫±ng b·∫±ng ch·ª©ng m·ªõi
    
    conclusion_r1 = normalize_conclusion(judge_result.get("conclusion", ""))
    
    # SPEED OPTIMIZATION: B·ªè qua COUNTER-SEARCH n·∫øu flag t·∫Øt
    if ENABLE_COUNTER_SEARCH and conclusion_r1 == "TIN GI·∫¢":
        print(f"\n[COUNTER-SEARCH] JUDGE Round 1 k·∫øt lu·∫≠n TIN GI·∫¢ ‚Üí T√¨m d·∫´n ch·ª©ng B·∫¢O V·ªÜ claim...")
        
        try:
            from app.search import call_google_search, _is_international_event, _extract_english_query
            
            # IMPROVED: Multi-language counter queries
            counter_queries = []
            
            # 1. Vietnamese confirmation query
            counter_queries.append(f"{text_input} tin t·ª©c ch√≠nh th·ªëng")
            
            # 2. English for international events (key improvement)
            if _is_international_event(text_input):
                en_text = _extract_english_query(text_input)
                if en_text and len(en_text) > 10:
                    counter_queries.append(f"{en_text} confirmed official")
                    counter_queries.append(f"{en_text} news Reuters AP")
            else:
                counter_queries.append(f"{text_input} Reuters AFP BBC")
            
            counter_evidence = []
            for query in counter_queries[:2]:  # Ch·ªâ 2 queries ƒë·ªÉ nhanh
                results = call_google_search(query, "")
                counter_evidence.extend(results[:5])
                if len(counter_evidence) >= 5:
                    break
            
            if counter_evidence:
                print(f"[COUNTER-SEARCH] T√¨m th·∫•y {len(counter_evidence)} d·∫´n ch·ª©ng c√≥ th·ªÉ ·ªßng h·ªô claim")
                
                # T·∫°o evidence bundle m·ªõi v·ªõi counter-evidence
                counter_bundle = {
                    "layer_1_tools": evidence_bundle.get("layer_1_tools", []),
                    "layer_2_high_trust": counter_evidence[:5],
                    "layer_3_general": evidence_bundle.get("layer_3_general", []),
                    "layer_4_social_low": []
                }
                counter_evidence_json = json.dumps(_trim_evidence_bundle(counter_bundle), indent=2, ensure_ascii=False)
                
                # JUDGE Round 1.5: Xem x√©t l·∫°i v·ªõi d·∫´n ch·ª©ng m·ªõi
                print(f"[JUDGE] Round 1.5: Xem x√©t l·∫°i v·ªõi d·∫´n ch·ª©ng m·ªõi...")
                
                counter_prompt = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
                counter_prompt = counter_prompt.replace("{evidence_bundle_json}", counter_evidence_json)
                counter_prompt = counter_prompt.replace("{current_date}", current_date)
                counter_prompt += f"""

[COUNTER-SEARCH EVIDENCE - QUAN TR·ªåNG]
ƒê√£ t√¨m th√™m d·∫´n ch·ª©ng t·ª´ ngu·ªìn tin ch√≠nh th·ªëng. H√£y xem x√©t l·∫°i k·∫øt lu·∫≠n.

[NGUY√äN T·∫ÆC B·∫ÆT BU·ªòC - ANTI-HALLUCINATION]
1. B·∫†N B·∫ÆT BU·ªòC ph·∫£i d·ª±a v√†o evidence trong bundle, KH√îNG ƒê∆Ø·ª¢C t·ª± suy di·ªÖn
2. N·∫øu evidence m·ªõi X√ÅC NH·∫¨N claim (c√≥ ngu·ªìn uy t√≠n ƒë∆∞a tin) ‚Üí B·∫ÆT BU·ªòC TIN TH·∫¨T
3. "Kh√¥ng t√¨m th·∫•y evidence" ‚â† TIN GI·∫¢ (Innocent until proven guilty)
4. CH·ªà k·∫øt lu·∫≠n TIN GI·∫¢ n·∫øu c√≥ b·∫±ng ch·ª©ng B√ÅC B·ªé TR·ª∞C TI·∫æP claim
5. Tin qu·ªëc t·∫ø c√≥ th·ªÉ ƒë∆∞·ª£c Reuters/AP/BBC ƒë∆∞a tin tr∆∞·ªõc b√°o VN

[CRITIC FEEDBACK TR∆Ø·ªöC ƒê√ì]
{critic_report}
"""
                
                counter_text = await call_agent_with_capability_fallback(
                    role="JUDGE",
                    prompt=counter_prompt,
                    temperature=0.1,
                    timeout=25.0  # Same as JUDGE
                )
                
                counter_result = _parse_json_from_text(counter_text)
                
                # Parse k·∫øt qu·∫£
                if counter_result.get("verdict_metadata"):
                    counter_conclusion = counter_result["verdict_metadata"].get("conclusion")
                    counter_confidence = counter_result["verdict_metadata"].get("probability_score")
                else:
                    counter_conclusion = counter_result.get("conclusion")
                    counter_confidence = counter_result.get("confidence_score")
                
                counter_conclusion = normalize_conclusion(counter_conclusion or "")
                
                print(f"[JUDGE] Round 1.5: {counter_conclusion} ({counter_confidence}%)")
                
                # N·∫øu Counter-Search ƒë·ªïi √Ω ‚Üí C·∫≠p nh·∫≠t judge_result
                if counter_conclusion == "TIN TH·∫¨T":
                    print(f"[COUNTER-SEARCH] ‚úÖ Counter-evidence ƒë√£ thay ƒë·ªïi k·∫øt lu·∫≠n: TIN GI·∫¢ ‚Üí TIN TH·∫¨T")
                    judge_result["conclusion"] = "TIN TH·∫¨T"
                    judge_result["confidence_score"] = counter_confidence or 75
                    judge_result["reason"] = (judge_result.get("reason", "") + 
                        f"\n\n[COUNTER-SEARCH] Sau khi t√¨m th√™m d·∫´n ch·ª©ng, claim ƒë∆∞·ª£c x√°c nh·∫≠n l√† TIN TH·∫¨T.")
                else:
                    print(f"[COUNTER-SEARCH] ‚ùå Counter-evidence kh√¥ng thay ƒë·ªïi k·∫øt lu·∫≠n, gi·ªØ TIN GI·∫¢")
            else:
                print(f"[COUNTER-SEARCH] Kh√¥ng t√¨m th·∫•y d·∫´n ch·ª©ng m·ªõi")
                
        except Exception as e:
            print(f"[COUNTER-SEARCH] L·ªói: {e}")

    # =========================================================================
    # PHASE 3: SELF-CORRECTION (RE-SEARCH LOOP)
    # =========================================================================
    
    # FIX: Parse confidence an to√†n - default 50 (neutral) thay v√¨ 0 ƒë·ªÉ tr√°nh trigger re-search sai
    confidence = 50  # Neutral default
    raw_confidence = judge_result.get("confidence_score")
    if raw_confidence is not None:
        try:
            confidence = int(raw_confidence)
        except (ValueError, TypeError):
            confidence = 50  # Keep neutral if parse fails
            print(f"[SELF-CORRECTION] Warning: Could not parse confidence '{raw_confidence}', using default 50")
    else:
        print(f"[SELF-CORRECTION] Warning: No confidence_score in judge result, using default 50")
    
    # FIX: needs_more_evidence ph·∫£i l√† True EXPLICIT, kh√¥ng ph·∫£i ch·ªâ v√¨ confidence th·∫•p do parse l·ªói    
    needs_more = judge_result.get("needs_more_evidence", False)
    if not isinstance(needs_more, bool):
        needs_more = str(needs_more).lower() == "true"
    
    # K√≠ch ho·∫°t Re-search n·∫øu:
    # 1. Judge Y√äU C·∫¶U EXPLICIT (needs_more_evidence = True) - ∆∞u ti√™n cao nh·∫•t
    # 2. Ho·∫∑c Confidence < 40 (r·∫•t th·∫•p, kh√¥ng ph·∫£i do parse fail)
    # 3. V√Ä ch∆∞a ph·∫£i l√† tin th·ªùi ti·∫øt (th·ªùi ti·∫øt th∆∞·ªùng check 1 l·∫ßn l√† ƒë·ªß)
    # 4. V√Ä judge_result kh√¥ng r·ªóng (c√≥ k·∫øt qu·∫£ th·ª±c s·ª±)
    is_weather = "th·ªùi ti·∫øt" in judge_result.get("claim_type", "").lower()
    has_valid_result = bool(judge_result.get("conclusion"))
    
    # FIX: Ch·ªâ trigger re-search khi TH·ª∞C S·ª∞ c·∫ßn, kh√¥ng ph·∫£i do parse error
    # =========================================================================
    # PHASE 3: UNIFIED RE-SEARCH & CORRECTION
    # =========================================================================
    # SPEED & ACCURACY OPTIMIZATION: G·ªôp Counter-Search v√† Self-Correction.
    # K√≠ch ho·∫°t Re-search n·∫øu:
    # 1. JUDGE Round 1 k·∫øt lu·∫≠n TIN GI·∫¢ (T√¨m d·∫´n ch·ª©ng B·∫¢O V·ªÜ)
    # 2. Ho·∫∑c JUDGE y√™u c·∫ßu explicit (needs_more_evidence = True)
    # 3. Ho·∫∑c Confidence r·∫•t th·∫•p (< 40%)
    # 4. HO·∫∂C C√≥ s·ª± m√¢u thu·∫´n l·ªõn gi·ªØa CRITIC v√† JUDGE (Adversarial Mismatch)
    
    conclusion_r1 = normalize_conclusion(judge_result.get("conclusion", ""))
    confidence_r1 = 50
    try:
        conf_val = judge_result.get("confidence_score")
        if conf_val is not None:
            confidence_r1 = int(conf_val)
    except:
        pass
        
    needs_more_r1 = judge_result.get("needs_more_evidence", False)
    if not isinstance(needs_more_r1, bool):
        needs_more_r1 = str(needs_more_r1).lower() == "true"
        
    critic_found_issues = critic_parsed.get("conclusion", {}).get("issues_found", False)
    # M·∫´u thu·∫´n: CRITIC b·∫£o OK nh∆∞ng JUDGE b·∫£o SAI, ho·∫∑c ng∆∞·ª£c l·∫°i
    adversarial_mismatch = (critic_found_issues and conclusion_r1 == "TIN TH·∫¨T") or (not critic_found_issues and conclusion_r1 == "TIN GI·∫¢")
    
    is_weather = "th·ªùi ti·∫øt" in judge_result.get("claim_type", "").lower()
    
    should_unified_research = (
        ENABLE_SELF_CORRECTION and (
            (conclusion_r1 == "TIN GI·∫¢" and ENABLE_COUNTER_SEARCH) # Phase 2.5 logic
            or needs_more_r1 # Phase 3 logic
            or confidence_r1 < 40 # Phase 3 logic
            or adversarial_mismatch # New logic
        ) and not is_weather
    )
    
    if should_unified_research:
        print(f"\n[UNIFIED-RE-SEARCH] K√≠ch ho·∫°t (REASON: {'TIN GI·∫¢' if conclusion_r1 == 'TIN GI·∫¢' else 'Needs More' if needs_more_r1 else 'Low Conf' if confidence_r1 < 40 else 'Adversarial Mismatch'})")
        
        # Thu th·∫≠p t·∫•t c·∫£ queries ti·ªÅm nƒÉng
        unified_queries = []
        
        # 1. Queries t·ª´ JUDGE
        unified_queries.extend(judge_result.get("additional_search_queries", []))
        unified_queries.extend(judge_result.get("verification_search_queries", []))
        
        # 2. N·∫øu l√† TIN GI·∫¢, th√™m c√°c queries mang t√≠nh "b·∫£o v·ªá" (Support Search)
        if conclusion_r1 == "TIN GI·∫¢":
            # IMPROVED: Multi-language support
            from app.search import _is_international_event, _extract_english_query
            
            unified_queries.append(f"{text_input} tin t·ª©c ch√≠nh th·ªëng")
            
            if _is_international_event(text_input):
                en_text = _extract_english_query(text_input)
                if en_text and len(en_text) > 10:
                    unified_queries.append(f"{en_text} confirmed Reuters AP")
                    unified_queries.append(f"{en_text} official news")
            else:
                unified_queries.append(f"{text_input} official news")
            
        # 3. Fallback queries
        if not unified_queries:
            unified_queries = [f"{text_input} fact check", f"{text_input} news"]
            
        # Unique and limit queries (gi·ªõi h·∫°n 3 queries ƒë·ªÉ nhanh)
        unique_queries = []
        for q in unified_queries:
            if q and q not in unique_queries:
                unique_queries.append(q)
        unique_queries = unique_queries[:3]
        
        print(f"[UNIFIED-RE-SEARCH] Queries: {unique_queries}")
        
        try:
            # Execute search
            re_search_plan = {
                "required_tools": [{
                    "tool_name": "search",
                    "parameters": {"queries": unique_queries}
                }]
            }
            new_evidence = await execute_tool_plan(re_search_plan, site_query_string, flash_mode)
            
            # Merge evidence (safe initialization)
            for layer in ["layer_2_high_trust", "layer_3_general", "layer_4_social_low"]:
                if layer not in evidence_bundle: evidence_bundle[layer] = []
                evidence_bundle[layer].extend(new_evidence.get(layer, []))
            
            # Remove duplicates by URL
            seen_urls = {item.get("url") or item.get("link") for item in (evidence_bundle.get("layer_2_high_trust") or [])}
            # Trim evidence
            trimmed_bundle_v2 = _trim_evidence_bundle(evidence_bundle)
            evidence_bundle_json_v2 = json.dumps(trimmed_bundle_v2, indent=2, ensure_ascii=False)
            
            # Re-Run JUDGE Round 2
            print(f"\n[JUDGE] B·∫Øt ƒë·∫ßu ph√°n quy·∫øt Round 2 (Final)...")
            judge_prompt_v2 = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
            judge_prompt_v2 = judge_prompt_v2.replace("{evidence_bundle_json}", evidence_bundle_json_v2)
            judge_prompt_v2 = judge_prompt_v2.replace("{current_date}", current_date)
            judge_prompt_v2 += f"\n\n[√ù KI·∫æN CRITIC & K·∫æT QU·∫¢ R1]:\nCRITIC: {critic_report}\nR1 CONCLUSION: {conclusion_r1} ({confidence_r1}%)\n\n[INSTRUCTION]: H√£y xem x√©t b·∫±ng ch·ª©ng m·ªõi ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n cu·ªëi c√πng ch√≠nh x√°c nh·∫•t."
            
            judge_result_r1_backup = judge_result.copy()
            
            judge_text_v2 = await call_agent_with_capability_fallback(
                role="JUDGE",
                prompt=judge_prompt_v2,
                temperature=0.1,
                timeout=80.0
            )
            
            judge_result_r2 = _parse_json_from_text(judge_text_v2)
            
            # Adapter Round 2
            verdict_meta_v2 = judge_result_r2.get("verdict_metadata")
            if verdict_meta_v2:
                judge_result_r2["conclusion"] = verdict_meta_v2.get("conclusion")
                judge_result_r2["confidence_score"] = verdict_meta_v2.get("probability_score")
                
                exec_summary = judge_result_r2.get("executive_summary") or {}
                dialectical = judge_result_r2.get("dialectical_analysis") or {}
                synthesis = dialectical.get("synthesis") or exec_summary.get("bluf")
                
                combined_reason = ""
                citations = judge_result_r2.get("key_evidence_citations") or []
                if citations:
                    cite = citations[0]
                    combined_reason = f"C·∫≠p nh·∫≠t b·∫±ng ch·ª©ng m·ªõi t·ª´ {cite.get('source')}: \"{cite.get('quote', '')[:100]}...\". "
                
                judge_result_r2["reason"] = (combined_reason + (synthesis or "")).strip()

            else:
                # Fallback flat schema R2
                if not judge_result_r2.get("conclusion"):
                    judge_result_r2["conclusion"] = judge_result_r2.get("final_conclusion") or judge_result_r2.get("verdict")
                if not judge_result_r2.get("reason"):
                    judge_result_r2["reason"] = judge_result_r2.get("reasoning") or judge_result_r2.get("explanation")
            
            # C·∫≠p nh·∫≠t k·∫øt qu·∫£ n·∫øu R2 h·ª£p l·ªá
            if judge_result_r2.get("conclusion"):
                judge_result = judge_result_r2
                judge_result["cached"] = False
                print(f"[JUDGE] Round 2 Success: {judge_result.get('conclusion')} ({judge_result.get('confidence_score')}%)")
            else:
                print("[JUDGE] Round 2 failed or invalid, keeping Round 1 results.")
                judge_result = judge_result_r1_backup
                
        except Exception as e:
            print(f"[UNIFIED-RE-SEARCH] Error: {e}")
            judge_result = judge_result_r1_backup
    else:
        print("[SELF-CORRECTION] Kh√¥ng k√≠ch ho·∫°t c√°c v√≤ng ph·ª• (Fast Lane).")

    # Post-processing normalization
    if judge_result:
        # Map old schema keys if needed (fallback)
        if "final_conclusion" in judge_result and "conclusion" not in judge_result:
            judge_result["conclusion"] = judge_result["final_conclusion"]
            
        judge_result["conclusion"] = normalize_conclusion(judge_result.get("conclusion"))
        return judge_result

    # Fallback final
    return _heuristic_summarize(text_input, evidence_bundle, current_date)

