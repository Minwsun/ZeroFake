# app/agent_synthesizer.py

import os
import json
import re
from dotenv import load_dotenv
from typing import Dict, Any, List

from app.weather import classify_claim
from app.model_clients import (
    call_gemini_model,
    call_agent_with_capability_fallback,
    ModelClientError,
    RateLimitError,
)
from app.tool_executor import execute_tool_plan  # Import for Re-Search

load_dotenv()


GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SYNTHESIS_PROMPT = ""
CRITIC_PROMPT = ""  # NEW: Prompt cho CRITIC agent


# CÃ i Ä‘áº·t an toÃ n
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]


WEATHER_SOURCE_KEYWORDS = [
    "weather",
    "forecast",
    "accuweather",
    "windy",
    "meteoblue",
    "ventusky",
    "nchmf",
    "thoitiet",
    "openweathermap",
    "wunderground",
    "metoffice",
    "bom.gov",
]


# ==============================================================================
# SYNTH LOGIC: PhÃ¢n loáº¡i claim Ä‘á»ƒ quyáº¿t Ä‘á»‹nh quyá»n tá»± quyáº¿t cá»§a Agent
# ==============================================================================

def _classify_claim_type(text_input: str) -> str:
    """
    SYNTH: PhÃ¢n loáº¡i claim thÃ nh 2 loáº¡i:
    
    - "KNOWLEDGE": Kiáº¿n thá»©c (Ä‘á»‹a lÃ½, khoa há»c, Ä‘á»‹nh nghÄ©a)
      â†’ Agent cÃ³ quyá»n Tá»° QUYáº¾T dá»±a trÃªn kiáº¿n thá»©c ná»™i táº¡i
      â†’ KHÃ”NG báº¯t buá»™c pháº£i cÃ³ evidence
      
    - "NEWS": Tin tá»©c (sá»± kiá»‡n, tuyÃªn bá»‘, thÃ´ng tin thá»i sá»±)
      â†’ Báº®T BUá»˜C pháº£i cÃ³ evidence Ä‘á»ƒ káº¿t luáº­n
      â†’ KhÃ´ng cÃ³ evidence = khÃ´ng thá»ƒ káº¿t luáº­n cháº¯c cháº¯n
    """
    text_lower = text_input.lower()
    
    # KNOWLEDGE patterns - Agent cÃ³ thá»ƒ tá»± quyáº¿t
    knowledge_patterns = [
        # Äá»‹a lÃ½ cá»‘ Ä‘á»‹nh
        r"(thá»§ Ä‘Ã´|thá»§ phá»§|thÃ nh phá»‘ lá»›n nháº¥t|diá»‡n tÃ­ch|biÃªn giá»›i|giÃ¡p vá»›i)",
        r"(chÃ¢u lá»¥c|biá»ƒn|Ä‘áº¡i dÆ°Æ¡ng|sÃ´ng|nÃºi|há»“|sa máº¡c|rá»«ng)",
        r"(quá»‘c gia|nÆ°á»›c|tá»‰nh|vÃ¹ng miá»n)",
        # DÃ¢n sá»‘/DÃ¢n tá»™c
        r"(dÃ¢n sá»‘|dÃ¢n tá»™c|ngÃ´n ngá»¯ chÃ­nh thá»©c)",
        # Khoa há»c/Ká»¹ thuáº­t cá»‘ Ä‘á»‹nh
        r"(cÃ´ng thá»©c|Ä‘á»‹nh luáº­t|nguyÃªn lÃ½|nguyÃªn tá»‘|phÃ¢n tá»­)",
        r"(phÃ¡t minh ra|phÃ¡t hiá»‡n ra|Ä‘Æ°á»£c thÃ nh láº­p nÄƒm)",
        # Äá»‹nh nghÄ©a
        r"(lÃ  gÃ¬\??|nghÄ©a lÃ |Ä‘á»‹nh nghÄ©a|thuá»™c vá»)",
        # Sá»± tháº­t lá»‹ch sá»­ cá»‘ Ä‘á»‹nh
        r"(nÄƒm \d{4}|vÃ o nÄƒm \d{4}|tá»« nÄƒm \d{4})",
        # ThÃ´ng tin ká»¹ thuáº­t cá»‘ Ä‘á»‹nh
        r"(Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi|do .+ phÃ¡t triá»ƒn|thuá»™c sá»Ÿ há»¯u cá»§a)",
    ]
    
    for pattern in knowledge_patterns:
        if re.search(pattern, text_lower):
            return "KNOWLEDGE"
    
    # NEWS patterns - Báº®T BUá»˜C pháº£i cÃ³ evidence
    news_patterns = [
        # Thá»i Ä‘iá»ƒm gáº§n
        r"(sÃ¡ng nay|hÃ´m nay|tá»‘i qua|má»›i Ä‘Ã¢y|vá»«a má»›i|gáº§n Ä‘Ã¢y|má»›i nháº¥t)",
        r"(Ä‘ang diá»…n ra|ngay lÃºc nÃ y|hiá»‡n táº¡i)",
        # TuyÃªn bá»‘
        r"(tuyÃªn bá»‘|cÃ´ng bá»‘|phÃ¡t biá»ƒu|cho biáº¿t|thÃ´ng bÃ¡o|kháº³ng Ä‘á»‹nh)",
        r"(theo nguá»“n tin|theo bÃ¡o cÃ¡o)",
        # Sá»± kiá»‡n
        r"(xáº£y ra|diá»…n ra|dá»± kiáº¿n)",
        # Tin tá»©c indicators
        r"(\[nÃ³ng\]|\[breaking\]|\[cáº­p nháº­t\])",
    ]
    
    for pattern in news_patterns:
        if re.search(pattern, text_lower):
            return "NEWS"
    
    # Default: NEWS (cáº§n evidence Ä‘á»ƒ an toÃ n)
    return "NEWS"


def normalize_conclusion(conclusion: str) -> str:
    """
    Normalize conclusion to BINARY classification: TIN THáº¬T or TIN GIáº¢ only.
    
    ðŸŸ¢ NGUYÃŠN Táº®C Má»šI: PRESUMPTION OF TRUTH
    - Máº·c Ä‘á»‹nh lÃ  TIN THáº¬T náº¿u khÃ´ng cÃ³ dáº¥u hiá»‡u TIN GIáº¢ rÃµ rÃ ng
    - Chá»‰ tráº£ vá» TIN GIáº¢ khi cÃ³ keywords chá»‰ Ä‘á»‹nh rÃµ rÃ ng
    """
    if not conclusion:
        return "TIN THáº¬T"  # Äá»”I: Máº·c Ä‘á»‹nh TIN THáº¬T náº¿u khÃ´ng cÃ³ káº¿t luáº­n
    
    conclusion_upper = conclusion.upper().strip()
    
    # ðŸ”´ CHá»ˆ TIN GIáº¢ KHI CÃ“ Dáº¤U HIá»†U RÃ• RÃ€NG
    fake_indicators = [
        # Vietnamese fake indicators
        "TIN GIáº¢", "TIN GIA", "GIáº¢ Máº O", "FAKE", "FALSE",
        "Bá»ŠA Äáº¶T", "BIA DAT", "Lá»ªA Äáº¢O", "LUA DAO", "SCAM",
        "ZOMBIE", "OUTDATED", "Lá»–I THá»œI", "LOI THOI",
        "KHÃ”NG ÄÃšNG", "KHONG DUNG", "SAI Sá»° THáº¬T", "SAI SU THAT",
        "KHÃ”NG CÃ“ CÆ  Sá»ž", "KHONG CO CO SO", "VÃ” CÄ‚N Cá»¨", "VO CAN CU",
        "ALMOST CERTAINLY FALSE", "HIGHLY UNLIKELY",
        "BÃC Bá»Ž", "BAC BO", "KHÃ”NG XÃC NHáº¬N", "KHONG XAC NHAN",
        # Y táº¿ sai
        "Y Táº¾ SAI", "Y TE SAI", "MISLEADING",
        # Sá»‘ liá»‡u phi thá»±c táº¿
        "PHI THá»°C Táº¾", "PHI THUC TE", "UNREALISTIC",
        # GÃ‚Y HIá»‚U Láº¦M - váº«n coi lÃ  TIN GIáº¢
        "GÃ‚Y HIá»‚U Láº¦M", "GAY HIEU LAM",
    ]
    
    # Náº¿u cÃ³ báº¥t ká»³ indicator TIN GIáº¢ nÃ o -> TIN GIáº¢
    for indicator in fake_indicators:
        if indicator in conclusion_upper:
            return "TIN GIáº¢"
    
    # ðŸŸ¢ Táº¤T Cáº¢ CÃC TRÆ¯á»œNG Há»¢P KHÃC -> TIN THáº¬T
    # Bao gá»“m: TIN THáº¬T, CHÆ¯A KIá»‚M CHá»¨NG, TRUE, PROBABLE, LIKELY, etc.
    return "TIN THáº¬T"


# Product version database for outdated information detection
# Format: product_pattern -> (latest_version, release_year)
PRODUCT_VERSIONS = {
    # Apple iPhone (as of Dec 2025)
    r"iphone\s*(\d+)": {"latest": 17, "year": 2025, "name": "iPhone"},
    # Samsung Galaxy S
    r"galaxy\s*s\s*(\d+)": {"latest": 25, "year": 2025, "name": "Galaxy S"},
    # Samsung Galaxy Note
    r"galaxy\s*note\s*(\d+)": {"latest": 20, "year": 2020, "name": "Galaxy Note"},
    # Google Pixel
    r"pixel\s*(\d+)": {"latest": 9, "year": 2024, "name": "Pixel"},
    # PlayStation
    r"playstation\s*(\d+)|ps\s*(\d+)": {"latest": 5, "year": 2020, "name": "PlayStation"},
    # Xbox (Xbox One=1, Series X=2)
    r"xbox\s*series\s*([xs])": {"latest": "x", "year": 2020, "name": "Xbox Series"},
    # Windows
    r"windows\s*(\d+)": {"latest": 11, "year": 2021, "name": "Windows"},
    # macOS versions
    r"macos\s*(\d+)|mac\s*os\s*(\d+)": {"latest": 15, "year": 2024, "name": "macOS"},
    # MacBook chips
    r"macbook.*m(\d+)": {"latest": 4, "year": 2024, "name": "MacBook M-chip"},
}


def _detect_outdated_product(text_input: str) -> dict | None:
    """
    Detect if the input mentions an outdated product version.
    Returns dict with product info if outdated, None otherwise.
    """
    text_lower = text_input.lower()
    
    for pattern, info in PRODUCT_VERSIONS.items():
        match = re.search(pattern, text_lower)
        if match:
            # Get the version number from match groups
            version_str = None
            for group in match.groups():
                if group:
                    version_str = group
                    break
            
            if version_str:
                try:
                    # Handle numeric versions
                    if version_str.isdigit():
                        mentioned_version = int(version_str)
                        latest_version = info["latest"]
                        
                        if isinstance(latest_version, int) and mentioned_version < latest_version:
                            return {
                                "product": info["name"],
                                "mentioned_version": mentioned_version,
                                "latest_version": latest_version,
                                "latest_year": info["year"],
                                "is_outdated": True,
                                "years_behind": latest_version - mentioned_version
                            }
                except (ValueError, TypeError):
                    pass
    
    return None


def _is_common_knowledge(text_input: str) -> bool:
    """
    Detect if the claim is about well-known, easily verifiable facts.
    These are facts that are widely accepted and don't need extensive verification.
    """
    text_lower = text_input.lower()
    
    # Well-known tech facts
    common_knowledge_patterns = [
        # Company ownership/development
        ("chatgpt", "openai"),
        ("gpt-4", "openai"),
        ("gpt-3", "openai"),
        ("google", "alphabet"),
        ("youtube", "google"),
        ("instagram", "meta"),
        ("whatsapp", "meta"),
        ("facebook", "meta"),
        ("iphone", "apple"),
        ("android", "google"),
        ("windows", "microsoft"),
        ("azure", "microsoft"),
        ("aws", "amazon"),
        
        # Historical events that are well-documented
        ("facebook", "meta", "2021"),
        ("messi", "world cup", "2022"),
        ("argentina", "world cup", "2022"),
    ]
    
    for pattern in common_knowledge_patterns:
        if all(keyword in text_lower for keyword in pattern):
            return True
    
    return False


def _is_weather_source(item: Dict[str, Any]) -> bool:
    source = (item.get("source") or item.get("url") or "").lower()
    if not source:
        return False
    return any(keyword in source for keyword in WEATHER_SOURCE_KEYWORDS)


def load_synthesis_prompt(prompt_path="prompts/synthesis_prompt.txt"):
    """Táº£i prompt cho Agent 2 (Synthesizer)"""
    global SYNTHESIS_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            SYNTHESIS_PROMPT = f.read()
        print("INFO: Táº£i Synthesis Prompt thÃ nh cÃ´ng.")
    except Exception as e:
        print(f"Lá»–I: khÃ´ng thá»ƒ táº£i {prompt_path}: {e}")
        raise


def load_critic_prompt(prompt_path="prompts/critic_prompt.txt"):
    """Táº£i prompt cho CRITIC agent (Devil's Advocate)"""
    global CRITIC_PROMPT
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            CRITIC_PROMPT = f.read()
        print("INFO: Táº£i CRITIC Prompt thÃ nh cÃ´ng.")
    except FileNotFoundError:
        # Fallback to default prompt if file not found
        CRITIC_PROMPT = (
            "Báº¡n lÃ  Biá»‡n lÃ½ Ä‘á»‘i láº­p (Devil's Advocate). "
            "HÃ£y chá»‰ ra 3 Ä‘iá»ƒm yáº¿u, mÃ¢u thuáº«n hoáº·c kháº£ nÄƒng Ä‘Ã¢y lÃ  tin cÅ©/satire/tin Ä‘á»“n. "
            "Chá»‰ tráº£ lá»i ngáº¯n gá»n, gay gáº¯t."
        )
        print(f"WARNING: KhÃ´ng tÃ¬m tháº¥y {prompt_path}, dÃ¹ng prompt máº·c Ä‘á»‹nh.")
    except Exception as e:
        print(f"Lá»–I: khÃ´ng thá»ƒ táº£i {prompt_path}: {e}")


def _parse_json_from_text(text: str) -> dict:
    """TrÃ­ch xuáº¥t JSON an toÃ n tá»« text tráº£ vá» cá»§a LLM"""
    if not text:
        print("Lá»–I: Agent 2 (Synthesizer) khÃ´ng tÃ¬m tháº¥y JSON.")
        return {}

    cleaned = text.strip()
    # Remove Markdown code fences if present
    if cleaned.startswith("```"):
        cleaned = re.sub(r"^```[a-zA-Z0-9_-]*\s*", "", cleaned)
        cleaned = cleaned.rstrip("`").strip()

    match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            print(f"Lá»–I: Agent 2 (Synthesizer) tráº£ vá» JSON khÃ´ng há»£p lá»‡. Text: {cleaned[:300]}...")
            return {}
    # Try direct JSON load if regex failed
    try:
        return json.loads(cleaned)
    except Exception:
        print(f"Lá»–I: Agent 2 (Synthesizer) khÃ´ng tÃ¬m tháº¥y JSON. Raw response: {cleaned[:300]}...")
        return {}


def _trim_snippet(s: str, max_len: int = 200) -> str:
    """
    OPTIMIZED: Giáº£m max_len tá»« 500 xuá»‘ng 200 Ä‘á»ƒ tiáº¿t kiá»‡m token.
    Vá»›i 3 evidence items * 200 chars = 600 chars thay vÃ¬ 10 * 500 = 5000 chars.
    Tiáº¿t kiá»‡m ~90% token cho evidence.
    """
    if not s:
        return ""
    s = s.replace("\n", " ").strip()
    return s[:max_len]


def _trim_evidence_bundle(bundle: Dict[str, Any], cap_l2: int = 3, cap_l3: int = 3, cap_l4: int = 2) -> Dict[str, Any]:
    """
    OPTIMIZED: Giáº£m cap tá»« 10/10/5 xuá»‘ng 3/3/2 Ä‘á»ƒ tiáº¿t kiá»‡m token.
    Tá»•ng: 8 evidence items thay vÃ¬ 25 items.
    Má»¥c tiÃªu: Giáº£m latency tá»« ~70s xuá»‘ng ~25s.
    """
    if not bundle:
        return {"layer_1_tools": [], "layer_2_high_trust": [], "layer_3_general": [], "layer_4_social_low": []}
    out = {
        "layer_1_tools": [], # OpenWeather API data
        "layer_2_high_trust": [],
        "layer_3_general": [],
        "layer_4_social_low": []
    }
    
    # Lá»›p 1: OpenWeather API data (quan trá»ng cho tin thá»i tiáº¿t)
    for it in (bundle.get("layer_1_tools") or []):
        out["layer_1_tools"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date"),
            "weather_data": it.get("weather_data")  # Giá»¯ nguyÃªn dá»¯ liá»‡u gá»‘c tá»« OpenWeather
        })
    
    # Lá»›p 2
    for it in (bundle.get("layer_2_high_trust") or [])[:cap_l2]:
        out["layer_2_high_trust"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    # Lá»›p 3
    for it in (bundle.get("layer_3_general") or [])[:cap_l3]:
        out["layer_3_general"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    # Lá»›p 4
    for it in (bundle.get("layer_4_social_low") or [])[:cap_l4]:
        out["layer_4_social_low"].append({
            "source": it.get("source"),
            "url": it.get("url"),
            "snippet": _trim_snippet(it.get("snippet")),
            "rank_score": it.get("rank_score"),
            "date": it.get("date")
        })
    return out


def _as_str(x: Any) -> str:
    try:
        return x if isinstance(x, str) else ("" if x is None else str(x))
    except Exception:
        return ""


def _heuristic_summarize(text_input: str, bundle: Dict[str, Any], current_date: str) -> Dict[str, Any]:
    """
    Logic dá»± phÃ²ng khi LLM tháº¥t báº¡i.
    
    NGUYÃŠN Táº®C: PRESUMPTION OF TRUTH
    - Máº·c Ä‘á»‹nh lÃ  TIN THáº¬T náº¿u khÃ´ng cÃ³ báº±ng chá»©ng BÃC Bá»Ž
    - Chá»‰ TIN GIáº¢ khi: evidence BÃC Bá»Ž trá»±c tiáº¿p hoáº·c sáº£n pháº©m lá»—i thá»i
    """
    l1 = bundle.get("layer_1_tools") or []
    l2 = bundle.get("layer_2_high_trust") or []
    l3 = bundle.get("layer_3_general") or []

    try:
        claim = classify_claim(text_input)
    except Exception:
        claim = {"is_weather": False}

    is_weather_claim = claim.get("is_weather", False)
    text_lower = text_input.lower()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY 0: Sá»± tháº­t hiá»ƒn nhiÃªn (Common Knowledge)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if _is_common_knowledge(text_input):
        debate_log = {
            "red_team_argument": "TÃ´i khÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng bÃ¡c bá» sá»± tháº­t khoa há»c/ká»¹ thuáº­t nÃ y.",
            "blue_team_argument": "ÄÃ¢y lÃ  sá»± tháº­t Ä‘Ã£ Ä‘Æ°á»£c khoa há»c/cá»™ng Ä‘á»“ng cÃ´ng nháº­n rá»™ng rÃ£i.",
            "judge_reasoning": "Blue Team tháº¯ng. ÄÃ¢y lÃ  kiáº¿n thá»©c phá»• thÃ´ng Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c nháº­n."
        }
        return {
            "conclusion": "TIN THáº¬T",
            "confidence_score": 99,
            "reason": "ÄÃ¢y lÃ  sá»± tháº­t khoa há»c/ká»¹ thuáº­t Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng nháº­n rá»™ng rÃ£i.",
            "debate_log": debate_log,
            "key_evidence_snippet": "Kiáº¿n thá»©c phá»• thÃ´ng",
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "",
            "cached": False
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY 2: PhÃ¡t hiá»‡n sáº£n pháº©m Lá»–I THá»œI (Outdated Product)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    outdated_info = _detect_outdated_product(text_input)
    if outdated_info and outdated_info.get("is_outdated"):
        product = outdated_info["product"]
        mentioned = outdated_info["mentioned_version"]
        latest = outdated_info["latest_version"]
        latest_year = outdated_info["latest_year"]
        
        # Build Adversarial Dialectic debate
        debate_log = {
            "red_team_argument": _as_str(
                f"ThÃ´ng tin nÃ y SAI! {product} {mentioned} lÃ  phiÃªn báº£n cÅ©. "
                f"Hiá»‡n táº¡i Ä‘Ã£ cÃ³ {product} {latest} (ra máº¯t nÄƒm {latest_year}). "
                f"Viá»‡c Ä‘Äƒng tin vá» {product} {mentioned} nhÆ° tin má»›i lÃ  SAI Sá»° THáº¬T."
            ),
            "blue_team_argument": _as_str(
                f"ÄÃºng lÃ  {product} {mentioned} Ä‘Ã£ ra máº¯t tháº­t. "
                f"Tuy nhiÃªn, Ä‘Ã¢y lÃ  thÃ´ng tin lá»—i thá»i. TÃ´i thá»«a nháº­n thua cuá»™c."
            ),
            "judge_reasoning": _as_str(
                f"Red Team tháº¯ng. {product} {mentioned} lÃ  phiÃªn báº£n cÅ©. "
                f"Hiá»‡n táº¡i Ä‘Ã£ cÃ³ {product} {latest}. Tin lá»—i thá»i = TIN GIáº¢."
            )
        }
        
        return {
            "conclusion": "TIN GIáº¢",
            "confidence_score": 95,
            "reason": _as_str(
                f"{product} {mentioned} Ä‘Ã£ lá»—i thá»i. "
                f"Hiá»‡n táº¡i Ä‘Ã£ cÃ³ {product} {latest} (nÄƒm {latest_year}). "
                f"Tin vá» sáº£n pháº©m cÅ© = TIN GIáº¢."
            ),
            "debate_log": debate_log,
            "key_evidence_snippet": _as_str(f"{product} {latest} ra máº¯t nÄƒm {latest_year}"),
            "key_evidence_source": "",
            "evidence_link": "",
            "style_analysis": "ThÃ´ng tin lá»—i thá»i Ä‘Æ°á»£c trÃ¬nh bÃ y nhÆ° tin má»›i",
            "cached": False
        }

    # Æ¯u tiÃªn Lá»›p 1 (OpenWeather API) cho tin thá»i tiáº¿t
    if is_weather_claim and l1:
        weather_item = l1[0]
        weather_data = weather_item.get("weather_data", {})
        if weather_data:
            # So sÃ¡nh Ä‘iá»u kiá»‡n thá»i tiáº¿t
            main_condition = weather_data.get("main", "").lower()
            description = weather_data.get("description", "").lower()
            
            # Kiá»ƒm tra mÆ°a
            if "mÆ°a" in text_lower or "rain" in text_lower:
                if "rain" in main_condition or "rain" in description:
                    # Kiá»ƒm tra má»©c Ä‘á»™ mÆ°a
                    if "mÆ°a to" in text_lower or "mÆ°a lá»›n" in text_lower or "heavy rain" in text_lower:
                        if "heavy" in description or "torrential" in description:
                            return {
                                "conclusion": "TIN THáº¬T",
                                "reason": _as_str(f"Heuristic: OpenWeather API xÃ¡c nháº­n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}Â°C) cho {weather_data.get('location')} ngÃ y {weather_data.get('date')}."),
                                "style_analysis": "",
                                "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                                "key_evidence_source": _as_str(weather_item.get("source")),
                                "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                                "cached": False
                            }
                    else:
                        # MÆ°a thÆ°á»ng
                        return {
                            "conclusion": "TIN THáº¬T",
                            "reason": _as_str(f"Heuristic: OpenWeather API xÃ¡c nháº­n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}Â°C) cho {weather_data.get('location')} ngÃ y {weather_data.get('date')}."),
                            "style_analysis": "",
                            "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                            "key_evidence_source": _as_str(weather_item.get("source")),
                            "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                            "cached": False
                        }
            # Kiá»ƒm tra náº¯ng
            elif "náº¯ng" in text_lower or "sunny" in text_lower or "clear" in text_lower:
                if "clear" in main_condition or "sunny" in description:
                    return {
                        "conclusion": "TIN THáº¬T",
                        "reason": _as_str(f"Heuristic: OpenWeather API xÃ¡c nháº­n {weather_item.get('source')} - {description} ({weather_data.get('temperature')}Â°C) cho {weather_data.get('location')} ngÃ y {weather_data.get('date')}."),
                        "style_analysis": "",
                        "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                        "key_evidence_source": _as_str(weather_item.get("source")),
                        "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                        "cached": False
                    }
            # Náº¿u khÃ´ng khá»›p Ä‘iá»u kiá»‡n cá»¥ thá»ƒ, váº«n tráº£ vá» dá»¯ liá»‡u tá»« OpenWeather
            return {
                "conclusion": "TIN THáº¬T",
                "reason": _as_str(f"Heuristic: OpenWeather API cung cáº¥p dá»¯ liá»‡u thá»i tiáº¿t {weather_item.get('source')} - {description} ({weather_data.get('temperature')}Â°C) cho {weather_data.get('location')} ngÃ y {weather_data.get('date')}."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(weather_item.get("snippet")),
                "key_evidence_source": _as_str(weather_item.get("source")),
                "evidence_link": _as_str(weather_item.get("url") or weather_item.get("link")),
                "cached": False
            }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY 2: Kiá»ƒm tra nguá»“n L2 CÃ“ LIÃŠN QUAN Ä‘áº¿n claim
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ quan trá»ng tá»« claim Ä‘á»ƒ kiá»ƒm tra relevance
    person_keywords = []
    org_location_keywords = []
    
    # TÃ¬m tÃªn ngÆ°á»i (viáº¿t hoa, thÆ°á»ng lÃ  tá»« Ä‘áº§u tiÃªn)
    name_pattern = re.compile(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b')
    names = name_pattern.findall(text_input)
    person_keywords.extend([n.lower() for n in names])
    
    # TÃ¬m tÃªn tá»• chá»©c/CLB/Ä‘á»‹a Ä‘iá»ƒm
    org_patterns = [
        (r'clb\s+(\w+\s*\w*)', 'clb'),
        (r'fc\s+(\w+\s*\w*)', 'fc'),
        (r'Ä‘á»™i\s+(\w+\s*\w*)', 'Ä‘á»™i'),
    ]
    for pat, prefix in org_patterns:
        match = re.search(pat, text_lower)
        if match:
            org_location_keywords.append(match.group(1).strip())
    
    # ThÃªm cÃ¡c Ä‘á»‹a danh phá»• biáº¿n
    location_names = ["hÃ  ná»™i", "ha noi", "hanoi", "sÃ i gÃ²n", "saigon", "ho chi minh", 
                      "viá»‡t nam", "vietnam", "barca", "barcelona", "inter miami", "real madrid"]
    for loc in location_names:
        if loc in text_lower:
            org_location_keywords.append(loc)
    
    # Kiá»ƒm tra L2 sources cÃ³ liÃªn quan THá»°C Sá»° khÃ´ng
    # Äá»‘i vá»›i claim vá» ngÆ°á»i + tá»• chá»©c: Cáº¦N KHá»šP Cáº¢ HAI
    relevant_l2 = []
    has_person_org_claim = len(person_keywords) > 0 and len(org_location_keywords) > 0
    
    for item in l2:
        snippet = (item.get("snippet") or "").lower()
        title = (item.get("title") or "").lower()
        combined = snippet + " " + title
        
        if has_person_org_claim:
            # Claim cÃ³ cáº£ ngÆ°á»i + tá»• chá»©c -> cáº§n khá»›p Cáº¢ HAI
            has_person = any(kw in combined for kw in person_keywords if kw and len(kw) > 2)
            has_org = any(kw in combined for kw in org_location_keywords if kw and len(kw) > 2)
            
            if has_person and has_org:
                relevant_l2.append(item)
        else:
            # Claim Ä‘Æ¡n giáº£n -> chá»‰ cáº§n khá»›p 1 keyword
            is_relevant = False
            all_keywords = person_keywords + org_location_keywords
            for kw in all_keywords:
                if kw and len(kw) > 2 and kw in combined:
                    is_relevant = True
                    break
            if is_relevant:
                relevant_l2.append(item)
    
    # Giáº£m yÃªu cáº§u tá»« 2 xuá»‘ng 1: Chá»‰ cáº§n 1 nguá»“n uy tÃ­n LIÃŠN QUAN THá»°C Sá»° Ä‘á»ƒ há»— trá»£ TIN THáº¬T
    if len(relevant_l2) >= 1:
        top = relevant_l2[0]
        return {
            "conclusion": "TIN THáº¬T",
            "debate_log": {
                "red_team_argument": "TÃ´i khÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng bÃ¡c bá».",
                "blue_team_argument": _as_str(f"CÃ³ Ã­t nháº¥t 1 nguá»“n uy tÃ­n xÃ¡c nháº­n: {top.get('source')}."),
                "judge_reasoning": "Blue Team tháº¯ng vá»›i báº±ng chá»©ng tá»« nguá»“n uy tÃ­n."
            },
            "confidence_score": 85,
            "reason": _as_str(f"CÃ³ nguá»“n uy tÃ­n xÃ¡c nháº­n thÃ´ng tin nÃ y ({top.get('source')})."),
            "style_analysis": "",
            "key_evidence_snippet": _as_str(top.get("snippet")),
            "key_evidence_source": _as_str(top.get("source")),
            "evidence_link": _as_str(top.get("url") or top.get("link")),
            "cached": False
        }
    
    # ÄÃƒ XÃ“A: Block Ä‘Ã¡nh TIN GIáº¢ khi "cÃ³ L2 nhÆ°ng khÃ´ng liÃªn quan"
    # ÄÃ¢y lÃ  logic SAI: KhÃ´ng cÃ³ evidence â‰  Tin giáº£
    # Theo IFCN: Presumption of Truth - chá»‰ TIN GIáº¢ khi cÃ³ Báº°NG CHá»¨NG BÃC Bá»Ž


    if is_weather_claim and l2:
        weather_sources = [item for item in l2 if _is_weather_source(item)]
        if weather_sources:
            top = weather_sources[0]
            return {
                "conclusion": "TIN THáº¬T",
                "reason": _as_str(f"Heuristic (weather): Dá»±a trÃªn nguá»“n dá»± bÃ¡o thá»i tiáº¿t {top.get('source')} ({top.get('date') or 'N/A'})."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(top.get("snippet")),
                "key_evidence_source": _as_str(top.get("source")),
                "evidence_link": _as_str(top.get("url") or top.get("link")),
                "cached": False
            }

    if is_weather_claim:
        layer3 = bundle.get("layer_3_general") or []
        weather_layer3 = [item for item in layer3 if _is_weather_source(item)]
        if weather_layer3:
            top = weather_layer3[0]
            return {
                "conclusion": "TIN THáº¬T",
                "reason": _as_str(f"Heuristic (weather): Dá»±a trÃªn trang dá»± bÃ¡o {top.get('source')} cho Ä‘á»‹a Ä‘iá»ƒm Ä‘Æ°á»£c nÃªu."),
                "style_analysis": "",
                "key_evidence_snippet": _as_str(top.get("snippet")),
                "key_evidence_source": _as_str(top.get("source")),
                "evidence_link": _as_str(top.get("url") or top.get("link")),
                "cached": False
            }

    # PhÃ¡t hiá»‡n thÃ´ng tin gÃ¢y hiá»ƒu láº§m do Ä‘Ã£ cÅ© (Ä‘áº·c biá»‡t vá»›i sáº£n pháº©m/phiÃªn báº£n)
    if not is_weather_claim:
        evidence_items = l2 + l3
        old_items = [item for item in evidence_items if item.get("is_old")]
        fresh_items = [item for item in evidence_items if item.get("is_old") is False]

        marketing_keywords = [
            "giáº£m giÃ¡", "khuyáº¿n mÃ£i", "sale", "ra máº¯t", "má»Ÿ bÃ¡n", "Ä‘áº·t trÆ°á»›c",
            "phiÃªn báº£n", "model", "tháº¿ há»‡", "Ä‘á»i", "nÃ¢ng cáº¥p", "lÃªn ká»‡", "Æ°u Ä‘Ã£i",
            "launch", "promotion"
        ]
        product_pattern = re.compile(r"(iphone|ipad|macbook|galaxy|pixel|surface|playstation|xbox|sony|samsung|apple|oppo|xiaomi|huawei|vinfast)\s?[0-9a-z]{1,4}", re.IGNORECASE)
        mentions_product_cycle = any(kw in text_lower for kw in marketing_keywords) or bool(product_pattern.search(text_input))

        if old_items and (fresh_items or mentions_product_cycle):
            reference_old = old_items[0]
            old_source = reference_old.get("source") or reference_old.get("url") or "nguá»“n cÅ©"
            old_date = reference_old.get("date") or "trÆ°á»›c Ä‘Ã¢y"
            latest_snippet = _as_str(reference_old.get("snippet"))

            if fresh_items:
                latest_item = fresh_items[0]
                latest_source = latest_item.get("source") or latest_item.get("url") or "nguá»“n má»›i"
                latest_date = latest_item.get("date") or "gáº§n Ä‘Ã¢y"
                reason = _as_str(
                    f"ThÃ´ng tin vá» '{text_input}' dá»±a trÃªn nguá»“n {old_source} ({old_date}) Ä‘Ã£ cÅ©, "
                    f"trong khi cÃ¡c nguá»“n má»›i nhÆ° {latest_source} ({latest_date}) cho tháº¥y bá»‘i cáº£nh Ä‘Ã£ thay Ä‘á»•i. "
                    "Viá»‡c trÃ¬nh bÃ y nhÆ° tin nÃ³ng dá»… gÃ¢y hiá»ƒu láº§m."
                )
            else:
                reason = _as_str(
                    f"ThÃ´ng tin vá» '{text_input}' chá»‰ Ä‘Æ°á»£c há»— trá»£ bá»Ÿi nguá»“n cÅ© {old_source} ({old_date}). "
                    "Sáº£n pháº©m/sá»± kiá»‡n nÃ y Ä‘Ã£ xuáº¥t hiá»‡n tá»« lÃ¢u nÃªn viá»‡c trÃ¬nh bÃ y nhÆ° tin tá»©c má»›i lÃ  gÃ¢y hiá»ƒu láº§m."
                )

            return {
                "conclusion": "TIN GIáº¢",
                "reason": reason,
                "style_analysis": "Tin lá»—i thá»i",
                "key_evidence_snippet": latest_snippet,
                "key_evidence_source": _as_str(old_source),
                "evidence_link": _as_str(reference_old.get("url") or reference_old.get("link")),
                "cached": False
            }

        if mentions_product_cycle and fresh_items and not old_items:
            latest_item = fresh_items[0]
            latest_source = latest_item.get("source") or latest_item.get("url") or "nguá»“n má»›i"
            latest_date = latest_item.get("date") or "gáº§n Ä‘Ã¢y"
            reason = _as_str(
                f"KhÃ´ng tÃ¬m tháº¥y nguá»“n gáº§n Ä‘Ã¢y xÃ¡c nháº­n '{text_input}', trong khi cÃ¡c sáº£n pháº©m má»›i hÆ¡n Ä‘Ã£ xuáº¥t hiá»‡n "
                f"(vÃ­ dá»¥ {latest_source}, {latest_date}). ÄÃ¢y lÃ  thÃ´ng tin cÅ© Ä‘Æ°á»£c láº·p láº¡i khiáº¿n ngÆ°á»i Ä‘á»c hiá»ƒu láº§m bá»‘i cáº£nh hiá»‡n táº¡i."
            )
            return {
                "conclusion": "TIN GIáº¢",
                "reason": reason,
                "style_analysis": "Tin lá»—i thá»i",
                "key_evidence_snippet": _as_str(latest_item.get("snippet")),
                "key_evidence_source": _as_str(latest_source),
                "evidence_link": _as_str(latest_item.get("url") or latest_item.get("link")),
                "cached": False
            }

        claim_implies_present = any(
            kw in text_lower
            for kw in [
                "hiá»‡n nay", "bÃ¢y giá»", "Ä‘ang", "sáº¯p", "vá»«a", "today", "now", "currently",
                "má»›i Ä‘Ã¢y", "ngay lÃºc nÃ y", "trong thá»i gian tá»›i"
            ]
        )
        if claim_implies_present and old_items and not fresh_items:
            old_item = old_items[0]
            older_source = old_item.get("source") or old_item.get("url") or "nguá»“n cÅ©"
            older_date = old_item.get("date") or "trÆ°á»›c Ä‘Ã¢y"
            reason = _as_str(
                f"'{text_input}' Ã¡m chá»‰ thÃ´ng tin Ä‘ang diá»…n ra nhÆ°ng chá»‰ cÃ³ nguá»“n {older_source} ({older_date}) tá»« trÆ°á»›c kia. "
                "Viá»‡c dÃ¹ng láº¡i tin cÅ© khiáº¿n ngÆ°á»i Ä‘á»c hiá»ƒu sai vá» tÃ¬nh tráº¡ng hiá»‡n táº¡i."
            )
            return {
                "conclusion": "TIN GIáº¢",
                "reason": reason,
                "style_analysis": "Tin lá»—i thá»i",
                "key_evidence_snippet": _as_str(old_item.get("snippet")),
                "key_evidence_source": _as_str(older_source),
                "evidence_link": _as_str(old_item.get("url") or old_item.get("link")),
                "cached": False
            }

        misleading_tokens = [
            "Ä‘Ã£ káº¿t thÃºc", "Ä‘Ã£ dá»«ng", "ngá»«ng Ã¡p dá»¥ng", "khÃ´ng cÃ²n Ã¡p dá»¥ng",
            "Ä‘Ã£ há»§y", "Ä‘Ã£ hoÃ£n", "Ä‘Ã£ Ä‘Ã³ng", "Ä‘Ã£ ngÆ°ng", "no longer", "ended", "discontinued"
        ]
        for item in evidence_items:
            snippet_lower = (item.get("snippet") or "").lower()
            if any(token in snippet_lower for token in misleading_tokens):
                source = item.get("source") or item.get("url") or "nguá»“n cáº­p nháº­t"
                reason = _as_str(
                    f"'{text_input}' bá» qua cáº­p nháº­t tá»« {source} cho biáº¿t sá»± kiá»‡n/chÆ°Æ¡ng trÃ¬nh Ä‘Ã£ káº¿t thÃºc hoáº·c thay Ä‘á»•i "
                    "nÃªn thÃ´ng tin dá»… gÃ¢y hiá»ƒu láº§m."
                )
                return {
                    "conclusion": "TIN GIáº¢",
                    "reason": reason,
                    "style_analysis": "Tin Ä‘Ã£ khÃ´ng cÃ²n Ä‘Ãºng",
                    "key_evidence_snippet": _as_str(item.get("snippet")),
                    "key_evidence_source": _as_str(source),
                    "evidence_link": _as_str(item.get("url") or item.get("link")),
                    "cached": False
                }

    # FIX: Máº·c Ä‘á»‹nh TIN THáº¬T khi khÃ´ng cÃ³ báº±ng chá»©ng BÃC Bá»Ž (innocent until proven guilty)
    # TrÆ°á»›c Ä‘Ã¢y máº·c Ä‘á»‹nh TIN GIáº¢ gÃ¢y false positive cao
    return {
        "conclusion": "TIN THáº¬T",
        "confidence_score": 60,
        "reason": _as_str("KhÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng BÃC Bá»Ž thÃ´ng tin nÃ y. Dá»±a trÃªn nguyÃªn táº¯c 'innocent until proven guilty'."),
        "debate_log": {
            "red_team_argument": "KhÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng pháº£n bÃ¡c rÃµ rÃ ng.",
            "blue_team_argument": "KhÃ´ng cÃ³ nguá»“n nÃ o bÃ¡c bá» thÃ´ng tin nÃ y.",
            "judge_reasoning": "Khi khÃ´ng cÃ³ báº±ng chá»©ng bÃ¡c bá», tin Ä‘Æ°á»£c coi lÃ  cÃ³ thá»ƒ Ä‘Ãºng."
        },
        "style_analysis": "",
        "key_evidence_snippet": "",
        "key_evidence_source": "",
        "evidence_link": "",
        "cached": False
    }


def _normalize_agent2_model(model_key: str | None) -> str:
    """Normalize Agent 2 model identifier."""
    if not model_key:
        return "models/gemini-2.5-pro"
    mapping = {
        "gemini_flash": "models/gemini-2.5-flash",
        "gemini flash": "models/gemini-2.5-flash",
        "gemini-2.5-flash": "models/gemini-2.5-flash",
        "models/gemini_flash": "models/gemini-2.5-flash",
        "gemini_pro": "models/gemini-2.5-pro",
        "gemini pro": "models/gemini-2.5-pro",
        "models/gemini-2.5-pro": "models/gemini-2.5-pro",
        "openai/gpt-oss-120b": "openai/gpt-oss-120b",
        "meta-llama/llama-3.3-70b-instruct": "meta-llama/llama-3.3-70b-instruct",
        "qwen/qwen-2.5-72b-instruct": "qwen/qwen-2.5-72b-instruct",
        "gemma-3-1b": "models/gemma-3-1b-it",
        "gemma-3-1b-it": "models/gemma-3-1b-it",
        "gemma-3-2b": "models/gemma-3-4b-it",  # 2B not available, fallback to 4B
        "gemma-3-4b": "models/gemma-3-4b-it",
        "gemma-3-4b-it": "models/gemma-3-4b-it",
        "gemma-3-12b": "models/gemma-3-12b-it",
        "gemma-3-12b-it": "models/gemma-3-12b-it",
        "gemma-3-27b": "models/gemma-3-27b-it",
        "gemma-3-27b-it": "models/gemma-3-27b-it",
        "google/gemma-3-1b": "models/gemma-3-1b-it",
        "google/gemma-3-2b": "models/gemma-3-4b-it",
        "google/gemma-3-4b": "models/gemma-3-4b-it",
        "google/gemma-3-12b": "models/gemma-3-12b-it",
        "google/gemma-3-27b": "models/gemma-3-27b-it",
        "models/gemma-3-1b": "models/gemma-3-1b-it",
        "models/gemma-3-2b": "models/gemma-3-4b-it",
        "models/gemma-3-4b": "models/gemma-3-4b-it",
        "models/gemma-3-12b": "models/gemma-3-12b-it",
        "models/gemma-3-27b": "models/gemma-3-27b-it",
        "models/gemma-3-1b-it": "models/gemma-3-1b-it",
        "models/gemma-3-4b-it": "models/gemma-3-4b-it",
        "models/gemma-3-12b-it": "models/gemma-3-12b-it",
        "models/gemma-3-27b-it": "models/gemma-3-27b-it",
        "models/gemma-3n-e2b-it": "models/gemma-3n-e2b-it",
        "models/gemma-3n-e4b-it": "models/gemma-3n-e4b-it",
    }
    return mapping.get(model_key, model_key)


def _detect_agent2_provider(model_name: str) -> str:
    """Detect provider for Agent 2 model."""
    if not model_name:
        return "gemini"
    lowered = model_name.lower()
    if "gemini" in lowered or "gemma" in lowered or model_name.startswith("models/"):
        return "gemini"
    # All Agent 2 models now use Gemini API
    return "gemini"

async def execute_final_analysis(
    text_input: str,
    evidence_bundle: dict,
    current_date: str,
    model_key: str | None = None,
    flash_mode: bool = False,
    site_query_string: str = "",  # Added for re-search
) -> dict:
    """
    Pipeline OPTIMIZED: SYNTH â†’ CRITIC â†’ JUDGE
    
    SYNTH Logic:
    - KNOWLEDGE claims: Agent cÃ³ quyá»n tá»± quyáº¿t dá»±a trÃªn kiáº¿n thá»©c
    - NEWS claims: Báº¯t buá»™c pháº£i cÃ³ evidence
    
    Optimizations applied:
    - Reduced evidence bundle size (3/3/2 items)
    - Reduced snippet length (200 chars)
    - Reduced timeouts (30s/40s)
    - Simplified prompts
    """
    if not SYNTHESIS_PROMPT:
        raise ValueError("Synthesis prompt (prompt 2) chÆ°a Ä‘Æ°á»£c táº£i.")
    if not CRITIC_PROMPT:
        print("WARNING: Critic prompt chÆ°a Ä‘Æ°á»£c táº£i, dÃ¹ng máº·c Ä‘á»‹nh.")

    # =========================================================================
    # SYNTH: PhÃ¢n loáº¡i claim Ä‘á»ƒ quyáº¿t Ä‘á»‹nh quyá»n tá»± quyáº¿t
    # =========================================================================
    claim_type = _classify_claim_type(text_input)
    print(f"\n[SYNTH] Claim type: {claim_type}")
    
    if claim_type == "KNOWLEDGE":
        synth_instruction = (
            "\n\n[SYNTH INSTRUCTION - KNOWLEDGE CLAIM]\n"
            "ÄÃ¢y lÃ  KNOWLEDGE CLAIM (kiáº¿n thá»©c cá»‘ Ä‘á»‹nh: Ä‘á»‹a lÃ½, khoa há»c, Ä‘á»‹nh nghÄ©a).\n"
            "â†’ Báº¡n cÃ³ quyá»n Tá»° QUYáº¾T dá»±a trÃªn kiáº¿n thá»©c ná»™i táº¡i.\n"
            "â†’ KHÃ”NG báº¯t buá»™c pháº£i cÃ³ evidence Ä‘á»ƒ káº¿t luáº­n.\n"
            "â†’ Náº¿u thÃ´ng tin Ä‘Ãºng vá»›i kiáº¿n thá»©c cá»§a báº¡n â†’ TIN THáº¬T.\n"
            "â†’ Náº¿u thÃ´ng tin sai vá»›i kiáº¿n thá»©c cá»§a báº¡n â†’ TIN GIáº¢.\n"
        )
        print(f"[SYNTH] Agent cÃ³ quyá»n tá»± quyáº¿t dá»±a trÃªn kiáº¿n thá»©c")
    else:
        synth_instruction = (
            "\n\n[SYNTH INSTRUCTION - NEWS CLAIM]\n"
            "ÄÃ¢y lÃ  NEWS CLAIM (tin tá»©c, sá»± kiá»‡n, tuyÃªn bá»‘).\n"
            "â†’ Báº®T BUá»˜C pháº£i cÃ³ evidence Ä‘á»ƒ káº¿t luáº­n cháº¯c cháº¯n.\n"
            "â†’ Náº¿u KHÃ”NG cÃ³ evidence liÃªn quan â†’ Ã¡p dá»¥ng PRESUMPTION OF TRUTH (TIN THáº¬T vá»›i confidence tháº¥p).\n"
            "â†’ Náº¿u cÃ³ evidence XÃC NHáº¬N â†’ TIN THáº¬T vá»›i confidence cao.\n"
            "â†’ Náº¿u cÃ³ evidence BÃC Bá»Ž â†’ TIN GIáº¢.\n"
        )
        print(f"[SYNTH] Báº¯t buá»™c pháº£i cÃ³ evidence cho NEWS claim")

    # Trim evidence before sending to models
    trimmed_bundle = _trim_evidence_bundle(evidence_bundle)
    evidence_bundle_json = json.dumps(trimmed_bundle, indent=2, ensure_ascii=False)

    # =========================================================================
    # PHASE 1: CRITIC AGENT (BIá»†N LÃ Äá»I Láº¬P)
    # =========================================================================
    critic_report = "KhÃ´ng cÃ³ pháº£n biá»‡n."
    try:
        print(f"\n[CRITIC] Báº¯t Ä‘áº§u pháº£n biá»‡n (Model: {model_key})...")
        critic_prompt_filled = CRITIC_PROMPT.replace("{text_input}", text_input)
        critic_prompt_filled = critic_prompt_filled.replace("{evidence_bundle_json}", evidence_bundle_json)
        critic_prompt_filled = critic_prompt_filled.replace("{current_date}", current_date)
        
        critic_report = await call_agent_with_capability_fallback(
            role="CRITIC",
            prompt=critic_prompt_filled,
            temperature=0.5,
            timeout=20.0  # 20s - balanced for Gemma 27B
        )
        print(f"[CRITIC] Report: {critic_report[:150]}...")
        
    except Exception as e:
        print(f"[CRITIC] Gáº·p lá»—i: {e}")
        critic_report = "Lá»—i khi cháº¡y Critic Agent."

    # =========================================================================
    # PHASE 2: JUDGE AGENT (THáº¨M PHÃN) - Round 1
    # =========================================================================
    judge_result = {}
    try:
        print(f"\n[JUDGE] Báº¯t Ä‘áº§u phÃ¡n quyáº¿t Round 1...")
        judge_prompt_filled = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
        judge_prompt_filled = judge_prompt_filled.replace("{evidence_bundle_json}", evidence_bundle_json)
        judge_prompt_filled = judge_prompt_filled.replace("{current_date}", current_date)
        
        # Add SYNTH instruction and CRITIC report
        judge_prompt_filled += synth_instruction
        judge_prompt_filled += f"\n\n[Ã KIáº¾N BIá»†N LÃ (CRITIC)]:\n{critic_report}"
        
        judge_text = await call_agent_with_capability_fallback(
            role="JUDGE",
            prompt=judge_prompt_filled,
            temperature=0.1,  # Strict logic
            timeout=25.0  # 25s - enough for complex reasoning
        )
        
        judge_result = _parse_json_from_text(judge_text)

        # ---------------------------------------------------------------------
        # ADAPTER: Convert New "Cognitive Architecture" JSON to Flat Schema
        # ---------------------------------------------------------------------
        verdict_meta = judge_result.get("verdict_metadata")
        if verdict_meta:
            # CONCLUSION
            judge_result["conclusion"] = verdict_meta.get("conclusion")
            judge_result["confidence_score"] = verdict_meta.get("probability_score")
            
            # REASON (Combine BLUF + Synthesis)
            exec_summary = judge_result.get("executive_summary") or {}
            dialectical = judge_result.get("dialectical_analysis") or {}
            
            bluf = exec_summary.get("bluf")
            synthesis = dialectical.get("synthesis")
            
            combined_reason = ""
            if bluf:
                combined_reason += f"{bluf}\n\n"
            if synthesis:
                combined_reason += f"ANALYSIS: {synthesis}"
            
            judge_result["reason"] = combined_reason.strip() or "No rationale provided."
            
            # DEBATE LOG
            judge_result["debate_log"] = {
                "red_team_argument": dialectical.get("antithesis", "N/A"),
                "blue_team_argument": dialectical.get("thesis", "N/A"),
                "judge_reasoning": dialectical.get("synthesis", "N/A")
            }
            
            # STYLE / WEP
            judge_result["style_analysis"] = verdict_meta.get("wep_label") or "N/A"
            
            # KEY EVIDENCE
            citations = judge_result.get("key_evidence_citations") or []
            if citations and isinstance(citations, list) and len(citations) > 0:
                first_cit = citations[0]
                judge_result["key_evidence_snippet"] = first_cit.get("quote") or "N/A"
                judge_result["key_evidence_source"] = first_cit.get("source") or "N/A"
                judge_result["evidence_link"] = first_cit.get("url") or ""
                
            print(f"[JUDGE] Round 1 (Cognitive Schema): {judge_result.get('conclusion')} ({judge_result.get('confidence_score')}%)")
        else:
            # FIX: Handle FLAT SCHEMA (fallback models may return simpler JSON)
            # Fallback models cÃ³ thá»ƒ tráº£ vá» nhiá»u format khÃ¡c nhau
            
            # 1. TÃ¬m conclusion tá»« nhiá»u field cÃ³ thá»ƒ
            if not judge_result.get("conclusion"):
                for key in ["final_conclusion", "verdict", "result", "classification", "åˆ¤å®š"]:
                    if judge_result.get(key):
                        judge_result["conclusion"] = judge_result[key]
                        break
            
            # 2. TÃ¬m confidence_score tá»« nhiá»u field cÃ³ thá»ƒ
            if not judge_result.get("confidence_score"):
                for key in ["probability_score", "confidence", "score", "probability", "certainty", "Ä‘á»™_tin_cáº­y"]:
                    val = judge_result.get(key)
                    if val is not None:
                        try:
                            judge_result["confidence_score"] = int(val) if isinstance(val, (int, float)) else int(str(val).replace("%", ""))
                        except:
                            pass
                        break
                        
                # Náº¿u váº«n khÃ´ng cÃ³, thá»­ tÃ¬m trong nested objects
                if not judge_result.get("confidence_score"):
                    for nested_key in ["metadata", "verdict_info", "analysis"]:
                        nested = judge_result.get(nested_key)
                        if isinstance(nested, dict):
                            for key in ["probability_score", "confidence", "score", "confidence_score"]:
                                val = nested.get(key)
                                if val is not None:
                                    try:
                                        judge_result["confidence_score"] = int(val) if isinstance(val, (int, float)) else int(str(val).replace("%", ""))
                                    except:
                                        pass
                                    break
            
            # 3. TÃ¬m reason tá»« nhiá»u field cÃ³ thá»ƒ (má»Ÿ rá»™ng danh sÃ¡ch)
            if not judge_result.get("reason"):
                reason_keys = [
                    "reasoning", "explanation", "rationale", "analysis", 
                    "lÃ½_do", "giáº£i_thÃ­ch", "bluf", "summary", "message",
                    "judgment", "verdict_reason", "conclusion_reason", 
                    "justification", "evidence_analysis", "finding",
                    "key_judgment", "final_analysis", "assessment"
                ]
                for key in reason_keys:
                    if judge_result.get(key):
                        judge_result["reason"] = str(judge_result[key])
                        print(f"[JUDGE] Found reason in field '{key}'")
                        break
                        
                # Náº¿u váº«n khÃ´ng cÃ³, thá»­ tÃ¬m trong nested objects
                if not judge_result.get("reason"):
                    nested_searches = [
                        ("executive_summary", ["bluf", "summary", "key_judgment", "message"]),
                        ("analysis", ["reasoning", "explanation", "summary", "text"]),
                        ("verdict_info", ["reason", "explanation", "analysis"]),
                        ("verdict_metadata", ["reason", "explanation", "temporal_reason"]),
                        ("dialectical_analysis", ["synthesis", "thesis", "antithesis"]),
                    ]
                    for nested_key, sub_keys in nested_searches:
                        nested = judge_result.get(nested_key)
                        if isinstance(nested, dict):
                            for key in sub_keys:
                                if nested.get(key):
                                    judge_result["reason"] = str(nested[key])
                                    print(f"[JUDGE] Found reason in '{nested_key}.{key}'")
                                    break
                            if judge_result.get("reason"):
                                break
                
                # FIX: Thá»­ láº¥y tá»« temporal_analysis TRÆ¯á»šC (fallback model thÆ°á»ng tráº£ vá» field nÃ y)
                if not judge_result.get("reason"):
                    temporal = judge_result.get("temporal_analysis")
                    if isinstance(temporal, dict):
                        # Æ¯u tiÃªn currency_reason vÃ¬ Ä‘Ã¢y lÃ  field Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong schema
                        for key in ["currency_reason", "reason", "explanation", "analysis", "currency_status"]:
                            val = temporal.get(key)
                            if val and isinstance(val, str) and len(val) > 5:
                                # Combine vá»›i currency_status náº¿u cÃ³ Ä‘á»ƒ táº¡o reason Ä‘áº§y Ä‘á»§ hÆ¡n
                                currency_status = temporal.get("currency_status", "")
                                if key == "currency_reason":
                                    judge_result["reason"] = f"[{currency_status}] {val}" if currency_status else val
                                else:
                                    judge_result["reason"] = str(val)
                                print(f"[JUDGE] Found reason in 'temporal_analysis.{key}'")
                                break
                    elif isinstance(temporal, str) and len(temporal) > 20:
                        judge_result["reason"] = temporal
                        print(f"[JUDGE] Using 'temporal_analysis' string as reason")
                
                # Náº¿u váº«n khÃ´ng cÃ³, dÃ¹ng wep_label + conclusion lÃ m reason
                if not judge_result.get("reason"):
                    wep = judge_result.get("wep_label", "")
                    conclusion = judge_result.get("conclusion", "")
                    if wep:
                        judge_result["reason"] = f"ÄÃ¡nh giÃ¡: {wep}. Káº¿t luáº­n: {conclusion}."
                        print(f"[JUDGE] Using wep_label as fallback reason")
                
                # Thá»­ láº¥y báº¥t ká»³ string field nÃ o cÃ³ Ä‘á»™ dÃ i > 50 lÃ m reason
                if not judge_result.get("reason"):
                    for key, val in judge_result.items():
                        if isinstance(val, str) and len(val) > 50 and key not in ["conclusion", "text_input"]:
                            judge_result["reason"] = val
                            print(f"[JUDGE] Using field '{key}' as reason")
                            break
                
                # CHá»ˆ log DEBUG náº¿u sau táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p váº«n khÃ´ng tÃ¬m Ä‘Æ°á»£c reason
                if not judge_result.get("reason"):
                    print(f"[JUDGE] DEBUG: Could not find reason after all attempts. Available keys: {list(judge_result.keys())}")
                    # Fallback cuá»‘i cÃ¹ng: táº¡o reason tá»« conclusion
                    judge_result["reason"] = f"Káº¿t luáº­n: {judge_result.get('conclusion', 'N/A')}. Xem báº±ng chá»©ng chi tiáº¿t bÃªn dÆ°á»›i."
            
            # 4. Log káº¿t quáº£
            if judge_result.get("conclusion"):
                conf = judge_result.get("confidence_score")
                conf_str = f"{conf}%" if conf is not None else "N/A"
                print(f"[JUDGE] Round 1 (Flat Schema): {judge_result.get('conclusion')} ({conf_str})")
            else:
                # JSON parse Ä‘Æ°á»£c nhÆ°ng khÃ´ng cÃ³ conclusion há»£p lá»‡
                print(f"[JUDGE] WARNING: JSON parsed but no valid conclusion found. Keys: {list(judge_result.keys())}")
                # FIX: LUÃ”N dÃ¹ng heuristic fallback khi khÃ´ng cÃ³ conclusion
                print(f"[JUDGE] Fallback to heuristic analyzer...")
                return _heuristic_summarize(text_input, evidence_bundle, current_date)

        # ---------------------------------------------------------------------
    except Exception as e:
        print(f"[JUDGE] Gáº·p lá»—i Round 1: {e}")
        return _heuristic_summarize(text_input, evidence_bundle, current_date)

    # =========================================================================
    # PHASE 2.5: COUNTER-SEARCH (TÃ¬m dáº«n chá»©ng Báº¢O Vá»† claim trÆ°á»›c khi káº¿t luáº­n TIN GIáº¢)
    # =========================================================================
    # Náº¿u JUDGE Round 1 káº¿t luáº­n TIN GIáº¢ â†’ Search thÃªm Ä‘á»ƒ tÃ¬m dáº«n chá»©ng á»§ng há»™ claim
    # ÄÃ¢y lÃ  cÆ¡ há»™i "pháº£n biá»‡n láº¡i CRITIC" báº±ng báº±ng chá»©ng má»›i
    
    conclusion_r1 = normalize_conclusion(judge_result.get("conclusion", ""))
    
    if conclusion_r1 == "TIN GIáº¢":
        print(f"\n[COUNTER-SEARCH] JUDGE Round 1 káº¿t luáº­n TIN GIáº¢ â†’ TÃ¬m dáº«n chá»©ng Báº¢O Vá»† claim...")
        
        try:
            from app.search import call_google_search
            
            # Search Ä‘á»ƒ tÃ¬m dáº«n chá»©ng XÃC NHáº¬N claim (pháº£n biá»‡n CRITIC)
            counter_queries = [
                f"{text_input} xÃ¡c nháº­n",
                f"{text_input} chá»©ng minh Ä‘Ãºng",
                f"{text_input} official",
            ]
            
            counter_evidence = []
            for query in counter_queries[:2]:  # Chá»‰ 2 queries Ä‘á»ƒ nhanh
                results = call_google_search(query, "")
                counter_evidence.extend(results[:5])
                if len(counter_evidence) >= 5:
                    break
            
            if counter_evidence:
                print(f"[COUNTER-SEARCH] TÃ¬m tháº¥y {len(counter_evidence)} dáº«n chá»©ng cÃ³ thá»ƒ á»§ng há»™ claim")
                
                # Táº¡o evidence bundle má»›i vá»›i counter-evidence
                counter_bundle = {
                    "layer_1_tools": evidence_bundle.get("layer_1_tools", []),
                    "layer_2_high_trust": counter_evidence[:5],
                    "layer_3_general": evidence_bundle.get("layer_3_general", []),
                    "layer_4_social_low": []
                }
                counter_evidence_json = json.dumps(_trim_evidence_bundle(counter_bundle), indent=2, ensure_ascii=False)
                
                # JUDGE Round 1.5: Xem xÃ©t láº¡i vá»›i dáº«n chá»©ng má»›i
                print(f"[JUDGE] Round 1.5: Xem xÃ©t láº¡i vá»›i dáº«n chá»©ng má»›i...")
                
                counter_prompt = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
                counter_prompt = counter_prompt.replace("{evidence_bundle_json}", counter_evidence_json)
                counter_prompt = counter_prompt.replace("{current_date}", current_date)
                counter_prompt += f"""

[COUNTER-SEARCH EVIDENCE]
ÄÃ£ tÃ¬m thÃªm dáº«n chá»©ng CÃ“ THá»‚ á»§ng há»™ claim. HÃ£y xem xÃ©t láº¡i káº¿t luáº­n.

[NGUYÃŠN Táº®C]
- Náº¿u dáº«n chá»©ng má»›i XÃC NHáº¬N claim â†’ TIN THáº¬T
- Náº¿u dáº«n chá»©ng má»›i KHÃ”NG liÃªn quan â†’ Giá»¯ nguyÃªn TIN GIáº¢
- CHá»ˆ káº¿t luáº­n TIN GIáº¢ náº¿u cÃ³ báº±ng chá»©ng BÃC Bá»Ž rÃµ rÃ ng

[CRITIC FEEDBACK TRÆ¯á»šC ÄÃ“]
{critic_report}
"""
                
                counter_text = await call_agent_with_capability_fallback(
                    role="JUDGE",
                    prompt=counter_prompt,
                    temperature=0.1,
                    timeout=25.0  # Same as JUDGE
                )
                
                counter_result = _parse_json_from_text(counter_text)
                
                # Parse káº¿t quáº£
                if counter_result.get("verdict_metadata"):
                    counter_conclusion = counter_result["verdict_metadata"].get("conclusion")
                    counter_confidence = counter_result["verdict_metadata"].get("probability_score")
                else:
                    counter_conclusion = counter_result.get("conclusion")
                    counter_confidence = counter_result.get("confidence_score")
                
                counter_conclusion = normalize_conclusion(counter_conclusion or "")
                
                print(f"[JUDGE] Round 1.5: {counter_conclusion} ({counter_confidence}%)")
                
                # Náº¿u Counter-Search Ä‘á»•i Ã½ â†’ Cáº­p nháº­t judge_result
                if counter_conclusion == "TIN THáº¬T":
                    print(f"[COUNTER-SEARCH] âœ… Counter-evidence Ä‘Ã£ thay Ä‘á»•i káº¿t luáº­n: TIN GIáº¢ â†’ TIN THáº¬T")
                    judge_result["conclusion"] = "TIN THáº¬T"
                    judge_result["confidence_score"] = counter_confidence or 75
                    judge_result["reason"] = (judge_result.get("reason", "") + 
                        f"\n\n[COUNTER-SEARCH] Sau khi tÃ¬m thÃªm dáº«n chá»©ng, claim Ä‘Æ°á»£c xÃ¡c nháº­n lÃ  TIN THáº¬T.")
                else:
                    print(f"[COUNTER-SEARCH] âŒ Counter-evidence khÃ´ng thay Ä‘á»•i káº¿t luáº­n, giá»¯ TIN GIáº¢")
            else:
                print(f"[COUNTER-SEARCH] KhÃ´ng tÃ¬m tháº¥y dáº«n chá»©ng má»›i")
                
        except Exception as e:
            print(f"[COUNTER-SEARCH] Lá»—i: {e}")

    # =========================================================================
    # PHASE 3: SELF-CORRECTION (RE-SEARCH LOOP)
    # =========================================================================
    
    # FIX: Parse confidence an toÃ n - default 50 (neutral) thay vÃ¬ 0 Ä‘á»ƒ trÃ¡nh trigger re-search sai
    confidence = 50  # Neutral default
    raw_confidence = judge_result.get("confidence_score")
    if raw_confidence is not None:
        try:
            confidence = int(raw_confidence)
        except (ValueError, TypeError):
            confidence = 50  # Keep neutral if parse fails
            print(f"[SELF-CORRECTION] Warning: Could not parse confidence '{raw_confidence}', using default 50")
    else:
        print(f"[SELF-CORRECTION] Warning: No confidence_score in judge result, using default 50")
    
    # FIX: needs_more_evidence pháº£i lÃ  True EXPLICIT, khÃ´ng pháº£i chá»‰ vÃ¬ confidence tháº¥p do parse lá»—i    
    needs_more = judge_result.get("needs_more_evidence", False)
    if not isinstance(needs_more, bool):
        needs_more = str(needs_more).lower() == "true"
    
    # KÃ­ch hoáº¡t Re-search náº¿u:
    # 1. Judge YÃŠU Cáº¦U EXPLICIT (needs_more_evidence = True) - Æ°u tiÃªn cao nháº¥t
    # 2. Hoáº·c Confidence < 40 (ráº¥t tháº¥p, khÃ´ng pháº£i do parse fail)
    # 3. VÃ€ chÆ°a pháº£i lÃ  tin thá»i tiáº¿t (thá»i tiáº¿t thÆ°á»ng check 1 láº§n lÃ  Ä‘á»§)
    # 4. VÃ€ judge_result khÃ´ng rá»—ng (cÃ³ káº¿t quáº£ thá»±c sá»±)
    is_weather = "thá»i tiáº¿t" in judge_result.get("claim_type", "").lower()
    has_valid_result = bool(judge_result.get("conclusion"))
    
    # FIX: Chá»‰ trigger re-search khi THá»°C Sá»° cáº§n, khÃ´ng pháº£i do parse error
    should_research = (
        needs_more  # Judge yÃªu cáº§u explicit
        or (confidence < 40 and has_valid_result)  # Confidence tháº¥p tháº­t sá»±
    ) and not is_weather and has_valid_result
    
    if should_research:
        print(f"\n[SELF-CORRECTION] KÃ­ch hoáº¡t Re-Search (Confidence: {confidence}%, Needs More: {needs_more}, Has Result: {has_valid_result})")
        
        new_queries = judge_result.get("additional_search_queries", [])
        if not new_queries:
            # Fallback náº¿u Judge khÃ´ng Ä‘Æ°a query
            new_queries = [f"{text_input} sá»± tháº­t", f"{text_input} fact check"]
            
        print(f"[SELF-CORRECTION] Queries má»›i: {new_queries}")
        
        if new_queries:
            # Thá»±c hiá»‡n search bá»• sung
            re_search_plan = {
                "required_tools": [{
                    "tool_name": "search",
                    "parameters": {"queries": new_queries}
                }]
            }
            
            # Execute search
            new_evidence = await execute_tool_plan(re_search_plan, site_query_string, flash_mode)
            
            # FIX: Safe initialization - Ä‘áº£m báº£o cÃ¡c layer keys tá»“n táº¡i trÆ°á»›c khi merge
            for layer_key in ["layer_2_high_trust", "layer_3_general", "layer_4_social_low"]:
                if layer_key not in evidence_bundle:
                    evidence_bundle[layer_key] = []
                if not isinstance(evidence_bundle[layer_key], list):
                    evidence_bundle[layer_key] = []
            
            # Merge vÃ o bundle cÅ© (now safe)
            evidence_bundle["layer_2_high_trust"].extend(new_evidence.get("layer_2_high_trust", []))
            evidence_bundle["layer_3_general"].extend(new_evidence.get("layer_3_general", []))
            evidence_bundle["layer_4_social_low"].extend(new_evidence.get("layer_4_social_low", []))
            
            # Remove duplicates based on URL
            seen_urls = set()
            for layer in ["layer_2_high_trust", "layer_3_general", "layer_4_social_low"]:
                unique_items = []
                for item in evidence_bundle[layer]:
                    url = item.get("url")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        unique_items.append(item)
                evidence_bundle[layer] = unique_items
                
            print(f"[SELF-CORRECTION] ÄÃ£ merge evidence má»›i. Tá»•ng L2: {len(evidence_bundle['layer_2_high_trust'])}")
            
            # Re-Generate Critic (Nhanh) - Optional, but good for completeness
            # Äá»ƒ tiáº¿t kiá»‡m thá»i gian, cÃ³ thá»ƒ bá» qua Critic R2 hoáº·c cháº¡y nhanh
            # á»ž Ä‘Ã¢y ta update láº¡i Critic Report vá»›i báº±ng chá»©ng má»›i
            evidence_bundle_json_v2 = json.dumps(_trim_evidence_bundle(evidence_bundle), indent=2, ensure_ascii=False)
            
            # Re-Run Judge Round 2
            print(f"[JUDGE] Báº¯t Ä‘áº§u phÃ¡n quyáº¿t Round 2 (Final)...")
            judge_prompt_filled_v2 = SYNTHESIS_PROMPT.replace("{text_input}", text_input)
            judge_prompt_filled_v2 = judge_prompt_filled_v2.replace("{evidence_bundle_json}", evidence_bundle_json_v2)
            judge_prompt_filled_v2 = judge_prompt_filled_v2.replace("{current_date}", current_date)
            judge_prompt_filled_v2 += f"\n\n[Ã KIáº¾N BIá»†N LÃ (CRITIC - ROUND 1)]:\n{critic_report}\n(LÆ°u Ã½: Báº±ng chá»©ng Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t thÃªm sau vÃ²ng 1)"
            
            # FIX: LÆ°u káº¿t quáº£ Round 1 lÃ m backup
            judge_result_r1_backup = judge_result.copy() if judge_result else {}
            
            try:
                judge_text_v2 = await call_agent_with_capability_fallback(
                    role="JUDGE",
                    prompt=judge_prompt_filled_v2,
                    temperature=0.1,
                    timeout=80.0
                )
                judge_result_r2 = _parse_json_from_text(judge_text_v2)
                
                # ---------------------------------------------------------------------
                # ADAPTER ROUND 2: Convert "Cognitive Architecture" JSON to Flat Schema
                # ---------------------------------------------------------------------
                verdict_meta = judge_result_r2.get("verdict_metadata")
                if verdict_meta:
                    # CONCLUSION
                    judge_result_r2["conclusion"] = verdict_meta.get("conclusion")
                    judge_result_r2["confidence_score"] = verdict_meta.get("probability_score")
                    
                    # REASON (Combine BLUF + Synthesis)
                    exec_summary = judge_result_r2.get("executive_summary") or {}
                    dialectical = judge_result_r2.get("dialectical_analysis") or {}
                    
                    bluf = exec_summary.get("bluf")
                    synthesis = dialectical.get("synthesis")
                    
                    combined_reason = ""
                    if bluf:
                        combined_reason += f"{bluf}\n\n"
                    if synthesis:
                        combined_reason += f"ANALYSIS: {synthesis}"
                    
                    judge_result_r2["reason"] = combined_reason.strip() or "No rationale provided."
                    
                    # DEBATE LOG
                    judge_result_r2["debate_log"] = {
                        "red_team_argument": dialectical.get("antithesis", "N/A"),
                        "blue_team_argument": dialectical.get("thesis", "N/A"),
                        "judge_reasoning": dialectical.get("synthesis", "N/A")
                    }
                    
                    # STYLE / WEP
                    judge_result_r2["style_analysis"] = verdict_meta.get("wep_label") or "N/A"
                    
                    # KEY EVIDENCE
                    citations = judge_result_r2.get("key_evidence_citations") or []
                    if citations and isinstance(citations, list) and len(citations) > 0:
                        first_cit = citations[0]
                        judge_result_r2["key_evidence_snippet"] = first_cit.get("quote") or "N/A"
                        judge_result_r2["key_evidence_source"] = first_cit.get("source") or "N/A"
                        judge_result_r2["evidence_link"] = first_cit.get("url") or ""
                        
                    print(f"[JUDGE] Round 2 (Cognitive Schema): {judge_result_r2.get('conclusion')} ({judge_result_r2.get('confidence_score')}%)")
                else:
                    # FIX: Handle FLAT SCHEMA for Round 2 (same logic as Round 1)
                    
                    # 1. TÃ¬m conclusion tá»« nhiá»u field cÃ³ thá»ƒ
                    if not judge_result_r2.get("conclusion"):
                        for key in ["final_conclusion", "verdict", "result", "classification"]:
                            if judge_result_r2.get(key):
                                judge_result_r2["conclusion"] = judge_result_r2[key]
                                break
                    
                    # 2. TÃ¬m confidence_score tá»« nhiá»u field cÃ³ thá»ƒ
                    if not judge_result_r2.get("confidence_score"):
                        for key in ["probability_score", "confidence", "score", "probability", "certainty"]:
                            val = judge_result_r2.get(key)
                            if val is not None:
                                try:
                                    judge_result_r2["confidence_score"] = int(val) if isinstance(val, (int, float)) else int(str(val).replace("%", ""))
                                except:
                                    pass
                                break
                    
                    # 3. TÃ¬m reason tá»« nhiá»u field cÃ³ thá»ƒ
                    if not judge_result_r2.get("reason"):
                        for key in ["reasoning", "explanation", "rationale", "analysis", "summary", "bluf"]:
                            if judge_result_r2.get(key):
                                judge_result_r2["reason"] = str(judge_result_r2[key])
                                break
                    
                    # 4. Log káº¿t quáº£
                    if judge_result_r2.get("conclusion"):
                        conf = judge_result_r2.get("confidence_score")
                        conf_str = f"{conf}%" if conf is not None else "N/A"
                        print(f"[JUDGE] Round 2 (Flat Schema): {judge_result_r2.get('conclusion')} ({conf_str})")
                    else:
                        print(f"[JUDGE] WARNING Round 2: No valid conclusion. Keys: {list(judge_result_r2.keys())}")
                
                # FIX: Chá»‰ sá»­ dá»¥ng Round 2 náº¿u cÃ³ káº¿t quáº£ há»£p lá»‡
                if judge_result_r2.get("conclusion"):
                    judge_result = judge_result_r2
                    judge_result["cached"] = False
                    print(f"[JUDGE] Káº¿t quáº£ Round 2: {judge_result.get('conclusion')} ({judge_result.get('confidence_score')}%)")
                    
                    # FIX: Äáº£m báº£o reason vÃ  evidence_link Ä‘Æ°á»£c copy tá»« R2
                    if not judge_result.get("reason"):
                        judge_result["reason"] = judge_result_r1_backup.get("reason", "Xem báº±ng chá»©ng bÃªn dÆ°á»›i.")
                    if not judge_result.get("evidence_link"):
                        judge_result["evidence_link"] = judge_result_r1_backup.get("evidence_link", "")
                else:
                    # Round 2 khÃ´ng cÃ³ káº¿t quáº£ há»£p lá»‡ - giá»¯ Round 1
                    print(f"[JUDGE] Round 2 failed to produce valid result. Keeping Round 1 result.")
                    judge_result = judge_result_r1_backup
                    
            except Exception as e:
                print(f"[JUDGE] Lá»—i Round 2: {e}. Giá»¯ nguyÃªn káº¿t quáº£ Round 1.")
                judge_result = judge_result_r1_backup  # FIX: Ensure we use backup
        else:
             print("[SELF-CORRECTION] KhÃ´ng cÃ³ query má»›i, bá» qua Round 2.")

    # Post-processing normalization
    if judge_result:
        # Map old schema keys if needed (fallback)
        if "final_conclusion" in judge_result and "conclusion" not in judge_result:
            judge_result["conclusion"] = judge_result["final_conclusion"]
            
        judge_result["conclusion"] = normalize_conclusion(judge_result.get("conclusion"))
        return judge_result

    # Fallback final
    return _heuristic_summarize(text_input, evidence_bundle, current_date)

