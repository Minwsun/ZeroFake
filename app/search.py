import os
import re
import json
import httpx
from datetime import datetime
from urllib.parse import urlparse, urlencode

from duckduckgo_search import DDGS
from dotenv import load_dotenv

load_dotenv()

# SearXNG Configuration
SEARXNG_URL = os.getenv("SEARXNG_URL", "http://localhost:8080")  # Self-hosted default
WARP_PROXY = os.getenv("WARP_PROXY", "socks5://127.0.0.1:40000")
WARP_ENABLED = os.getenv("WARP_ENABLED", "false").lower() == "true"

# Legacy Google API keys (kept for compatibility)
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")

MAX_RESULTS = 25  # L·∫•y ƒë·ªß evidence cho CRITIC v√† JUDGE
SEARXNG_TIMEOUT = 30  # Timeout cho SearXNG requests
DDG_TIMEOUT = 20  # Timeout cho DuckDuckGo fallback

# Keywords indicating international events that need English search
INTERNATIONAL_KEYWORDS = [
    "apple", "google", "microsoft", "amazon", "meta", "facebook", "twitter", "x.com",
    "tesla", "spacex", "nvidia", "openai", "chatgpt", "samsung", "sony", "nintendo",
    "iphone", "ipad", "macbook", "galaxy", "pixel", "vision pro", "quest",
    "reuters", "bbc", "cnn", "nytimes", "ap news", "afp",
    "world cup", "champions league", "premier league", "nba", "nfl", "olympics",
    "us", "usa", "uk", "china", "japan", "korea", "europe", "america",
    "biden", "trump", "putin", "xi jinping", "elon musk", "tim cook", "satya nadella",
    "baltimore", "washington", "new york", "london", "tokyo", "beijing", "paris",
    "francis scott key", "mh370", "boeing", "airbus",
]


def get_site_query(config_path: str = "config.json") -> str:
    """Return empty string so we search the entire web."""
    return ""


def _clean_query(query: str) -> str:
    """Remove noise prefixes and emoji from query."""
    # Remove common Vietnamese news prefixes
    query = re.sub(r'^(TIN N√ìNG|N√ìNG|BREAKING|TIN M·ªöI|S·ªêC|C·∫¢NH B√ÅO|‚ö†Ô∏è|üî¥|üì¢|üö®|‚ùó)[:!]*\s*', '', query, flags=re.IGNORECASE)
    # Remove source citations that aren't helpful for search
    query = re.sub(r'^(Theo Reuters|Theo BBC|Theo AP|Th√¥ng tin t·ª´ AP|BBC ƒë∆∞a tin)[:]*\s*', '', query, flags=re.IGNORECASE)
    # Remove call-to-action phrases
    query = re.sub(r'\s*[-‚Äì]\s*(Xem ngay|Chia s·∫ª ngay|ƒê·ªçc th√™m|Click here).*$', '', query, flags=re.IGNORECASE)
    return query.strip()


def _is_international_event(text: str) -> bool:
    """Check if the claim is about an international event that needs English search."""
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in INTERNATIONAL_KEYWORDS)


def _extract_english_query(text: str) -> str:
    """Extract or create an English-friendly query from Vietnamese text."""
    # Keep proper nouns and numbers, remove Vietnamese particles
    # Common translations for search
    translations = {
        "v√¥ ƒë·ªãch": "champion winner",
        "ra m·∫Øt": "launch release",
        "qua ƒë·ªùi": "died death",
        "m·∫•t t√≠ch": "disappeared missing",
        "s·∫≠p c·∫ßu": "bridge collapse",
        "th√°ng": "",  # Remove, keep the number
        "nƒÉm": "",
        "v·ª´a": "",
        "ƒë√™m qua": "",
        "h√¥m nay": "",
    }
    
    result = text
    for vn, en in translations.items():
        result = re.sub(vn, en, result, flags=re.IGNORECASE)
    
    # Keep alphanumeric, spaces, and common punctuation
    result = re.sub(r'[^\w\s\-\./]', ' ', result)
    result = re.sub(r'\s+', ' ', result).strip()
    
    return result


def _ensure_news_keyword(query: str) -> str:
    query = (query or "").strip()
    lower = query.lower()
    if not any(kw in lower for kw in ["tin t·ª©c", "news", "th√¥ng tin", "b√°o", "article"]):
        return f"{query} tin t·ª©c".strip()
    return query


def _sort_key(item: dict) -> tuple:
    """Simple sort by date (newest first)."""
    date_str = item.get("date") or "1970-01-01"
    try:
        ts = datetime.strptime(date_str[:10], "%Y-%m-%d").timestamp()
    except Exception:
        ts = 0
    return (-ts,)  # Sort by date descending


def _create_http_client() -> httpx.Client:
    """Create HTTP client with optional WARP proxy."""
    if WARP_ENABLED:
        print(f"üîí S·ª≠ d·ª•ng Cloudflare WARP proxy: {WARP_PROXY}")
        return httpx.Client(
            proxy=WARP_PROXY,
            timeout=SEARXNG_TIMEOUT,
            follow_redirects=True,
        )
    else:
        return httpx.Client(
            timeout=SEARXNG_TIMEOUT,
            follow_redirects=True,
        )


def _run_searxng(query: str, time_range: str = "month") -> list:
    """
    G·ªçi SearXNG API ƒë·ªÉ t√¨m ki·∫øm, ch·ªâ s·ª≠ d·ª•ng Google engine.
    
    Args:
        query: T·ª´ kh√≥a t√¨m ki·∫øm
        time_range: Kho·∫£ng th·ªùi gian (day, week, month, year)
    
    Returns:
        List c√°c k·∫øt qu·∫£ t√¨m ki·∫øm, ho·∫∑c None n·∫øu l·ªói (ƒë·ªÉ trigger fallback)
    """
    params = {
        "q": query,
        "format": "json",
        "engines": "google",  # CH·ªà s·ª≠ d·ª•ng Google ƒë·ªÉ ƒë·∫°t ch·∫•t l∆∞·ª£ng cao nh·∫•t
        "language": "vi-VN",
        "safesearch": "0",
        "pageno": "1",
    }
    
    # Map time range
    if time_range == "w":
        params["time_range"] = "week"
    elif time_range == "d":
        params["time_range"] = "day"
    elif time_range == "y":
        params["time_range"] = "year"
    else:
        params["time_range"] = "month"
    
    search_url = f"{SEARXNG_URL.rstrip('/')}/search"
    
    try:
        with _create_http_client() as client:
            response = client.get(search_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            results = data.get("results", [])
            print(f"‚úÖ SearXNG (Google): T√¨m th·∫•y {len(results)} k·∫øt qu·∫£")
            return results
            
    except httpx.TimeoutException:
        print(f"‚è±Ô∏è SearXNG timeout sau {SEARXNG_TIMEOUT}s - s·∫Ω fallback sang DuckDuckGo")
        return None  # Trigger fallback
    except httpx.HTTPStatusError as e:
        print(f"‚ùå SearXNG HTTP error: {e.response.status_code} - s·∫Ω fallback sang DuckDuckGo")
        return None  # Trigger fallback
    except Exception as exc:
        print(f"‚ùå SearXNG l·ªói: {exc} - s·∫Ω fallback sang DuckDuckGo")
        return None  # Trigger fallback


def _run_ddg_fallback(query: str, timelimit: str = "m") -> list:
    """
    DuckDuckGo fallback khi SearXNG kh√¥ng kh·∫£ d·ª•ng.
    
    Args:
        query: T·ª´ kh√≥a t√¨m ki·∫øm
        timelimit: Kho·∫£ng th·ªùi gian (d, w, m, y)
    
    Returns:
        List c√°c k·∫øt qu·∫£ t√¨m ki·∫øm
    """
    print(f"ü¶Ü Fallback: ƒêang g·ªçi DuckDuckGo cho: {query}")
    try:
        with DDGS() as ddgs:
            results = ddgs.text(
                keywords=query,
                region="vi-vn",
                safesearch="off",
                timelimit=timelimit,
                max_results=MAX_RESULTS,
            ) or []
            print(f"‚úÖ DuckDuckGo: T√¨m th·∫•y {len(results)} k·∫øt qu·∫£")
            return results
    except Exception as exc:
        print(f"‚ùå DuckDuckGo l·ªói: {exc}")
        return []


def call_google_search(text_input: str, site_query_string: str) -> list:
    """
    IMPROVED: Use DDGS().news() for proper news search instead of text() with site: query.
    Priority: VN News ‚Üí International News ‚Üí Web fallback
    """
    print(f"ƒêang g·ªçi Search cho: {text_input}")
    
    # Clean the query first
    cleaned_input = _clean_query(text_input)
    en_query = _extract_english_query(cleaned_input)
    query_vi = _ensure_news_keyword(cleaned_input)
    
    # Determine timelimit
    timelimit = None
    if any(kw in query_vi.lower() for kw in ["m·ªõi nh·∫•t", "latest", "h√¥m nay", "today", "v·ª´a"]):
        timelimit = "w"  # This week

    all_items = []
    seen = set()

    def _ingest_ddg(results, source_type="web"):
        """Ingest results from DuckDuckGo."""
        for r in results:
            # DDG .news() returns different keys than .text()
            link = r.get("url") or r.get("href")
            if not link or link in seen:
                continue
            seen.add(link)

            snippet = r.get("body") or r.get("snippet") or ""
            title = r.get("title") or ""
            date_raw = r.get("date") or ""
            source = r.get("source") or ""  # .news() often has source field

            # Skip if snippet too short
            if len(snippet) < 30 and "youtube.com" not in link:
                continue

            # Show source in title if available
            display_title = title
            if source and source not in title:
                display_title = f"[{source}] {title}"

            all_items.append({
                "title": display_title,
                "link": link,
                "snippet": snippet,
                "source": source,
                "pagemap": {},
                "date": date_raw,
            })

    def _run_ddg_news(q: str, tl: str | None, region: str = "vi-vn"):
        """Use DDGS().news() for actual news articles."""
        try:
            with DDGS() as ddgs:
                return ddgs.news(
                    keywords=q,
                    region=region,
                    safesearch="off",
                    timelimit=tl,
                    max_results=MAX_RESULTS
                ) or []
        except Exception as exc:
            print(f"  DDG NEWS error ({region}): {exc}")
            return []

    def _run_ddg_text(q: str, tl: str | None, region: str = "vi-vn"):
        """Fallback to web search."""
        try:
            with DDGS() as ddgs:
                kwargs = {
                    "keywords": q,
                    "region": region,
                    "safesearch": "off",
                    "max_results": MAX_RESULTS,
                }
                if tl:
                    kwargs["timelimit"] = tl
                return ddgs.text(**kwargs) or []
        except Exception as exc:
            print(f"  DDG TEXT error ({region}): {exc}")
            return []

    # --- NEWS-FIRST SEARCH STRATEGY ---
    # ∆Øu ti√™n tin t·ª©c m·ªõi nh·∫•t t·ª´ ngu·ªìn news tr∆∞·ªõc

    # Default timelimit = 1 th√°ng ƒë·ªÉ l·∫•y tin m·ªõi nh·∫•t
    if timelimit is None:
        timelimit = "m"  # Default: tin trong th√°ng

    # 1. PRIORITY 1: Vietnamese News (tin t·ª©c ti·∫øng Vi·ªát m·ªõi nh·∫•t)
    print(f"  [DDG-NEWS] T√¨m tin t·ª©c VN: {cleaned_input[:50]}...")
    _ingest_ddg(_run_ddg_news(cleaned_input, timelimit, region="vi-vn"), source_type="news")

    # 2. PRIORITY 2: International News (LU√îN search c·∫£ ti·∫øng Anh)
    if en_query and len(en_query) > 5:
        print(f"  [DDG-NEWS] T√¨m tin t·ª©c QT: {en_query[:50]}...")
        _ingest_ddg(_run_ddg_news(en_query, timelimit, region="wt-wt"), source_type="news")

    # 3. FALLBACK WEB: Khi √≠t news (<5), search web ƒë·ªÉ l·∫•y th√™m t·ª´ Wikipedia, etc.
    if len(all_items) < 5:
        print(f"  [DDG-WEB] Fallback t√¨m ki·∫øm web: {query_vi[:50]}...")
        _ingest_ddg(_run_ddg_text(query_vi, None, region="vi-vn"), source_type="web")  # No timelimit for wiki
        
        # English web fallback ƒë·ªÉ l·∫•y Wikipedia ti·∫øng Anh
        if en_query and len(en_query) > 5:
            print(f"  [DDG-WEB] T√¨m ki·∫øm Wikipedia EN: {en_query[:50]}...")
            _ingest_ddg(_run_ddg_text(en_query, None, region="wt-wt"), source_type="web")

    # Sort by date (newest first)
    all_items.sort(key=_sort_key)

    print(f"üìä Search: T·ªïng c·ªông {len(all_items)} b·∫±ng ch·ª©ng.")
    return all_items[:MAX_RESULTS]

